---
title: Introduction to Prediction Intervals
author: Bryan Shalloway
date: '2021-03-18'
slug: intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals
thumbnail: /post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/prediction-intervals-sample-1.png
---


<div id="TOC">
<ul>
<li><a href="#providing-more-than-point-estimates">Providing More Than Point Estimates</a><ul>
<li><a href="#considering-uncertainty">Considering Uncertainty</a></li>
<li><a href="#observation-specific-intervals">Observation Specific Intervals</a></li>
</ul></li>
<li><a href="#a-few-things-to-understand-about-prediction-intervals">A Few Things to Understand About Prediction Intervals</a><ul>
<li><a href="#prediction-intervals-and-confidence-intervals">Prediction Intervals and Confidence Intervals</a></li>
<li><a href="#analytic-method-of-calculating-prediction-intervals">Analytic Method of Calculating Prediction Intervals</a></li>
<li><a href="#visual-comparison-of-prediction-intervals-and-confidence-intervals">Visual Comparison of Prediction Intervals and Confidence Intervals</a></li>
<li><a href="#inference-or-prediction">Inference or Prediction?</a></li>
<li><a href="#cautions-with-overfitting">Cautions with Overfitting</a></li>
</ul></li>
<li><a href="#generalizability">Generalizability</a></li>
<li><a href="#review-prediction-intervals">Review Prediction Intervals</a><ul>
<li><a href="#coverage">Coverage</a></li>
<li><a href="#interval-width">Interval Width</a></li>
</ul></li>
<li><a href="#closing-notes">Closing Notes</a></li>
</ul>
</div>

<p>Prediction intervals provide a measure of uncertainty for predictions on individual observations. This post…</p>
<ul>
<li>builds up a motivating example for using prediction intervals</li>
<li>describes factors that influence prediction intervals</li>
<li>shows examples of how to build and review prediction intervals</li>
</ul>
<p>I use the R programming language and the <a href="https://www.tidyverse.org/">tidyverse</a> + <a href="https://www.tidymodels.org/">tidymodels</a> suite of packages to create all models and figures.</p>
<p>This is the first of three posts on prediction intervals:</p>
<ol style="list-style-type: decimal">
<li>Introduction to Prediction Intervals (this post)</li>
<li>Quantile Regression for Prediction Intervals</li>
<li>Simulating Prediction Intervals</li>
</ol>
<div id="providing-more-than-point-estimates" class="section level1">
<h1>Providing More Than Point Estimates</h1>
<p>Imagine you are an analyst for a business to business (B2B) seller and are responsible for identifying appropriate prices for complicated products with non-standard selling prices<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. If you have more than one or two variables that influence price, statistical or machine learning models offer useful techniques for determining the optimal way to combine features to pinpoint expected prices of future deals<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> (of course margin, market positioning, and other business considerations also matter).</p>
<p>You might first build a model for <em>expected sale price</em> that is fit based on the patterns in historical sales data. The naive approach would be to use predictions from this model as a reference point. If a customer’s offer falls above the expected price, you interpret this as a “good” deal; if the offer falls below the predicted price the deal as “bad.”</p>
<p>As a stand-in for the large complicated products typically sold in B2B markets, I will use <a href="https://github.com/topepo/AmesHousing">data on home sales</a> from Ames, Iowa for my examples<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>Load in the data and then split it into a training dataset (for exploration and model development) and validation dataset (holdout data not used in model training but reserved for evaluating model performance)<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<pre class="r"><code>library(tidyverse)
library(tidymodels)
library(AmesHousing)

ames &lt;- make_ames() %&gt;% 
  mutate(Years_Old = Year_Sold - Year_Built,
         Years_Old = ifelse(Years_Old &lt; 0, 0, Years_Old))

set.seed(4595)
data_split &lt;- initial_split(ames, strata = &quot;Sale_Price&quot;, p = 0.75)

ames_train &lt;- training(data_split)
ames_holdout  &lt;- testing(data_split)</code></pre>
<p>Specify preprocessing steps and a multiple linear regression model to predict <em>Sale Price</em> – actually <span class="math inline">\(\log_{10}{(Sale\:Price)}\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. (There are many important features I do not include. I am also skipping a lot of <em>important</em> steps in predictive modeling<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>).</p>
<pre class="r"><code>lm_recipe &lt;- 
  recipe(
    Sale_Price ~ Lot_Area + Neighborhood  + Years_Old + Gr_Liv_Area + Overall_Qual + Total_Bsmt_SF + Garage_Area, 
    data = ames_train
  ) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;%
  step_log(Lot_Area, Gr_Liv_Area, base = 10) %&gt;%
  step_log(Total_Bsmt_SF, Garage_Area, base = 10, offset = 1) %&gt;%
  step_novel(Neighborhood, Overall_Qual) %&gt;% 
  step_other(Neighborhood, Overall_Qual, threshold = 50) %&gt;% 
  step_dummy(Neighborhood, Overall_Qual) %&gt;%
  step_interact(terms = ~contains(&quot;Neighborhood&quot;)*Lot_Area)

lm_mod &lt;- linear_reg() %&gt;% 
  set_engine(engine = &quot;lm&quot;) %&gt;%
  set_mode(&quot;regression&quot;)

lm_wf &lt;- workflows::workflow() %&gt;% 
  add_model(lm_mod) %&gt;% 
  add_recipe(lm_recipe) %&gt;% 
  fit(ames_train)</code></pre>
<p>Then make predictions on the holdout set.</p>
<pre class="r"><code>data_preds &lt;- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout)
) %&gt;% 
  bind_cols(relocate(ames_holdout, Sale_Price)) %&gt;% 
  mutate(.pred = 10^.pred) %&gt;%
  select(Sale_Price, .pred, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)</code></pre>
<p>The one-row table below shows that given the case of</p>
<ul>
<li><code>Lot_Area</code> = 31,770sqft ;</li>
<li><code>Neighborhood</code> = North Ames;</li>
<li><code>Years_Old</code> = 50yrs;</li>
<li><code>Gr_Liv_Area</code>: 1,656sqft;</li>
<li><code>overall_Qual</code> = “Above Average”;</li>
<li><code>Total_Bsmt_SF</code> = 1,080sqft;</li>
<li><code>Garage_Area</code> = 528sqft,</li>
</ul>
<p>the model would predict the <code>Sale_Price</code> for the home to be $184,503.</p>
<pre class="r"><code>data_preds %&gt;% 
  select(-Sale_Price) %&gt;% 
  head(1) %&gt;% 
  knitr::kable(digits = 0)</code></pre>
<table>
<colgroup>
<col width="7%" />
<col width="9%" />
<col width="14%" />
<col width="10%" />
<col width="13%" />
<col width="15%" />
<col width="15%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">.pred</th>
<th align="right">Lot_Area</th>
<th align="left">Neighborhood</th>
<th align="right">Years_Old</th>
<th align="right">Gr_Liv_Area</th>
<th align="left">Overall_Qual</th>
<th align="right">Total_Bsmt_SF</th>
<th align="right">Garage_Area</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">184503</td>
<td align="right">31770</td>
<td align="left">North_Ames</td>
<td align="right">50</td>
<td align="right">1656</td>
<td align="left">Above_Average</td>
<td align="right">1080</td>
<td align="right">528</td>
</tr>
</tbody>
</table>
<p>Using the naive approach, whether an offer is above or below this <em>expected</em> price may provide a first indication of whether the deal is ‘good’ or ‘bad’ for the seller<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. The magnitude of the difference between the offer and the expected price is also important. However you often will care about how big this difference is relative to the underlying uncertainty in <code>Sale_Price</code> – which calls for the use of statistics, specifically predictive inference<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<div id="considering-uncertainty" class="section level2">
<h2>Considering Uncertainty</h2>
<p>Let’s consider an offer of $180,000 for our example house. You might first think you ought to reject the offer due to it being ~$4,500 less than the expected price. However a <em>less naive</em> approach would be to compare the $180,000 offer in the context of the model’s observed accuracy when making predictions on a holdout set<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>.</p>
<p>We need to be careful regarding how we consider performance and the errors of our model. For this particular problem, the variability in our model’s errors increase with the magnitude of the predictions for <code>Sale_Price</code><a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. We want an error metric that will not inflate with <code>Sale_Price</code>. Hence rather than review the difference between <code>Sale_Price</code> and Expected[<code>Sale_Price</code>], it is more appropriate to review either:</p>
<ul>
<li><span class="math inline">\(\log_{10}(Sale\_Price)\)</span> against Expected[<span class="math inline">\(\log_{10}(Sale\_Price)\)</span>] OR</li>
<li>an error metric that is in terms of percent of <code>Sale_Price</code></li>
</ul>
<p>I will use the latter method and focus on the percentage error<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>. The offer of $180,000 corresponds with a 2.5% difference from expectations. This falls well within the distribution of errors seen by our model on a holdout dataset<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>.</p>
<pre class="r"><code>offer &lt;- tibble(PE = (184503 - 180000) / 180000, offer = &quot;offer&quot;)

data_preds %&gt;% 
  mutate(PE = (Sale_Price - .pred) / Sale_Price) %&gt;% 
  relocate(PE) %&gt;% 
  ggplot(aes(x = PE))+
  geom_histogram(bins = 50)+
  geom_vline(aes(xintercept = PE, color = offer), data = offer)+
  scale_x_continuous(limits = c(-1, 1), labels = scales::percent)+
  labs(x = &quot;Percent Error&quot;)+
  theme_bw()</code></pre>
<p><img src="/post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>On average our model is off by around ~12% of the actual <code>Sale_Price</code> observed.</p>
<pre class="r"><code>data_preds %&gt;% 
  yardstick::mape(Sale_Price, .pred) %&gt;% 
  knitr::kable(digits = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">.metric</th>
<th align="left">.estimator</th>
<th align="right">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mape</td>
<td align="left">standard</td>
<td align="right">11.8</td>
</tr>
</tbody>
</table>
<p>90% of errors on the holdout dataset were between -27.1% and 21.3% of the actual <code>Sale_Price</code>.</p>
<pre class="r"><code>data_preds %&gt;% 
  mutate(PE = (Sale_Price - .pred) / Sale_Price,
         APE = abs(PE)) %&gt;% 
  summarise(quant_05 = quantile(PE, 0.05) * 100,
            quant_95 = quantile(PE, 0.95) * 100) %&gt;% 
  knitr::kable(digits = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">quant_05</th>
<th align="right">quant_95</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-27.1</td>
<td align="right">21.3</td>
</tr>
</tbody>
</table>
<p>All of which suggests the $180,000 offer does not represent a substantial outlier from the typical variability of observed from predicted <code>Sale_Price</code><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>.</p>
<p><strong>Is your model performant enough to be useful?</strong></p>
<p>Before using the model for predictive inference, one <em>should</em> have reviewed overall performance on a holdout dataset to ensure the model is sufficiently accurate for the business context. For example, for our problem is an average error of ~12% and 90% prediction intervals of +/- ~25% of <code>Sale_Price</code> useful? If the answer is “no,” that suggests the need for more effort in improving the accuracy of the model (e.g. trying other transformations, features, model types)<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>. For our examples we are assuming the answer is ‘yes,’ our model is accurate enough (so it is appropriate to move-on and focus on prediction intervals).</p>
</div>
<div id="observation-specific-intervals" class="section level2">
<h2>Observation Specific Intervals</h2>
<p>While a good starting point, a limitation with using aggregate error metrics to estimate intervals indiscriminately across observations<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> is that different attributes may associate with different levels of uncertainty in the prediction. Specifically, predictions further from the centroid of the data generally have more uncertainty in the expected price.</p>
<p>A toy seesaw is a good analogy for this phenomenon:</p>
<div class="figure">
<img src="https://www.clipartkey.com/mpngs/m/92-924895_seesaw-svg-png-icon-free-download-seesaw-icon.png" alt="image source" />
<p class="caption"><a href="https://www.clipartkey.com/mpngs/m/92-924895_seesaw-svg-png-icon-free-download-seesaw-icon.png">image source</a></p>
</div>
<blockquote>
<p>As the angle of the bench changes… the further you are from the center, the more distance you will move up/down →</p>
<p>The angle of the seesaw represents variability in the model function being estimated. The distance from the seesaw’s pivot point corresponds with the distance from the centroid of the data</p>
</blockquote>
<p>This variability in estimating the model’s expected value is represented by dashed blue lines in the chart below of a generic linear model:</p>
<div class="figure">
<img src="https://i.stack.imgur.com/GeeI3.png" alt="Image source" />
<p class="caption">Image <a href="https://i.stack.imgur.com/GeeI3.png">source</a></p>
</div>
<p>Because of this variability in the uncertainty of the expected value of the model, it is preferable to determine a plausible price range that is specific to the attributes of the observation<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.</p>
<p>The uncertainty described here corresponds with the <em>confidence intervals</em> and not quite as directly with the <em>prediction intervals</em> (the latter of which is the focus of this post). The distinction and relationship between these will be discussed in <a href="#a-few-things-to-understand-about-prediction-intervals">A Few Things to Understand About Prediction Intervals</a>.</p>
</div>
</div>
<div id="a-few-things-to-understand-about-prediction-intervals" class="section level1">
<h1>A Few Things to Understand About Prediction Intervals</h1>
<p>If you are primarily interested in how to build and review prediction intervals you can skip to <a href="#review-prediction-intervals">Review Prediction Intervals</a>.</p>
<div id="prediction-intervals-and-confidence-intervals" class="section level2">
<h2>Prediction Intervals and Confidence Intervals</h2>
<p>Prediction intervals and confidence intervals<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> are often confused. Confidence intervals generally refer to making inferences on <em>averages</em> – this is most useful for evaluating parameter estimates, performance metrics, relationships with covariates, etc. However if you are interested in price ranges on <em>individual</em> observations (as in our case), prediction intervals are what you want<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>. Rob Hyndman has a helpful post where he describes the differences in more detail: <a href="https://robjhyndman.com/hyndsight/intervals/">The difference between prediction intervals and confidence intervals</a><a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>.</p>
<p>In linear regression, “prediction intervals” refer to a type of <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence interval</a><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>, namely the confidence interval for a single observation (a “predictive confidence interval”). Confidence intervals have a specific statistical interpretation. In later posts on this topic, the intervals I create do not quite mirror the interpretations that go with a predictive confidence interval. I will use the term “prediction interval” somewhat loosely to refer to a plausible range of values for an observation<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>.</p>
</div>
<div id="analytic-method-of-calculating-prediction-intervals" class="section level2">
<h2>Analytic Method of Calculating Prediction Intervals</h2>
<p>Linear regression models can produce prediction intervals analytically. The equation below is for <em>simple</em> linear regression (meaning just one ‘x’ input) but is helpful for gaining an intuition on the key parts that contribute to the width of a prediction interval:</p>
<p class="text-align-center">
<span class="math inline">\(\hat{y}_h \pm t_{(1-\alpha/2, n-2)} \times \sqrt{MSE \times \left( 1+\dfrac{1}{n} + \dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}\)</span>
</p>
<p>Pseudo-translation of key parts of equation:</p>
<ul>
<li>{<span class="math inline">\(\hat{y}_h\)</span> : prediction}<br />
</li>
<li>{<span class="math inline">\(t_{(1-\alpha/2, n-2)}\)</span> : multiplier for desired level of confidence, e.g. 95%}<br />
</li>
<li>{MSE: multiplier for average error}<br />
</li>
<li>{<span class="math inline">\(\dfrac{MSE}{n}\)</span> : multiplier based on number of observations – fewer contributes to greater variability.}</li>
<li>{<span class="math inline">\(MSE\dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\)</span> : multiplier for distance from centroid of data}</li>
</ul>
<p>You will notice the equation for the prediction interval is very similar to that of the equation for a confidence interval<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>:</p>
<p class="text-align-center">
<span class="math inline">\(\hat{y}_h \pm t_{(1-\alpha/2, n-2)} \times \sqrt{MSE \left(\dfrac{1}{n} + \dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}\)</span>
</p>
<p>The variance for the prediction interval just has an extra Mean Squared Error (MSE) term<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>. The prediction interval is essentially the variance in estimating the model<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> combined with the variability of individual observations in the sample.</p>
<p>Paraphrasing, the uncertainty in predictions can be thought of as coming from two sources:</p>
<ol style="list-style-type: decimal">
<li>uncertainty in fitting the model, i.e. variability in determining the <em>expected</em> value of the target (‘target’ just means the variable of interest, the thing we are predicting)</li>
<li>uncertainty in the sample, i.e. inherent variability of the target in observations</li>
</ol>
<p>The first source may vary in part depending on position relative to the centroid (think back to the seesaw analogy in <a href="#observation-specific-intervals">Observation Specific Intervals</a> and the influence of position on amount of movement i.e. uncertainty). It will also vary depending on the number of observations – the more observations you have in your training data, the smaller the general variability due to model estimation will be. The latter part is assumed to be constant across observations<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a> and does not change as the number of observations increases.</p>
<p>Do not think of these sources of uncertainty as being compounded on one another to produce the prediction interval. They are added together<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> and then the square root is taken. In most cases the variability due to the sample will be greater than the variability in estimating the expected value. Hence, the interval’s width due to uncertainty in estimating the model is somewhat ‘watered down’ by the variability in the sample. Therefore (for prediction intervals produced analytically at least) we will generally not see large differences in the widths of prediction intervals across observations (even at points that are relatively far from the centroid of the data). Therefore our prediction intervals in later sections of this post will not actually be that much different from those that might have been produced using aggregated error metrics similar to those discussed in <a href="#considering-uncertainty">Considering Uncertainty</a>.</p>
<p>However understanding the intuition on these sources of uncertainty in prediction intervals is still helpful. In follow-up posts I use more flexible methods for building prediction intervals. These other methods can produce substantive differences in the width of prediction intervals between observations6[For example, simulation techniques are able to capture uncertainty in model estimation in ways that do not depend on distance of an observation from the centroid of the data. They also capture uncertainty that results from trying to estimate too many parameters relative to the size of the dataset.].</p>
</div>
<div id="visual-comparison-of-prediction-intervals-and-confidence-intervals" class="section level2">
<h2>Visual Comparison of Prediction Intervals and Confidence Intervals</h2>
<p>The chart below shows the key outputs concerning prediction that can be outputted by a generic linear model:</p>
<ul>
<li>expected value, i.e. the predictions, the average of the value of the target given a set of attributes (black line)</li>
<li>confidence intervals, i.e. range for the expected values (red dashed line)</li>
<li>prediction intervals (our primary interest in this post), i.e. range for the predictions on an individual observation (green dotted line)</li>
</ul>
<div class="figure">
<img src="https://i1.wp.com/statistical-research.com/wp-content/uploads/2013/10/prediction_confidence.png?" alt="Image source" />
<p class="caption"><a href="https://i1.wp.com/statistical-research.com/wp-content/uploads/2013/10/prediction_confidence.png?">Image source</a></p>
</div>
<p>Confidence intervals are more narrow (as they concern only uncertainty in the model’s estimation of the expected value). Prediction intervals are wider as they concern both model uncertainty and the sampling uncertainty of any observation (the latter of which contributes far greater variance). Both confidence and prediction intervals are wider the further the prediction is from the centroid of the data – however the effect is far greater for the confidence interval (as the uncertainty in the prediction interval is dominated by the random variance of the sample, which is assumed to be constant across observations). This explains why the curve of the confidence intervals is far more pronounced.</p>
</div>
<div id="inference-or-prediction" class="section level2">
<h2>Inference or Prediction?</h2>
<p>Predictive modeling is typically used either for making predictions or inferences. The assumptions associated with your model usually matter more when doing inference compared to prediction<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>. Therefore, to get a sense of how careful you should be regarding model assumptions for our example you might ask:</p>
<blockquote>
<p>“Are prediction intervals an example of prediction or inference?”</p>
</blockquote>
<p>The answer is, while it (kind of) depends…</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/QRLEXAFOvkM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p><em>really</em>, prediction intervals should be thought of as a kind of inference.</p>
<p>For this post the prediction interval is explicitly an inference on the predictions. In a later post I will actually just be making <em>predictions</em> for quantiles at upper and lower bounds of interest. However, generally, intervals should be thought of as a kind of inference<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>. Therefore you should be thoughtful regarding the assumptions you are making about the nature of the data and your model specification<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a>.</p>
<p>For example, an important assumption when doing predictive inference is heteroskedasticity of errors – which means that the variability of our errors should be constant across our predictions. As mentioned previously, if building a model for <em>Sale Price</em> we would find that errors tend to increase with our predictions. Hence instead of building a model to predict <em>Sale Price</em> we build a model to predict <span class="math inline">\(\log_{10}(Sale\:Price)\)</span> – this helps our model’s outputted prediction intervals to be more appropriate across observations<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a>.</p>
<p>In follow up posts the methods I describe do not depend <em>as</em> strongly on assumptions to produce reliable prediction intervals<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a>.</p>
</div>
<div id="cautions-with-overfitting" class="section level2">
<h2>Cautions with Overfitting</h2>
<p>You want the inferences and predictions of your model to be <em>generalizable</em>, i.e. usable outside of the context of your training dataset. This is why we holdout data to review performance: we want to review the model’s performance on data it has never seen. If your model <a href="https://en.wikipedia.org/wiki/Overfitting">overfits</a><a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> the training data, the prediction interval will often underestimate the expected variability in price and your prediction intervals may be too narrow. Two strategies you might use to correct for this:</p>
<ol style="list-style-type: decimal">
<li>Consider a more simple model<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></li>
<li>Tune your prediction intervals based on coverage measured on holdout data (coverage represents the proportion of observations that <em>actually</em> fall within their prediction interval).</li>
</ol>
<p>A quick heuristic to check if your model is overfitting is to compare the model’s performance on a holdout dataset against performance on the data that was used to train the model. If performance is substantially better on the training dataset, it may suggest the model has overfit.</p>
<p><strong>Simpler model</strong></p>
<p>If your model seems to be overfitting, you may want to try fitting a more simple model<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>. If your model has a small difference in performance metrics between training and holdout datasets, you likely will have more faith in the ranges given by the prediction intervals.</p>
<p><strong>Problem with comparing difference in performance between training &amp; holdout data</strong></p>
<p>However, for the most part, your performance is going to <em>always be better</em> on the training data than on the holdout data<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>. With regard to overfitting, you really care about whether performance is worse on the holdout dataset compared to an alternative simpler model’s performance on the holdout set. You don’t really care if a model’s performance on training and holdout data is similar, just that performance on a holdout dataset is as good as possible.</p>
<p>Let’s take an example when building random forest models and tuning on the <code>min_n</code> attribute (smaller values for <code>min_n</code><a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> represent more complicated models.):</p>
<div class="figure">
<img src="https://camo.githubusercontent.com/c498111162b8f8d4fced8fe07f7b532fe2de3c4a66f68e29b9f29832148ab674/68747470733a2f2f692e696d6775722e636f6d2f316e53687836792e706e67" alt="See my gist for code" />
<p class="caption">See my <a href="https://gist.github.com/brshallo/516e8e52fb911a96efafbf01d606a113">gist</a> for code</p>
</div>
<p>The gap in train-holdout (AKA, analysis-assessment<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a>) performance is greater for more complicated models<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> (which, as mentioned above, may signal overfitting). However more complicated models also perform better on the holdout data. Hence, we would likely select the complicated model<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a> <em>even though</em> the performance estimates on the training data will be the most unrealistic.</p>
<p>While a difference in performance on training and holdout datasets may provide a helpful indicator for overfitting, it is not really what you care about during model selection<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a>.</p>
<p><strong>Tune your intervals based on a holdout set</strong></p>
<p>Pretend that your selected model has a substantial difference in performance when evaluated on training versus holdout data. You are worried that the width of your model’s outputted prediction intervals will be optimistically narrow. However you still want to keep this model because it has the best performance when evaluated on holdout data<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>. An alternative to fitting a more simple model is to adjust the confidence level of your prediction intervals, tuning them based on the coverage level on a holdout dataset. To describe this solution, I will define three ways of thinking about coverage:</p>
<ul>
<li><em>target coverage</em>: The level of coverage you want to attain on a holdout dataset (i.e. the proportion of observations you want to fall within your prediction intervals).</li>
<li><em>expected coverage</em>: The level of confidence in the model for the prediction intervals, e.g. asking the model to predict 90% prediction intervals.</li>
<li><em>empirical coverage</em>: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model.</li>
</ul>
<p>Ideally all three align. However let’s say in our hypothetical example you have a 90% target coverage. If our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>.</p>
<p>This is called “adaptive coverage” and is discussed briefly by Emmanuel Candes in the video below:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/61tpigfLHso?start=1654" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>In a follow-up post, <strong>Building Prediction Intervals with Quantile Regression</strong> I will walk through a similar example where I tune the expected coverage.</p>
</div>
</div>
<div id="generalizability" class="section level1">
<h1>Generalizability</h1>
<p>Let’s check to see how performance of our model compares between our training and holdout datasets.</p>
<pre class="r"><code>train_preds &lt;- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_train)
) %&gt;% 
  bind_cols(relocate(ames_train, Sale_Price)) %&gt;% 
  mutate(.pred = 10^.pred)

bind_rows(
  yardstick::mape(train_preds, Sale_Price, .pred),
  yardstick::mape(data_preds, Sale_Price, .pred)
) %&gt;% 
  mutate(dataset = c(&quot;training&quot;, &quot;validation&quot;)) %&gt;% 
  knitr::kable(digits = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">.metric</th>
<th align="left">.estimator</th>
<th align="right">.estimate</th>
<th align="left">dataset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mape</td>
<td align="left">standard</td>
<td align="right">11.3</td>
<td align="left">training</td>
</tr>
<tr class="even">
<td align="left">mape</td>
<td align="left">standard</td>
<td align="right">11.8</td>
<td align="left">validation</td>
</tr>
</tbody>
</table>
<p>Performance on the training and holdout datasets are not grossly different, hence there does not seem to be a major concern with overfitting in our model<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a>.</p>
</div>
<div id="review-prediction-intervals" class="section level1">
<h1>Review Prediction Intervals</h1>
<p>90% prediction interval for our example case:</p>
<pre class="r"><code>preds_intervals &lt;- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),
  type = &quot;pred_int&quot;,
  level = 0.90
) %&gt;% 
  mutate(across(contains(&quot;.pred&quot;), ~10^.x)) %&gt;%
  bind_cols(data_preds) %&gt;% 
  select(contains(&quot;.pred&quot;), Sale_Price, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)

preds_intervals %&gt;% 
  select(-Sale_Price) %&gt;% 
  slice(1) %&gt;% 
  knitr::kable(digits = 0)</code></pre>
<table style="width:100%;">
<colgroup>
<col width="10%" />
<col width="10%" />
<col width="6%" />
<col width="7%" />
<col width="11%" />
<col width="8%" />
<col width="10%" />
<col width="12%" />
<col width="12%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">.pred_lower</th>
<th align="right">.pred_upper</th>
<th align="right">.pred</th>
<th align="right">Lot_Area</th>
<th align="left">Neighborhood</th>
<th align="right">Years_Old</th>
<th align="right">Gr_Liv_Area</th>
<th align="left">Overall_Qual</th>
<th align="right">Total_Bsmt_SF</th>
<th align="right">Garage_Area</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">140208</td>
<td align="right">242793</td>
<td align="right">184503</td>
<td align="right">31770</td>
<td align="left">North_Ames</td>
<td align="right">50</td>
<td align="right">1656</td>
<td align="left">Above_Average</td>
<td align="right">1080</td>
<td align="right">528</td>
</tr>
</tbody>
</table>
<p>This suggests that, given these attributes, we would expect that the <code>Sale_Price</code> for an individual observation would fall between $140,208 and $242,793 (with 90% confidence). Our example offer of $184,000 falls well within this range, meaning that – based on the model – the offer does not appear unreasonable.</p>
<p>Let’s view 90% prediction intervals against <em>actual</em> prices on a sample of observations in our holdout dataset:</p>
<pre class="r"><code>set.seed(1234)
p &lt;- preds_intervals %&gt;% 
  mutate(pred_interval = ggplot2::cut_number(.pred, 10)) %&gt;% 
  group_by(pred_interval) %&gt;% 
  sample_n(2) %&gt;% 
  ggplot(aes(x = .pred))+
  geom_point(aes(y = .pred, color = &quot;prediction interval&quot;))+
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = &quot;prediction interval&quot;))+
  geom_point(aes(y = Sale_Price, color = &quot;actuals&quot;))+
  labs(title = &quot;90% prediction intervals on a holdout dataset&quot;,
       subtitle = &quot;Linear model (analytic method)&quot;,
       y = &quot;Sale_Price prediction intervals and actuals&quot;)+
  theme_bw()+
  coord_fixed()

p +
  scale_x_log10(labels = scales::dollar)+
  scale_y_log10(labels = scales::dollar)</code></pre>
<p><img src="/post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/prediction-intervals-sample-1.png" width="672" /></p>
<p>(Log transformations are applied on the x and y axis to adjust for the fact that variability in estimates, and therefore prediction intervals, increases with predicted <code>Sale_Price</code><a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a>.)</p>
<div id="coverage" class="section level2">
<h2>Coverage</h2>
<p>Let’s review what proportion of our holdout observations are actually “covered” by their 90% prediction intervals<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a>.</p>
<pre class="r"><code>preds_intervals %&gt;%
  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower &amp; Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% 
  summarise(n = n(),
            n_covered = sum(
              covered
            ),
            stderror = 100 * sd(covered) / sqrt(n),
            coverage_prop = 100 * n_covered / n) %&gt;% 
  knitr::kable(digits = 2)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">n</th>
<th align="right">n_covered</th>
<th align="right">stderror</th>
<th align="right">coverage_prop</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">731</td>
<td align="right">678</td>
<td align="right">0.96</td>
<td align="right">92.75</td>
</tr>
</tbody>
</table>
<p>~93% of our holdout observations were actually covered<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a>. We can likely trust these intervals aren’t too narrow<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a>. It is usually better to be on the slightly conservative than optimistic side regarding coverage<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a>.</p>
<p>In addition to reviewing overall coverage, it may be helpful to review coverage across predictions. This may be helpful in providing evidence of whether we have “marginal” or “conditional” coverage<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>.</p>
<p>The chart below splits the holdout data into five segments ordered by predicted price with equal number of observations<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a> and checks the proportion covered over each quintile.</p>
<pre class="r"><code>preds_intervals %&gt;%
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% 
  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower &amp; Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% 
  group_by(price_grouped) %&gt;% 
  summarise(n = n(),
            n_covered = sum(
              covered
            ),
            stderror = sd(covered) / sqrt(n),
            n_prop = n_covered / n) %&gt;% 
  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% 
  separate(x_tmp, c(&quot;min&quot;, &quot;max&quot;), sep = &quot;,&quot;) %&gt;% 
  mutate(across(c(min, max), as.double)) %&gt;% 
  ggplot(aes(x = forcats::fct_reorder(scales::dollar(max), max), y = n_prop))+
  geom_line(aes(group = 1))+
  geom_errorbar(aes(ymin = n_prop - 2 * stderror, ymax = n_prop + 2 * stderror))+
  coord_cartesian(ylim = c(0.70, 1.01))+
  # scale_x_discrete(guide = guide_axis(n.dodge = 2))+
  labs(x = &quot;Max Predicted Price for Quintile&quot;,
       y = &quot;Coverage at Quintile&quot;,
       title = &quot;Coverage by Quintile of Predictions&quot;,
       subtitle = &quot;On a holdout Set&quot;,
       caption = &quot;Error bars represent {coverage} +/- 2 * {coverage standard error}&quot;)+
  theme_bw()</code></pre>
<p><img src="/post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>This suggests there may be slightly different levels of coverage at different quintiles (e.g. the cheapest predicted houses have less coverage than those in the 2nd quintile.)<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a>.</p>
<p>A test for checking whether variability in the categorical variable <code>covered</code> (covered / not-covered) does not vary across the quintile of predicted <code>Sale_Price</code> (1st, 2nd, 3rd, 4th, 5th) could be done using a <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-square statistical test</a>:</p>
<pre class="r"><code>preds_intervals %&gt;% 
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% 
  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower &amp; Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% 
  with(chisq.test(price_grouped, covered))</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  price_grouped and covered
## X-squared = 10.389, df = 4, p-value = 0.03436</code></pre>
<p>The results provide evidence that whether an observation is covered or not-covered may depend (to some extent) on which quintile of predicted <code>Sale_Price</code> the observation falls within<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a>.</p>
<p>I will show in a follow-up post on <strong>Simulation Methods for Building Prediction Intervals</strong> intervals that seem to have more consistent coverage rates across predicted <code>Sale_Price</code>.</p>
</div>
<div id="interval-width" class="section level2">
<h2>Interval Width</h2>
<p>Another helpful way to evaluate prediction intervals is by their width. More narrow bands indicate a more precise model.</p>
<p>Remember from the section on <a href="#considering-uncertainty">Considering Uncertainty</a> that the errors in <code>Sale_Price</code> increase with the size of the prediction. To adjust for this, an appropriate metric is interval width as a percentage of predicted <code>Sale_Price</code><a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a>. I will review these across quintiles of the predictions.</p>
<pre class="r"><code>lm_interval_widths &lt;- preds_intervals %&gt;% 
  mutate(interval_width = .pred_upper - .pred_lower,
         interval_pred_ratio = interval_width / .pred) %&gt;% 
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% 
  group_by(price_grouped) %&gt;% 
  summarise(n = n(),
            mean_interval_width_percentage = mean(interval_pred_ratio),
            stdev = sd(interval_pred_ratio),
            stderror = stdev / sqrt(n)) %&gt;% 
  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% 
  separate(x_tmp, c(&quot;min&quot;, &quot;max&quot;), sep = &quot;,&quot;) %&gt;% 
  mutate(across(c(min, max), as.double)) %&gt;% 
  select(-price_grouped) 

lm_interval_widths %&gt;% 
  knitr::kable(digits = 4)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">n</th>
<th align="right">mean_interval_width_percentage</th>
<th align="right">stdev</th>
<th align="right">stderror</th>
<th align="right">min</th>
<th align="right">max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">147</td>
<td align="right">0.5426</td>
<td align="right">0.0046</td>
<td align="right">4e-04</td>
<td align="right">43000</td>
<td align="right">121000</td>
</tr>
<tr class="even">
<td align="right">146</td>
<td align="right">0.5399</td>
<td align="right">0.0029</td>
<td align="right">2e-04</td>
<td align="right">121000</td>
<td align="right">146000</td>
</tr>
<tr class="odd">
<td align="right">146</td>
<td align="right">0.5402</td>
<td align="right">0.0038</td>
<td align="right">3e-04</td>
<td align="right">146000</td>
<td align="right">176000</td>
</tr>
<tr class="even">
<td align="right">146</td>
<td align="right">0.5412</td>
<td align="right">0.0049</td>
<td align="right">4e-04</td>
<td align="right">176000</td>
<td align="right">223000</td>
</tr>
<tr class="odd">
<td align="right">146</td>
<td align="right">0.5413</td>
<td align="right">0.0033</td>
<td align="right">3e-04</td>
<td align="right">223000</td>
<td align="right">487000</td>
</tr>
</tbody>
</table>
<p>The relative interval width is consistent across predicted <code>Sale_Price</code> and between observations<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a>. The 90% interval represents an average of ~54% of the predicted <code>Sale_Price</code><a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>. Recall from <a href="#analytic-method-of-calculating-prediction-intervals">Analytic Method of Calculating Prediction Intervals</a> that the only factor contributing to differences in interval width across predictions is the distance of an observations from the centroid of the data. This has a relatively small impact on interval width given the number of observations and sample variance (which are treated as constant across observations)<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> in our data.</p>
</div>
</div>
<div id="closing-notes" class="section level1">
<h1>Closing Notes</h1>
<p><em>Upsides of prediction intervals using parametric method with linear regression:</em></p>
<ul>
<li>Most common (largest number of people are familiar)</li>
<li>Straightforward calculation with low computation costs</li>
</ul>
<p><em>Downsides:</em></p>
<ul>
<li>Less flexible than alternative approaches.</li>
<li>Prediction intervals produced in this way have a variety of <a href="https://online.stat.psu.edu/stat501/lesson/3/3.3/#paragraph--766">strong assumptions</a> that they depend on (generally more heavily than other approaches).</li>
<li>Assumes the correct model (If your model overfits, the associated prediction intervals will likely be overly optimistic<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>)</li>
</ul>
<p><strong>Wrap-up:</strong></p>
<p>Prediction intervals provide an indicator for the uncertainty of individual predictions. They have advantages over point estimates in that they take into account the variability in the data to provide a “reasonable” range of values for an observation.</p>
<p>Measures like <em>coverage</em> and <em>interval width</em> are helpful for evaluating prediction intervals. These metrics will be used in follow-up posts on <strong>Building Prediction Intervals Using Quantile Regression</strong> and <strong>Building Prediction Intervals Using Simulation</strong>, where I walk through more sophisticated and flexible methods for building prediction intervals.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Non-standard, negotiated prices are common in the B2B world.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Analysts are sometimes tempted to compare the specific deal of interest against <em>only</em> historical deals that have similar attributes. The problem with attempting to do this is that, when you filter you lose data. After filtering across more than a couple dimensions, you likely will only have a few deals left in your sub-segment – too few to get a reliable sense of price variability. Prediction intervals offer a method for determining the variability of your estimates that spans across the multi-dimensional space that your data inhabits.</p>
<p>That said there are many situations where it does make sense to segment your data. Also the errors in your model should be independent from one another. During residual analysis you should ensure that there is not any bias across particular dimensions or ranges of data. Hence looking at particular segments of observations can be important – it’s just important to do so thoughtfully.</p>
<p>I’ve seen many occassions when an analyst keeps slicing their data and reviewing univariate relationships until they’ve convinced themselves they’ve found something – even when the sample sizes at this point are small and the nature of the relationship unconvincing.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>This is similar to what I did on a <a href="https://www.bryanshalloway.com/2020/08/17/pricing-insights-from-historical-data-part-1/">previous post on pricing</a>.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>If I was being a little more careful, I’d also split the <code>train</code> data into a validation set or I’d use <code>rsample</code> to set-up cross validation. In this post though I largely use <code>test</code> how you might use a validation set though. See <a href="https://www.tmwr.org/resampling.html">Tidy Modeling with R, 10.2</a> for more.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A log transform on the target can be thought of roughly as changing the model to be about changes in percent of <code>Sale_Price</code> rather than raw <code>Sale_Price</code>. Throughout the post I will be performing a log transformation on our target, <code>Sale_Price</code> prior to modeling. Hence in some cases I make comparisons against the log of dollars offered OR I will use some metric that is in terms of <em>percent</em> rather than raw <code>Sale_Price</code>. It is generally best to conduct model evaluation against whatever transformed scale is used in model building (i.e. in our case <code>log(Sale_Price, 10)</code>). This is suggested in <a href="https://www.tmwr.org/performance.html">Tidymodels with R</a>:</p>
<blockquote>
<p>“It is best practice to analyze the predictions on the transformed scale (if one were used) even if the predictions are reported using the original units.” -Kuhn, Silge</p>
</blockquote>
<p>This may not apply perfectly in our case (as we are talking more about evaluation steps that would likely happen after initial model building and selection and has more to do with particular predictions)… however we will still keep this in mind and try to mindful of how we present our evaluation metrics.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>E.g. data exploration, review of model assumptions, use of a separate validation set, feature engineering, tuning, etc.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Relative to prior deals which the model was trained on.<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>When you are dealing with problems that consider both magnitude and variability, statistics are often helpful. Statistics can help to provide a sense of the accuracy of the model and the extent to which an offer seems reasonable in the context of the variability in prices.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Statistics can provide context as to whether this represents a <em>small</em> or <em>large</em> deviation from expectations.<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>This is known as heteroskedasticity. We generally want our error metrics to be homoskedastic, i.e. ‘same variance’. This is why we built the model on the target <code>log(Sale_Price, 10)</code>. Note that the base rate of 10 doesn’t really matter that much, we could just have easily have built the model using the natural logarithm and gotten similar results.<a href="#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>I chose this method as I thought it would be easier to explain and some readers may be intimated by logs. During model building and evaluation the former would often be more appropriate to review. In particular the root mean squared error (RMSE) of the errors of the predictions for <code>log(Sale_Price)</code>.<a href="#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>Data not seen during model training and reserved for the purpose of model evaluation<a href="#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>Based on the observation level estimates given by our model on a holdout set.<a href="#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>Or a focus on other pieces of information outside of your model for making decisions, or going about addressing the problem in another way or redirecting your efforts towards a problem you can make more of an impact on.<a href="#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>As shown above in <a href="#considering-uncertainty">Considering Uncertainty</a>.<a href="#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>Rather than just looking at your observation of interest through the lens of aggregate error metrics.<a href="#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>Or when using Bayesian methods, credible intervals.<a href="#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>Confidence intervals are more directly related to the <em>standard error</em> of your sample, while prediction intervals are associated with the <em>standard deviation</em> of the errors. The width of your confidence intervals follow the central limit theorem and are thus sensitive to the sample size of your data and become more narrow as you collect more observations (and you gain ‘confidence’ in where the ‘true’ expected value resides). For prediction intervals, more observations may improve your estimate of the model or give you more faith in your prediction intervals, however if you have sufficient observations, the width of the intervals will not necessarily shrink in any <em>substantial</em> way.<a href="#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>Confidence intervals are typically the default output of many packages and functions that output “intervals” – <a href="https://github.com/robjhyndman/forecast">forecast</a> and other forecasting packages though typically default to prediction intervals.<a href="#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>Again, speaking through a frequentist lens.<a href="#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>“Predictive band” may be a more appropriate term.<a href="#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>I.e. the band around the <em>average</em> value, rather than the band around the value for an <em>individual</em> observation – as discussed in <a href="#a-few-things-to-understand-about-prediction-intervals">A Few Things to Understand About Prediction Intervals</a>.<a href="#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>If this is unclear to you, try distributing the MSE term and look again. Penn State also has helpful lessons on regression available <a href="https://online.stat.psu.edu/stat501/lesson/3/3.3#paragraph--766">publicly online</a>.<a href="#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>The square root of which is used to calculate the confidence interval.<a href="#fnref24" class="footnote-back">↩</a></p></li>
<li id="fn25"><p>This assumption of constant variability of the sample is common in many techniques for producing prediction intervals – even some techniques that use simulation based approaches.<a href="#fnref25" class="footnote-back">↩</a></p></li>
<li id="fn26"><p>To get the variance.<a href="#fnref26" class="footnote-back">↩</a></p></li>
<li id="fn27"><p>This is because your assumptions will influence the nature of the uncertainty in your model. When focused on prediction, you generally just care about minimizing error (in one form or another), assumptions regarding distributions of parameters, errors, etc. can often be ignored or at least relaxed.<a href="#fnref27" class="footnote-back">↩</a></p></li>
<li id="fn28"><p>Prediction intervals concern uncertainty, and whenever you are making estimates around uncertainty you are generally doing a kind of statistical inference.<a href="#fnref28" class="footnote-back">↩</a></p></li>
<li id="fn29"><p>Even when using simulation based techniques that, for other estimates, may allow you to be a little more cavalier regarding model assumptions… when it comes to predictive inference you will essentially always have <em>some</em> assumptions you need to hold onto.<a href="#fnref29" class="footnote-back">↩</a></p></li>
<li id="fn30"><p>If we did not do this our prediction intervals would be far too large for smaller values of <code>Sale_Price</code> and far too small for larger values of <code>Sale_Price</code>.<a href="#fnref30" class="footnote-back">↩</a></p></li>
<li id="fn31"><p>However there are limitations on assumption free predictive inference, (<a href="https://www.stat.cmu.edu/~ryantibs/papers/limits.pdf">Barber, et al</a>).<a href="#fnref31" class="footnote-back">↩</a></p></li>
<li id="fn32"><p>Model generalizability is a big topic I am going to just barely touch on it here.<a href="#fnref32" class="footnote-back">↩</a></p></li>
<li id="fn33"><p>That still fits the data well but is likely to overfit on the training data.<a href="#fnref33" class="footnote-back">↩</a></p></li>
<li id="fn34"><p>Lower <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">variance</a>.<a href="#fnref34" class="footnote-back">↩</a></p></li>
<li id="fn35"><p>This is true of some models to a greater extent than others.<a href="#fnref35" class="footnote-back">↩</a></p></li>
<li id="fn36"><p>Representing the minimum number of observations in a node for it to be split.<a href="#fnref36" class="footnote-back">↩</a></p></li>
<li id="fn37"><p><a href="http://www.feat.engineering/resampling.html">Feature Engineering and Selection, 3.4 Resampling</a> is a helpful resource for understanding the subtle distinction between these.<a href="#fnref37" class="footnote-back">↩</a></p></li>
<li id="fn38"><p>When <code>min_n</code> is smaller.<a href="#fnref38" class="footnote-back">↩</a></p></li>
<li id="fn39"><p>Again, when <code>min_n</code> is smallest.<a href="#fnref39" class="footnote-back">↩</a></p></li>
<li id="fn40"><p>This is likely the case even when you are planning on using the model to generate prediction intervals and not just point estimates.<a href="#fnref40" class="footnote-back">↩</a></p></li>
<li id="fn41"><p>Or some other reason, e.g. you are not able to retrain it or have some business constraint that means you are stuck with this model.<a href="#fnref41" class="footnote-back">↩</a></p></li>
<li id="fn42"><p>Tuning your alpha level until the coverage on a holdout dataset is where you want it.<a href="#fnref42" class="footnote-back">↩</a></p></li>
<li id="fn43"><p>This is typically the way this check would be used – if there is no difference, you can say there is no overfitting. If there is a difference, you may still be OK with that provided performance is still good enough (or better than alternatives) on a holdout dataset.<a href="#fnref43" class="footnote-back">↩</a></p></li>
<li id="fn44"><p>As noted previously the model was built to predict <code>log(Sale_Price, 10)</code> (as opposed to <code>Sale_Price</code>) as errors increase with the predicted price of the house. Hence why the transformed scales in the text are appropriate. If you are curious though, the figure below shows the prediction intervals on raw <code>Sale_Price</code> scale.</p>
<pre class="r"><code>p + 
  scale_x_continuous(labels = scales::dollar)+
  scale_y_continuous(labels = scales::dollar)</code></pre>
<p><img src="/post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/unnamed-chunk-8-1.png" width="672" /><a href="#fnref44" class="footnote-back">↩</a></p></li>
<li id="fn45"><p>I.e. the rate at which the <em>actual</em> value fall within the range of the prediction interval (coverage is essentially checking whether the range you expect from your prediction intervals is actually what is achieved). For our 90% prediction intervals, we will expect coverage on a holdout dataset to be 90%.<a href="#fnref45" class="footnote-back">↩</a></p></li>
<li id="fn46"><p>The difference between 93% and the expected coverage of 90% may just be the result of random variability in the data.<a href="#fnref46" class="footnote-back">↩</a></p></li>
<li id="fn47"><p>Though they are perhaps slightly conservative. To get a better estimate for coverage, we could use cross validation as this will provide estimates for coverage across multiple iterations of model fitting.<a href="#fnref47" class="footnote-back">↩</a></p></li>
<li id="fn48"><p>As models tend to perform worse than expected when put into production. I.e. it is better that our empirical coverage is greater than our target coverage than less than our target coverage – even if it would be ideal for these to align.<a href="#fnref48" class="footnote-back">↩</a></p></li>
<li id="fn49"><p>“Marginal” coverage generally means “on average across observations”, whereas “conditional” coverage means conditioned on the specific attributes for an observation. Hence “90%” marginal coverage allows that coverage may be higher in some areas and lower in others. 90% conditional coverage means that the conditional rate should be 90% across observations. “Conditional” coverage is generally harder to get than “marginal” coverage and is actually impossible to achieve without some assumptions.<a href="#fnref49" class="footnote-back">↩</a></p></li>
<li id="fn50"><p>Mostly equal – there are 146 or 147 observations summarized in each row.<a href="#fnref50" class="footnote-back">↩</a></p></li>
<li id="fn51"><p>Not shown here, but it may also be valuable to review your coverage levels across key variables of interest.<a href="#fnref51" class="footnote-back">↩</a></p></li>
<li id="fn52"><p>More explicitly, this test shows that the distribution of <code>price_grouped by quintile</code> across <code>covered</code> would only occur ~3.4% of the time under the assumption of “no relationship”. This is very unlikely, so we reject the NULL hypothesis of “no relationship”.<a href="#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>Alternatively could just have evaluated the interval widths in the <code>log(Sale_Price, 10)</code> terms outputted by the model.<a href="#fnref53" class="footnote-back">↩</a></p></li>
<li id="fn54"><p>Varying on average less than half a percent between observations.<a href="#fnref54" class="footnote-back">↩</a></p></li>
<li id="fn55"><p>Methods in follow-up posts will show greater diversity of interval widths.<a href="#fnref55" class="footnote-back">↩</a></p></li>
<li id="fn56"><p>While the seesaw analogy does make a difference in causing some observations to have wider intervals than others, in aggregate you can see the difference is not <em>that</em> big – values like the Mean Squared Error (MSE) or similar aggregate error metrics actually give a pretty close semblance of what you will expect to get when going through the process of creating prediction intervals specific to each observation. Note also however that the difference in interval width will still be greater for the more extreme observations. In future posts some of the methods I walk through will be more flexible or have other advantages (e.g. less strict reliance on distributional assumptions).<a href="#fnref56" class="footnote-back">↩</a></p></li>
<li id="fn57"><p>Though linear models are less likely to overfit compared to most model types.<a href="#fnref57" class="footnote-back">↩</a></p></li>
</ol>
</div>
