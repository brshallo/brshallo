---
title: Pricing insights from historical data (part 1)
author: Bryan Shalloway
date: '2020-08-11'
slug: pricing-insights-from-historical-data-part-1
categories:
  - rstats
  - r
  - programming
  - business
tags:
  - rstats
  - r
---


>"All models are wrong, some models are useful." -"Tukey"

Setting prices for large complicated products is challenging, particularly in [business to business](link to helpful resource) (B2B) contexts. B2B sellers may lack information, e.g. accurate 'street' prices for the various configurations of their products. However they typically *do have* historical data on internal sales transactions as well as leadership with a strong desire for *insights* into pricing behavior. The [Pricing challenges] section in the [Appendix] discusses potential internal and external limitations -- in this post I'll put aside the question of *how* to set prices. Instead, I’ll walk through some straight forward, statistical methods you can use to evaluate differences, trends, and abnormalities in your organizations pricing that rely only on historical sales information.

With internal data^[Internal sales data alone is limited in that its focused on only a component of sales, rather than considering the full picture -- this puts the analyst in a familiar position of one with incomplete information, and a constrained scope of influence.] you can still support answers to many important questions and provide a starting place towards more sophisticated pricing strategies or analyses. I will tackle some potential questions:

* Which factors influence price? What is the magnitude of the influence?
* How have these factors changed over time?
* Which customers seem to fall outside the 'normal' behavior in regard to the price they are receiving?

For my examples, I will use data from the Ames, Iowa housing market. See [Dataset considerations] in the [Appendix] for why I chose it as well as problems with using the [ames](link to make_ames function) dataset as an analogue for B2B selling/pricing scenarios. My examples were built using the R programming language, you can find the source code at my [github] page.

# What influeces price?

**Dollars per unit:**

Products have features. These features^[Dataset should be structured such that each feature is a column and each row an observation, e.g. a sale] can be used to train a model to estimate price. For a linear model, the outputted coefficents associated with these features can be thought of as pseudo-proxies for the expected *dollar per unit* value associated with the component ([ceteris paribus](https://en.wikipedia.org/wiki/Ceteris_paribus)). Let's build a simple model for home price that uses *just* square footage as a feature for predicting price paid. 

The model comes out to: ... (use package that converts `stats::lm` model output into latex). The coefficeint on sale price is (kind of) a measure of expected dollars per square foot. If you build the model without an intercept^[I.e. make it zero], the coefficient more directly represents dollar / square foot (would get a coefficient / $ per square foot of __). However it's *typically* more appropriate to [leave the intercept in the model](link to SO post with evidence).

**Model assumptions, robust methods**:

Linear regression has a variety of [model assumptions](link to assumptions of linear regression), if you are building a model primarily for prediction, you do not need to worry about these as much^[At least to the extent that satisfying them doesn't improve your predictions.]. However if you are building the model to perform inference (e.g. trying to interpret the coefficients as representing the value-add of different components of a product) these assumptions matter more. For our simple model we can see we break the assumption of __.

__graph checking model assumptions for our simple model__

Often you will perform transformations on your variables that aide you in satsifying these assumptions -- however doing such changes the interpretation of your coefficients^[E.g. a log transform on an input changes the interpretation of the coefficient to be something closer to dollar per percentage change of input.] You may also simply use a linear model algorithm whose estimates are [robust](link to definition of robust) under violations of these assumptions. Using a robust linear estimator, our coefficient is: ___ .

**Multiple variables and the tug-of-war between collinear inputs**:

Let's add to our original model a new variable: number of bathrooms (I will continue to use a robust estimator). 

__Figure of model coefficients with new variables added__

Notice that the coefficient on square footage has decreased -- this is because number of bathrooms and square footage of house are highly correlated (correlation of ...). The new model parameters are estimated while taking each of these factors into account. Some of the impact that previously existed entirely in the coefficient on square footage is now shared between these two coefficients. At its extreme, this can lead to potentially unintuitive results. For example, if we add another variable: `number of bedrooms`, we see the estimate for the coefficient (which is also highly correlated with the existing model inputs) is actually negative. __MODEL WITH THREE VARIABLES__ This doesn't mean houses with more bedrooms are associated with negative house prices, though it suggests that a house with the same square footage and number of bathrooms will be less expensive if it has more bedrooms (perhaps representing a preference for larger rooms or open spaces). 

Standard linear regression models feature this tug-of-war like optimization whereby correlated variables share (/ battle over) a general influence on the model. This can cause variable coefficients that would, in a univariable context be similar, seem to move in opposing directions when considered together in multivariable contexts. In pricing analysis exercises (where you care about the individual parameter estimates and want them to be meaningful) this behavior may be desirable, but it has potential drawbacks^[It depends :-/]:

* As you increase the number of variables in the model, this effect can make for models with a high degree of instability / variance in the values of the coefficients -- meaning that the coefficients in your model can change dramatically even from small changes in the data^[This is what "variance" means in the bias-variance trade-off in model development.]
* You may want to limit models with unintuitive variable relationships (e.g. where highly related factors have coefficients that seem to act in opposing directions).

Two common approaches:

* *Regularization*: puts constraints on the linear model that discourage high levels of variance in your coefficient estimates. See the section [Regularization and colinear variables] for a more full discussion on this.
* *Bayesian approaches*: can use [priors](link to definition of priors in linear model) and rigorous estimation procedures to limit  [overfitting](link to overfitting) and subdue extreme estimates

Each will modulate coefficient estimates differently in conditions of multicolinearity of model variables. The model below shows the coefficient estimates for each of several model types^[Variables are usually be standardized before applying regularization. If our goal is interpretting the coefficient estimates, we may need to transform the coefficients for interpretation purposes.].

__plot of coefficient estimates from different models__

Note differences...

# How have influential factors changed over time?

If you want to evaluate a general impact of year / time on price, you could simply include it as a variable in the model and see the associated variable. If you want to evaluate how the influence of factors has *changed* over time however you could simply build models between different years and compare the coefficient estimates.

The chart below shows coefficient estimates for variables ... across ... years in the data. I used method ... to build the model.

__Chart of coefficients for each year__

Note that... estimates have changed... across years. To evaluate if a variables change is significant between two years we could use a statistical test (link to SO article that describes methodology for comparing if difference between coefficients is statistically signficant). 

To more rigorously evaluate which variables are trending up or down in their influence on price we could build a linear regression model of the coefficient estimates against year and compare the magnitude and/or significance of the change in coefficients across years. 

__Chart of trend of each variable up or down over time__

...

# Identify abnormalities / abnormalities?

Models can also be used to identify anomalies in sales by identifying individual sales that deviate significantly from expected sale value. This outlier detection can be useful for identifying methods to further improve the model (i.e. which piece(s) of information would explain the outliers in price you are seeing) or for identifying customer outliers that may deviate from norms in ways worthy of investigation.

...

# Appendix

## Pricing challenges

Final price paid by a customer may vary substantially within a given product. This variability is often due in part to a high degree of complexity inherent in the product and different configurations between customers^[A variety of factors though push organizations to simplify their products and this process -- for the purposes of this post though, I'll assume a complicated product portfolio.]. Fluctuations in product demand and macroeconomic factors are other important influences, as are factors associated with the buyer’s / seller’s negotiation skill and ability to use their brand and market information to leverage a higher or lower discount. 

The final price paid may also be influenced by a myriad of competing internal interests. Sales representatives may want leniency in price guidelines so they can hit their quota. Leadership may be concerned about potential brand erosion that often comes with lowering prices. Equity holders may be focused on immediate profitability or may be willing to sacrifice margin in order to expand market share. Effectively setting price guidelines requires the application of various economic, mathematical, and sociological principles^[For a more full discussion on these concepts see UVA coursera specialization on [Cost and Economics in Pricing Strategy]( https://www.coursera.org/learn/uva-darden-bcg-pricing-strategy-cost-economics?utm_source=gg&utm_medium=sem&utm_content=01-CourseraCatalog-DSA-US&campaignid=9918777773&adgroupid=102058276958&device=c&keyword=&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=434544785640&hide_mobile_promo=&gclid=CjwKCAjwsan5BRAOEiwALzomXyDwos6rlUmAwFrv9BjJFUPnyvzPRedArpRD2iRkocMemgtsZrfihxoCjfUQAvD_BwE).] which may not be feasible to set-up^[Organizations may lack the money or the will.]. Implementation of which requires reliable data, which could be lacking due to:

* Market information may be inaccurate or unavailable^[Maybe your company doesn’t want to pay the expensive prices that data vendors set for this information (this may especially be a problem if you are a small organization with a small budget).]. 
* Total costs of production may not be accessible.
* Current organizational goals may not be well defined.
* Information on deals closed may be more reliable than information on missed deals

These (or a host of other gaps in information) may make it difficult to define an objective function for identifying optimal price guidelines. 

## Dataset considerations

The relevant qualities of a dataset for the analysis in this post were:

1. Multiple years of data
1. Many features, with a few key variables associated with a large proportion of the variance

The `ames` housing dataset meets these qualifications and i was already familiar with it. Evaluating home prices can serve as a practical analogue for our problem -- both home sales and business to business sales often represent large purchases with many features influencing price. You can pretend that individual rows represent B2B transactions for a large corporation selling a complicated product line (rather than individual home sales).

There are many important differences between home sales and B2B sales that make this a poor analagoue. To name a few:

* in B2B contexts, repeat sales are typically more important than initial sales. In the housing market, repeat sales don't exist.
* information on home prices and prior home sales is accessible to both the buyer and seller -- meaning there are no options for targeted pricing
* in B2B contexts, an influential buyer may be able to leverage their brand name^[While a home seller may be more sympathetic to some buyers over others (E.g. a newly wedded couple looking to start a family over a real-estate mogul looking for investment properties), such preferences likely impact price less than the analogue in the B2B contexts where sellers seek to strike details with popular brands as means of establishing product relevance and enabling further marketing and potentially collaboration opportunities.]
* Volume selling schemes and other pricing strategies may have less of an impact on house prices compared to in B2B settings

Though for the notes in this post, this didn't matter.

# Regularization and colinear variables

Regularization typically comes in two flavors, either an L1 penalty (lasso regression) or L2 penalty (ridge regression), or some combination of these (elastic net) is applied to the linear model. These penalties provides a cost for larger coefficient which acts to decrease the variance in our estimates[^1]. In conditions of colinear inputs, these two penalties act differently on coefficient estimates of colinear features: 

* Lasso regression tends to choose a 'best' variable (among a subset of colinear variables) whose coefficient 'survives', while the other associated variables' coefficients are pushed towards zero
* For ridge regression, coefficients of similar variables gravitate to a similar value

[^1]: In both cases non-informative regressors will tend towards zero (in the case of ridge regression, they will never *quite* reach zero). These approaches typically require tuning to identify the ideal weight (i.e. pressure) assigned to the penalty. 