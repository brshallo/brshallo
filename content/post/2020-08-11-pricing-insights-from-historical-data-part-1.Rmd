---
title: Evaluating Product Prices (Part 1)
author: Bryan Shalloway
date: '2020-08-11'
categories:
  - business
  - programming
  - r
  - rstats
tags:
  - r
  - rstats
slug: pricing-insights-from-historical-data-part-1
---


>"All models are wrong, but some are useful." -George Box

# Problem context

Setting prices for large complicated products is challenging, particularly in [business to business](link to helpful resource) (B2B) contexts. B2B sellers may lack information, e.g. accurate estimates of customer budget or 'street' prices for the various configurations of their products. However they typically *do have* historical data on internal sales transactions as well as leadership with a strong desire for *insights* into pricing behavior. The [Pricing challenges] section discusses potential internal and external limitations in setting prices. For now I'll put aside the question of *how* to set prices. Instead, Iâ€™ll walk through some statistical methods that rely only on historical sales information and that can be used to evaluate differences, trends, and abnormalities in your organizations pricing.

With internal data^[Internal sales data alone is limited in that its focused on only a component of sales, rather than considering the full picture -- this puts the analyst in a familiar position of one with incomplete information, and a constrained scope of influence.] you can still support answers to many important questions and provide a starting place towards more sophisticated pricing strategies or analyses. In a series of posts I will tackle the following questions:

1. How do differences in product components associate with differenes in price? What is the magnitude of the influence of these factors?
1. How have these factors changed over time?
1. Which customers fall outside the 'normal' behavior in regard to the price they are receiving?
1. How can complexities in pricing strategy be captured by a statistically rigorous modeling framework? (E.g. when volume dictates price)

In this post, I will focus on question one. For my examples, I will use data from the Ames, Iowa housing market. See the [Dataset considerations] section for why I use the [ames](link to make_ames function) dataset as an analogue for B2B selling / pricing scenarios (as well as problems with this choice). My examples were built using the R programming language, you can find the source code at [my github]() page.

# What influeces price?

**Dollars per unit:**

Products have features. These features^[Dataset should be structured such that each feature is a column and each row an observation, e.g. a sale] can be used to train a model to estimate price. For a linear model, the outputted coefficents associated with these features can (sort of) act as proxies for the expected *dollar per unit* change associated with the component ([ceteris paribus](https://en.wikipedia.org/wiki/Ceteris_paribus)). 

```{r}
library(tidymodels)
library(tidyverse)
library(AmesHousing)
```

```{r}
ames <- AmesHousing::make_ames()

ames <- ames %>% 
  mutate(bathrooms = Full_Bath + 0.5 * Half_Bath,
         total_sqft = Gr_Liv_Area + Bsmt_Unf_SF)
```

Let's build a simple model for home price that uses *just* house square footage (not including basement), represented by `Gr_Liv_Area` as a feature for predicting home price. 

```{r simple-model-equation, results = 'asis'}
mod_sqft <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)

mod_sqft_noint <- lm(Sale_Price ~ Gr_Liv_Area - 1, data = ames)

mod_gr_latex <- equatiomatic::extract_eq(mod_sqft, use_coefs = TRUE, coef_digits = 1)

mod_gr_latex
```

The coefficient on sale price of *111.7* is a measure of expected dollars per unit change in square foot. If you build the model without an intercept^[I.e. make it zero so that the expected value of a house of 0 square foot is $0], the coefficient more directly equates to dollars per square foot^[In this case, the coefficient for the model becomes 119.7 if the intercept is set to zero.]. However it's *almost always* more appropriate to [leave the intercept in the model](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model).

## Model assumptions, robust methods

Linear regression has a variety of [model assumptions](https://en.wikipedia.org/wiki/Linear_regression#Assumptions). Following these is less important when using the model for predictions compared to making inferences^[At least to the extent that satisfying them doesn't improve your predictions, or suggest a different model type may be more appropriate.]. If you are interpretting the coefficients as representions of the value-add associated with components of a product, model assumptions matter. 

Transformations are often applied to the data to help satsify these assumptions and minimize the impact of outliers and influential points However (important in the pricing analysis context) such modification alters the meaning of the coefficients^[E.g. a log transform on an input changes the interpretation of the coefficient to be something closer to dollar per percentage change of input.]. The analyst may also opt to use an optimization algorithm that produces estimates [robust](https://en.wikipedia.org/wiki/Robust_regression) to influential points and violations of model assumptions. Using robust regression, our model is:

```{r}
mod_sqft_robust <- MASS::rlm(Sale_Price ~ Gr_Liv_Area, data = ames)
```

$$
\operatorname{Sale\_Price} = 15500.4 + 108.1(\operatorname{Gr\_Liv\_Area}) + \epsilon
$$

## The tug-of-war between colinear inputs

Let's add to our (robust regression) model a new variable: number of bathrooms represented by the `bathrooms` variable. 

```{r}
mod_2_robust <- MASS::rlm(Sale_Price ~ Gr_Liv_Area + bathrooms, data = ames)

ames %>% 
  select(Gr_Liv_Area, bathrooms) %>% 
  cor()
```

$$
\operatorname{Sale\_Price} = 12147 + 87.35(\operatorname{Gr\_Liv\_Area}) + 19178.3(\operatorname{bathrooms}) + \epsilon
$$

Notice the coefficient on square footage has decreased -- this is because number of bathrooms and square feet of home are highly correlated (they have a [correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of 0.71). The new estimates are created while taking more factors into account. Some of the impact on home price that previously existed entirely in the coefficient for `Gr_Liv_Area` is now shared with the highly related `bathrooms` variable. This phenomena can lead to unintuitive results.

```{r}
MASS::rlm(Sale_Price ~ Gr_Liv_Area + bathrooms + Bedroom_AbvGr, data = ames)

MASS::rlm(Sale_Price ~ Bedroom_AbvGr, data = ames) %>% 
  summary()

ames %>% 
  dplyr::select(where(is.numeric)) %>% 
  corrr::correlate() %>% 
  corrr::rearrange() %>% 
  corrr::rplot()+
  scale_x_discrete(guide = guide_axis(angle = 90))

library(corrr)
ames %>% 
  dplyr::select(contains(c("SF", "Area"))) %>% 
  corrr::correlate() %>% 
  corrr::rearrange(method = "HC") %>%
  corrr::rplot()+
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

For example, let's consider another variable: `Bedroom_AbvGr` which represents the number of bedrooms in the home (excluding those in the basement). If we build a robust simple linear regression model, we see there is a 

e see the estimate for the coefficient (which is also highly correlated with `number of bathrooms` and `square feet`) is actually negative. __MODEL WITH THREE VARIABLES__ This doesn't mean houses with more bedrooms are associated with negative house prices. Though it suggests a house with the same square footage and number of bathrooms will be less expensive if it has more bedrooms (perhaps representing a preference for larger rooms / open space among buyers). 

Typical linear regression models feature this tug-of-war like optimization whereby correlated variables share (/ battle over) a general influence on the model. At times this causes similar variable to seem to have opposing impacts. When evaluating coefficients for pricing analysis exercises^[Where you care about the individual parameter estimates and want them to be meaningful.] this behavior has potential drawbacks^[This may also be beneficial -- it depends :-/]:

* As you increase the number of variables in the model, this effect can make for models with a high degree of instability / variance in the parameter estimates -- meaning that the coefficients in your model could change dramatically even from small changes in the training data^[This is what "variance" means in the bias-variance trade-off common in model development.]
* You may want to limit methods that result in models with unintuitive variable relationships (like those discussed where highly related factors have coefficients that appear to act in opposing directions).

### Remedies

Two potential approaches:

* *Regularization*: puts constraints on the linear model that discourage high levels of variance in your coefficient estimates. See the section [Regularization and colinear variables] for a more full discussion on how L1 & L2 penalties affect estimates for colinear model inputs.
* *Bayesian approaches*: can use [priors](link to definition of priors in linear model) and rigorous estimation procedures to limit  [overfitting](link to overfitting) and subdue extreme estimates.

Each will modulate coefficient estimates differently. The figure below shows the coefficient estimates when using each of these methods.

__plot of coefficient estimates from different models__

Note differences...

I typically do...


## Significance vs effect size

Look for variables that are important but not overly colinear with house size. E.g. lot size.


# Closing note

You can use regression models to evaluate the impact of different factors on price. It is important to consider how coefficient estimates will respond to violations of your model assumptions, particularly those of multi-colinearity and to use estimation techniques that are appropriate for your problem. 

If you have questions or feedback feel free to direct message me at [brshallo](link). I will cover the other questions noted in [Problem Context] in a series of upcoming posts.

# How have influential factors changed over time?

If you want to evaluate a general impact of year / time on price, you could simply include it as a variable in the model and see the associated variable. If you want to evaluate how the influence of factors has *changed* over time however you could simply build models between different years and compare the coefficient estimates.

The chart below shows coefficient estimates for variables ... across ... years in the data. I used method ... to build the model.

Have something about function for modulating further back items on pricing...

__Chart of coefficients for each year__

Note that... estimates have changed... across years. To evaluate if a variables change is significant between two years we could use a statistical test (link to SO article that describes methodology for comparing if difference between coefficients is statistically signficant). 

To more rigorously evaluate which variables are trending up or down in their influence on price we could build a linear regression model of the coefficient estimates against year and compare the magnitude and/or significance of the change in coefficients across years. 

__Chart of trend of each variable up or down over time__

...

# Identify abnormalities / abnormalities?

Models can also be used to identify anomalies in sales by identifying individual sales that deviate significantly from expected sale value. This outlier detection can be useful for identifying methods to further improve the model (i.e. which piece(s) of information would explain the outliers in price you are seeing) or for identifying customer outliers that may deviate from norms in ways worthy of investigation.

...

# Appendix

## Pricing challenges

Final price paid by a customer may vary substantially within a given product. This variability is often due in part to a high degree of complexity inherent in the product and different configurations between customers^[A variety of factors though push organizations to simplify their products and this process -- for the purposes of this post though, I'll assume a complicated product portfolio.]. Fluctuations in product demand and macroeconomic factors are other important influences, as are factors associated with the buyerâ€™s / sellerâ€™s negotiation skill and ability to use their brand and market information to leverage a higher or lower discount. 

The final price paid may also be influenced by a myriad of competing internal interests. Sales representatives may want leniency in price guidelines so they can hit their quota. Leadership may be concerned about potential brand erosion that often comes with lowering prices. Equity holders may be focused on immediate profitability or may be willing to sacrifice margin in order to expand market share. Effectively setting price guidelines requires the application of various economic, mathematical, and sociological principles^[For a more full discussion on these concepts see UVA coursera specialization on [Cost and Economics in Pricing Strategy]( https://www.coursera.org/learn/uva-darden-bcg-pricing-strategy-cost-economics?utm_source=gg&utm_medium=sem&utm_content=01-CourseraCatalog-DSA-US&campaignid=9918777773&adgroupid=102058276958&device=c&keyword=&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=434544785640&hide_mobile_promo=&gclid=CjwKCAjwsan5BRAOEiwALzomXyDwos6rlUmAwFrv9BjJFUPnyvzPRedArpRD2iRkocMemgtsZrfihxoCjfUQAvD_BwE).] which may not be feasible to set-up^[Organizations may lack the money or the will.]. Implementation of which requires reliable data, which could be lacking due to:

* Market information may be inaccurate or unavailable^[Maybe your company doesnâ€™t want to pay the expensive prices that data vendors set for this information (this may especially be a problem if you are a small organization with a small budget).]. 
* Total costs of production may not be accessible.
* Current organizational goals may not be well defined.
* Information on deals closed may be more reliable than information on missed deals

These (or a host of other gaps in information) may make it difficult to define an objective function for identifying optimal price guidelines. 

## Dataset considerations

The relevant qualities of a dataset for the analysis in this post were:

1. Multiple years of data
1. Many features, with a few key variables associated with a large proportion of the variance

The `ames` housing dataset meets these qualifications and i was already familiar with it. Evaluating home prices can serve as a practical analogue for our problem -- both home sales and business to business sales often represent large purchases with many features influencing price. You can pretend that individual rows represent B2B transactions for a large corporation selling a complicated product line (rather than individual home sales).

There are many important differences between home sales and B2B sales that make this a poor analagoue. To name a few:

* in B2B contexts, repeat sales are typically more important than initial sales. In the housing market, repeat sales don't exist.
* information on home prices and prior home sales is accessible to both the buyer and seller -- meaning there are no options for targeted pricing
* in B2B contexts, an influential buyer may be able to leverage their brand name^[While a home seller may be more sympathetic to some buyers over others (E.g. a newly wedded couple looking to start a family over a real-estate mogul looking for investment properties), such preferences likely impact price less than the analogue in the B2B contexts where sellers seek to strike details with popular brands as means of establishing product relevance and enabling further marketing and potentially collaboration opportunities.]
* Volume selling schemes and other pricing strategies may have less of an impact on house prices compared to in B2B settings

Though for the notes in this post, this didn't matter.

## Regularization and colinear variables

Regularization typically comes in two flavors, either an L1 penalty (lasso regression) or L2 penalty (ridge regression), or some combination of these (elastic net) is applied to the linear model. These penalties provides a cost for larger coefficient which acts to decrease the variance in our estimates[^1]. In conditions of colinear inputs, these two penalties act differently on coefficient estimates of colinear features: 

* Lasso regression tends to choose a 'best' variable (among a subset of colinear variables) whose coefficient 'survives', while the other associated variables' coefficients are pushed towards zero
* For ridge regression, coefficients of similar variables gravitate to a similar value

[^1]: In both cases non-informative regressors will tend towards zero (in the case of ridge regression, they will never *quite* reach zero). These approaches typically require tuning to identify the ideal weight (i.e. pressure) assigned to the penalty. 

## Coefficients of a regularized model

Variable inputs are usually standardized before applying regularization. Hence because inputs are all (essentially) put on the same scale, the coefficient estimates can be directly compared with one another as measures of their relative influence on the target (home price). This ease of comparison may be convenient, though if our goal is interpretting the coefficient estimates in terms of dollar change per unit increase, we may need to transform the coefficients. This is what was done in figure ... by ...