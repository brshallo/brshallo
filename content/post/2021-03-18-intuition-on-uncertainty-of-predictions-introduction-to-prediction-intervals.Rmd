---
title: Intuitions on Prediction Intervals
author: Bryan Shalloway
date: '2021-03-18'
slug: intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals
thumbnail: /post/2021-03-18-intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals_files/figure-html/prediction-intervals-sample-1.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE)
```

**TLDR:** Prediction intervals can be a helpful indicator for understanding your model and for evaluating potential observations. Prediction Intervals can be thought of as a mix of uncertainty in estimating the expected value and uncertainty in the sample. Measures such as coverage and average interval width are helpful ways to evaluate your prediction intervals.

This post... 

* builds up to a potential motivating example for using prediction intervals
* describes how to think about prediction intervals
* shows how to build and then review prediction intervals

I use the R programming language and the [tidyverse](https://www.tidyverse.org/) + [tidymodels](https://www.tidymodels.org/) suite of packages to create all models and figures. Similar to a [previous post on pricing](https://www.bryanshalloway.com/2020/08/17/pricing-insights-from-historical-data-part-1/), I use the [AmesHousing](https://github.com/topepo/AmesHousing) data for my examples.

# Providing More Than Point Estimates

Imagine you are an analyst for a business to business (B2B) seller and are responsible for identifying appropriate prices for complicated products with non-standard prices^[Non-standard, negotiated prices are common in the B2B world.]. If you have more than one or two variables that influence price, then a statistical or machine learning model is a helpful approach for determining the optimal way to combine features to pinpoint expected prices for future deals[^analysts] (of course margin, market positioning, and other business considerations also matter). 

[^analysts]: Analysts are sometimes tempted to compare the specific deal of interest against *only* historical deals that have similar attributes. The problem with attempting to do this is that, when you filter you lose data. After filtering across more than a couple dimensions, you likely will only have a few deals left in your sub-segment -- too few to get a reliable sense of price variability. Prediction intervals offer a method for determining the variability of your estimates that spans across the multi-dimensional space that your data inhabits.  
    
    That said there are many situations where it does make sense to segment your data. Also the errors in your model should be independent from one another. During residual analysis you should ensure that there is not any bias across particular dimensions or ranges of data. Hence looking at particular segments of observations can be important -- it's just important to do so thoughtfully.
    
    I've seen many occassions when an analyst keeps slicing their data and reviewing univariate relationships until they've convinced themselves they've found something -- even when the sample sizes at this point are small and the nature of the relationship unconvincing.

You might first build a model of *expected sale prices* that is fit based on the patterns in historical sales data. The naive approach would be to use predictions from this model as a reference point. If a customer's offer falls above the "expected" price, you might interpret this is a "good" deal. If the offer falls "below" the predicted price you might be tempted to view the deal as "bad." 

As a stand-in for the large complicated products typically sold in B2B markets, I will use for my examples home sales from Ames, Iowa . 

With the code below I load in the data and split it into a training dataset (for exploration and model development) and validation dataset (holdout data not used in model training but reserved for evaluating model performance)^[If I was being a little more careful, I'd also split the `train` data into a validation set or I'd use `rsample` to set-up cross validation. In this post though I largely use `test` how you might use a validation set though. See [Tidy Modeling with R, 10.2](https://www.tmwr.org/resampling.html) for more.]. 

```{r load-data-packages}
library(tidyverse)
library(tidymodels)
library(AmesHousing)


ames <- make_ames() %>% 
  mutate(Years_Old = Year_Sold - Year_Built,
         Years_Old = ifelse(Years_Old < 0, 0, Years_Old))

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price", p = 0.75)

ames_train <- training(data_split)
ames_holdout  <- testing(data_split)
```

Specify pre-processing and workflow to build a relatively simple multiple linear regression model to predict `Sale_Price` -- actually $\log_{10}{}Sale\:Price$[^1]. (Note that I am skipping *a lot of important steps* you typically would take when building predictive models^[E.g. data exploration, review of model assumptions, use of a separate validation set, hyperparameter tuning, etc.]).

[^1]: A log transform on the target can be thought of roughly as changing the model tobe about changes in percent of `Sale_Price` rather than raw `Sale_Price`. Throughout the post I will be performing a log transformation on our target, `Sale_Price` prior to modeling. Hence in some cases I make comparisons against the log of dollars offered OR I will use some metric that is in terms of *percent* rather than raw `Sale_Price`. It is generally best to conduct model evaluation against whatever transformed scale is used in model building (i.e. in our case `log(Sale_Price, 10)`). This is suggested in [Tidymodels with R](https://www.tmwr.org/performance.html):
    
    > "It is best practice to analyze the predictions on the transformed scale (if one were used) even if the predictions are reported using the original units." -Kuhn, Silge    
    
    This may not apply perfectly in our case (as we are talking more about evaluation steps that would likely happen after initial model building and selection and has more to do with particular predictions)... however we will still keep this in mind and try to mindful of how we present our evaluation metrics.


```{r lin-rec-mod}
lm_recipe <- 
  recipe(
    Sale_Price ~ Lot_Area + Neighborhood  + Years_Old + Gr_Liv_Area + Overall_Qual + Total_Bsmt_SF + Garage_Area, 
    data = ames_train
  ) %>%
  step_log(Sale_Price, base = 10) %>%
  step_log(Lot_Area, Gr_Liv_Area, base = 10) %>%
  step_log(Total_Bsmt_SF, Garage_Area, base = 10, offset = 1) %>%
  step_novel(Neighborhood, Overall_Qual) %>% 
  step_other(Neighborhood, Overall_Qual, threshold = 50) %>% 
  step_dummy(Neighborhood, Overall_Qual) %>%
  step_interact(terms = ~contains("Neighborhood")*Lot_Area)

lm_mod <- linear_reg() %>% 
  set_engine(engine = "lm") %>%
  set_mode("regression")

lm_wf <- workflows::workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(lm_recipe) %>% 
  fit(ames_train)
```

Then make predictions on the holdout set. 

```{r}
data_preds <- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %>% bake(ames_holdout)
) %>% 
  bind_cols(relocate(ames_holdout, Sale_Price)) %>% 
  mutate(.pred = 10^.pred) %>%
  select(Sale_Price, .pred, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)

```

The one-row table below shows that given the case of 

* `Lot_Area` = 31,770sqft ;
* `Neighborhood` = North Ames; 
* `Years_Old` = 50yrs; 
* `Gr_Liv_Area`: 1,656sqft; 
* `overall_Qual` = "Above Average"; 
* `Total_Bsmt_SF` = 1,080sqft; 
* `Garage_Area` = 528sqft, 

the model would predict the `Sale_Price` for the home to be $184,503.

```{r}
data_preds %>% 
  select(-Sale_Price) %>% 
  head(1) %>% 
  knitr::kable()
```

Whether an offer is above or below this *expected* price may provide a first indication of whether the deal is 'good' or 'bad' for the seller^[Relative to prior deals which the model was trained on.]. The magnitude of the difference between the offer and the expected price might also be important. However you often will care about how big this difference is relative to the underlying *uncertainty* in `Sale_Price` on the given observation -- which calls for the use of statistics, specifically predictive inference. Statistics can help to provide a sense of the accuracy of the model and the extent to which an offer seems reasonable in the context of the variability in prices^[When you are dealing with problems that consider both magnitude and variability, statitistics are often helpful.].

# Considering Uncertainty

For example, Let's consider an offer of \$180,000 for the example house mentioned above. You might first think you ought to reject the offer due to it being ~\$4,500 less than the expected price. However statistics can provide context as to whether this represents a *small* or *large* deviation from expectations. A *less naive* approach would be to compare the \$180,000 offer in the context of the model's observed accuracy when making predictions on a holdout set. 

We need to be careful regarding how we consider performance and the errors of our model. For this particular problem, the variability in our model's errors increase with the magnitude of the predictions for `Sale_Price`^[This is known as heteroskedasticity. We generally want our error metrics to be homoskedastic, i.e. 'same variance'. This is why we built the model on the target `log(Sale_Price, 10)`. Note that the base rate of 10 doesn't really matter that much, we could just have easily have built the model using hte natural logarithm and gotten similar results.], We want an error metric that will not inflate with `Sale_Price`, hence rather than review the difference between `Sale_Price` and Expected[`Sale_Price`], it is more appropriate to review either:

* $\log_{10}Sale_Price$ against Expected[$\log_{10}Sale_Price$] OR
* an error metric that is in terms of percent

For simplicity of explanation, I will use the latter method and focus on the percent error^[During model building and evaluation the former would often be more appropriate to review. In particular the root mean squared error (RMSE) of the errors of the predictions for `log(Sale_Price)`.]. We see the offer of $180,000 -- representing a 2.5% deviation from expectations -- falls well within the distribution of typical errors seen by our model.

```{r}
offer <- tibble(PE = (184503 - 180000) / 180000, offer = "offer")

data_preds %>% 
  mutate(PE = (Sale_Price - .pred) / Sale_Price) %>% 
  relocate(PE) %>% 
  ggplot(aes(x = PE))+
  geom_histogram(bins = 50)+
  geom_vline(aes(xintercept = PE, color = offer), data = offer)+
  scale_x_continuous(limits = c(-1, 1), labels = scales::percent)+
  labs(x = "Percent Error")+
  theme_bw()
```

On average our model is off by around ~12% of the actual `Sale_Price` observed. This estimate is based on evaluation on our holdout dataset (data not seen during model training and reserved for the purpose of model evaluation).

```{r}
data_preds %>% 
  yardstick::mape(Sale_Price, .pred) %>% 
  knitr::kable()
```

90% of errors on the holdout dataset were between -27.1% and 21.3% of the actual `Sale_Price`. 

```{r}
data_preds %>% 
  mutate(PE = (Sale_Price - .pred) / Sale_Price,
         APE = abs(PE)) %>% 
  summarise(quant_05 = quantile(PE, 0.05),
            quant_95 = quantile(PE, 0.95)) %>% 
  knitr::kable()
```

All of which suggests th3 \$180,000 offer does not represent a substantial outlier from the typical variability of *actual* from *predicted* `Sale_Price`^[Based on the obervation level estimates given by our model on a holdout set.].

**Is your model performant enough to be useful?**

Before using your model for predictive inference, you *should* have reviewed performance on a holdout set to ensure your model is sufficiently accurate for your business context. For example, in our problem, is an average error of ~12% and 90% prediction intervals of +/- ~25% of `Sale_Price` useful? If the answer is "no," that suggests the need for more effort in improving the accuracy of the model (e.g. trying other transformations, features, model types)^[Or a focus on other pieces of information outside of your model for making decisions.]. For our examples we are assuming the answer is 'yes,' our model is accurate enough, so that we can move-on to focus on prediction intervals. 

# Observation Specific Intervals

While a good starting point, a limitation with using aggregate error metrics to estimate intervals for *specific* observations is that individual cases may have differences in the uncertainty of predictions. Specifically, predictions further from the centroid of the data generally have more uncertainty in the expected price. 

A see-saw is a good analogy for this phenomenon: 

> As the angle of the bench changes, the further you are from the center, the more distance you will move up/down &rightarrow;  
> The angle of the seasaw represents variability in the model function being estimated &rightarrow;  
> The distance from the see-saw's pivot point corresponds with the distance from the centroid of the data &rightarrow;  
* The dashed blue bars below represent this variability in estimating the model's expected value. 

![Image [source](https://i.stack.imgur.com/GeeI3.png)](https://i.stack.imgur.com/GeeI3.png)

Because of this it is often preferable to determine a plausible price range by using a prediction interval that is specific to the attributes for the observation^[Rather than just looking at your observation of interest through the lens of aggregate error metrics.]. 

(Note that the uncertainty described above corresponds with the *confidence intervals* and not as directly with the *prediction intervals*. The distinction and relationship between these will be discussed in [A Few Things to Understand About Prediction Intervals].)

# Understanding Prediction Intervals

## Prediction Intervals and Confidence Intervals 

Prediction intervals and confidence intervals^[Or when using bayesian methods, credible intervals.] are often confused. Confidence intervals generally refer to making inferences on aggregates or *average* relationships -- this is most useful for evaluating parameter estimates, performance metrics, relationships with covariates, etc. However if you are interested in price ranges on *individual* observations (as in our case), prediction intervals are what you want^[Confidence intervals are more directly related to the *standard error* of your sample, while prediction intervals are associated with the *standard deviation* of the errors. The width of your confidence intervals follow the central limit theorem and are thus sensitive to the sample size of your data and become more narrow as you collect more observations (and you gain 'confidence' in where the 'true' expected value resides). For prediction intervals, more observations may improve your estimate of the model or give you more faith in your prediction intervals, however the width of the intervals will not necessarily shrink in any substantial way.]. Rob Hyndman has a helpful post where he describes the differences in more detail: [The difference between prediction intervals and confidence intervals](https://robjhyndman.com/hyndsight/intervals/)^[Confidence intervals are typically the default output of many packages and functions that output "intervals" -- [forecast](https://github.com/robjhyndman/forecast) and other forecasting packages though typically default to prediction intervals.].

In linear regression, "prediction intervals" refers to a type of [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval)^[Again, speaking through a frequentist lens.], namely the confidence interval for a single observation (a "predictive confidence interval"). "Confidence intervals" have a specific statistical interpretation... in later posts on this topic, the intervals I create do not quite mirror the interpretations that go with a predictive confidence interval. I will use the term "prediction interval" somewhat loosely to refer to a plausible range of values for an observation based on its attributes^["Predictive band" may be a more appropriate term.].

## Analytic Method of Calculating Prediction Intervals

Linear regression models can produce prediction intervals analytically. The equation below is for *simple* linear regression (meaning just one 'x' input) but is helpful for gaining an intuition on the key parts that contribute to the width of a prediction interval:

<p class="text-align-center">\(\hat{y}_h \pm t_{(1-\alpha/2, n-2)} \times \sqrt{MSE \times \left( 1+\dfrac{1}{n} + \dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}\)</p>

Pseudo-translation of key parts of equation that influence interval:

 > {$\hat{y}_h$ : prediction}  
 > +/- {$t_{(1-\alpha/2, n-2)}$ : multiplier for desired level of confidence, e.g. 95%}   
 > x   {MSE: multiplier for average error}   
 > x   {$\dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}$ : multiplier for distance from centroid of data}

You will notice the equation for the prediction interval is very similar to that of the equation for a confidence interval^[I.e. the band around the *average* value, rather than the band around the value for an *individual* observation -- as discussed in [A Few Things to Understand About Prediction Intervals].]:

<p class="text-align-center">\(\hat{y}_h \pm t_{(1-\alpha/2, n-2)} \times \sqrt{MSE \left(\dfrac{1}{n} + \dfrac{(x_h-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}\)</p>

The variance for the prediction interval just has an extra Mean Squared Error (MSE) term^[If this is unclear to you, try distributing the MSE term and look again. Penn State also has helpful lessons on regression available [publicly online](https://online.stat.psu.edu/stat501/lesson/3/3.3#paragraph--766).]. The prediction interval is essentially the variance in estimating the model^[The square root of which is used to calculate the confidence interval.] combined with the variability in the sample of individual obsevations. 

Paraphrasing, the uncertainty in predictions can be thought of as coming from two sources:

1. uncertainty in fitting the model, i.e. determining the *expected* value
2. uncertainty in the sample 

The first may vary depending on position relative to the centroid (think back to the see-saw analogyin [Observation Specific Intervals] and the importance of position in determining magnitude of change i.e. uncertainty). The latter part is assumed to be constant across observations^[This assumption of constant variability of the sample is common in many techniques for producing prediction intervals -- even some techniques that use simulation based approaches.]. 

Do not think of these sources of uncertainty as being compounded on one another to produce the prediction interval. They are added together^[To get the variance.] and then the square root is taken. In most cases the variability due to the sample will be greater than the variability in estimating the expected value. Hence, the interval's width due to uncertainty in estimating the model is somewhat 'watered down' by the variability in the sample. Therefore (for prediction intervals produced analytically at least) we will generally not see huge differences in the widths of prediction intervals across observations (even at points that are relatively far from the centroid of the data). Therefore our prediction intervals in later sections in this will actually not be that much different from those that might been produced using aggregate metrics similar to those discussed in [Considering Uncertainty].

However understanding the intuition on these sources of uncertainty in prediction intervals is helpful. We will see in follow-up posts that use more sophisticated methods for producing prediction intervals that we can get substantive levels of variability in prediction intervals between observations.

## Visual Comparison of Prediction Intervals and Confidence Intervals

The chart below shows the key parts of a generic linear model that concern prediction: 

* expected value, i.e. predictions (black line) 
* the confidence intervals, i.e. range for the expected values (red dashed line) 
* prediction intervals (our interest in this post), i.e. range for the observations (green dotted line) 

![Image [source](https://i1.wp.com/statistical-research.com/wp-content/uploads/2013/10/prediction_confidence.png?)](https://i1.wp.com/statistical-research.com/wp-content/uploads/2013/10/prediction_confidence.png?)

Confidence intervals are more narrow (as they concern only uncertainty in the model's estimation of the expected value). Prediction intervals are wider as they concern both model uncertainty and the sampling uncertainty of any observation (the latter of which contributes far greater variance). Both confidence and prediction intervals are wider the further the prediction is from the centroid of the data -- however the effect is far greater for the confidence interval (as the uncertainty in the prediction interval is dominated by the variance in observations, which is assumed to be constant across observations). This explains why the curve in the confidence intervals is far more pronounced.

## Are Prediction Intervals an Example of Inference or Prediction?

Predictive modeling is typically used either for making predictions or inferences. The assumptions associated with your model usually matter more when doing inference compared to prediction^[This is because your assumptions will influence the nature of the uncertainty in your model. When focused on prediction, you generally just care about minimizing error (in one form or another), assumptions regarding distributions of parameters, errors, etc. can often be ignored or at least relaxed.]. To get a sense of how careful you should be regarding model assumptions therefore you might ask: "Are prediction intervals an example of prediction or inference?"

The answer is, while it (kind of) depends...

<iframe width="560" height="315" src="https://www.youtube.com/embed/QRLEXAFOvkM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

*really*, prediction intervals should be thought of as a kind of inference. 

In this post the prediction interval is explicitly an inference on the predictions. In a later post on quantile regression I will actually just be making predictions of upper and lower bounds. However, generally, intervals should be thought of as a kind of inference^[Prediction intervals concern uncertainty, and whenever you are making estimates around uncertainty you are generally doing a kind of statistical inference.]. Therefore you should be thoughtful regarding the assumptions you are making about the nature of your data and model specification^[Even when using simulation based techniques that, for other estimates, may allow you to be a little more cavalier regarding model assumptions... when it comes to predictive inference you will essentially always have *some* assumptions you need to hold onto.]. 

For example, an important assumption is heterskedasticity of errors -- which means that the variability *in our observations* should be constant across our predictions. As mentioned previously, when building a model for *Sale Price* we would find that errors tend to increase with our predictions. Hence instead of building a model to predict *Sale Price* we build models to predict $\log_{10}Sale\:Price$ -- this helps ensure that our model's outputted prediction intervals will be more appropriate across observations.

In follow up posts the methods I introduce do not depend *as* strongly on assumptions to produce reliable prediction intervals^[However there are limitations on assumption free predictive inference, ([Barber, et al](https://www.stat.cmu.edu/~ryantibs/papers/limits.pdf)).].

## Cautions with overfitting

Note that if your model is [overfitting](https://en.wikipedia.org/wiki/Overfitting)^[Avoiding overfitting is a big topic I am going to just barely touch on it here.] the data, that the prediction interval will often underestimate the expected variability in price and your prediction intervals may be too narrow. Two strategies you might use to correct this:

1. Consider a more simple model (that still fits the data well)
1. Tune your prediction bands based on coverage on holdout data (coverage represents the proportion of observations that actually fall within their prediction intervals)^[I show an example of measuring coverage in [Checking Coverage].].

A quick heuristic to check if your model is overfitting is to compare the model's performance on a holdout dataset compared against performance on the data that was used to train the model. If performance is substantially better on the training dataset, it may suggest your model is overfitting.

**Simpler model**

If your model seems to be overfittng, you may want to try fitting a more simple model^[Lower [variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).]. If your model shows a small difference in performance between training and holdout datasets, you likely will have more faith in the ranges given in the prediction intervals.

**Problem with comparing difference in performance between training & holdout data**

However, for the most part, your performance is going to *always be better* on the training data than on the holdout data^[This is true of some models to a greater extent than others.]. With regard to overfitting, you really care about whether performance is becoming *worse* on the holdout set compared to an alternative simpler model's performance on the holdout set. You don't really care if performance on training and holdout data is similar, just that performance on a holdout dataset is as good as possible.  

Let's take an example when building random forest models and tuning on the `min_n` attribute (smaller values for `min_n`^[Representing the minimum number of observations in a node for it to be split.] represent more complicated models.):

![See my [gist](https://gist.github.com/brshallo/516e8e52fb911a96efafbf01d606a113) for code](https://camo.githubusercontent.com/c498111162b8f8d4fced8fe07f7b532fe2de3c4a66f68e29b9f29832148ab674/68747470733a2f2f692e696d6775722e636f6d2f316e53687836792e706e67)

The gap in train-holdout performance is greater for more complicated models^[When `min_n` is smaller.] (which, as mentioned above, may signal overfitting) -- however performance on the holdout data is also better. Hence, in this instance, we would likely want to select the most complicated model among these^[Again, when `min_n` is smallest.] *even though* the performance estimates on the training data are the most unrealistic.

While a difference in performance on training and holdout datasets may provide a helpful indicator for overfitting, it is not really what you care about during model selection^[This is likely the case even when you are planning on using the model to generate prediction intervals and not just point estimates.].

**Tune your bands based on the holdout set**

Let's say your selected model has a substantial difference in performance between the training & holdout data and you are worried that the *expected* width of your prediction intervals may be optimistically narrow, however you still want to keep this model because it has the best performance when evaluated on holdout data. An alternative to trying to fit a more simple model is to adjust your prediction bands, tuning them based on the coverage level on a holdout dataset. To describe this idea I will define three ways of thinking about coverage:

* target coverage: The level of coverage you want to attain on a holdout dataset (i.e. the proportion of observations you want to fall within your prediction intervals).
* expected coverage: The level of confidence in the model for the prediction intervals, e.g. asking the model to predict 90% prediction intervals.
* empirical coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used to train the model.

Ideally all three align. However, Let's consider a hypothetical example, when your model is overfit and you have a 90% target coverage. You might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 94%^[Tuning your alpha level until the coverage on a holdout dataset is where you want it.]. 

This is called "adaptive coverage" and is discussed briefly by Emmanuel Candes in the video below:

<iframe width="560" height="315" src="https://www.youtube.com/embed/61tpigfLHso?start=1654" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

In a __follow-up post__ I will walk through an example similar to the one above.

# Generalizability

Let's check to see how performance of our model compares between our training and holdout datasets.

```{r}
train_preds <- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %>% bake(ames_train)
) %>% 
  bind_cols(relocate(ames_train, Sale_Price)) %>% 
  mutate(.pred = 10^.pred)

bind_rows(
  yardstick::mape(train_preds, Sale_Price, .pred),
  yardstick::mape(data_preds, Sale_Price, .pred)
) %>% 
  mutate(dataset = c("training", "validation")) %>% 
  knitr::kable()
```

Performance on the training and holdout datasets are not grossly different, hence there does not seem to be a major concern with overfitting in our model. 

# Extract Prediction Intervals

90% prediction interval for our example case:
```{r}
preds_intervals <- predict(
  workflows::pull_workflow_fit(lm_wf),
  workflows::pull_workflow_prepped_recipe(lm_wf) %>% bake(ames_holdout),
  type = "pred_int",
  level = 0.90
) %>% 
  mutate(across(contains(".pred"), ~10^.x)) %>%
  bind_cols(data_preds) %>% 
  select(contains(".pred"), Sale_Price, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)

preds_intervals %>% 
  select(-Sale_Price) %>% 
  slice(1) %>% 
  knitr::kable()
```

This suggests that, given these attributes, we would expect that the `Sale_Price` for an individual observation would fall between \$140,208 and \$242,793 (with 90% confidence). Our example offer of \$184,000 falls well within this range, meaning that -- based on the model -- the offer does not appear unreasonable.

Let's review what 90% prediction intervals look like against the *actual* prices on a subset of observations in our holdout dataset (log transformations are applied on the x and y axis to adjust for the fact that variability in estimates, and therefore prediction intervals, increase with the predicted `Sale_Price`[^6]):
```{r prediction-intervals-sample}
set.seed(1234)
p <- preds_intervals %>% 
  mutate(pred_interval = ggplot2::cut_number(.pred, 10)) %>% 
  group_by(pred_interval) %>% 
  sample_n(2) %>% 
  ggplot(aes(x = .pred))+
  geom_point(aes(y = .pred, color = "prediction interval"))+
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = "prediction interval"))+
  geom_point(aes(y = Sale_Price, color = "actuals"))+
  labs(title = "90% prediction intervals on a holdout dataset",
       subtitle = "Linear model (analytic method)",
       y = "Sale_Price prediction intervals and actuals")+
  theme_bw()+
  coord_fixed()

p +
  scale_x_log10(labels = scales::dollar)+
  scale_y_log10(labels = scales::dollar)

```

[^6]: As noted previously the model was built to predict `log(Sale_Price, 10)` (as opposed to `Sale_Price`) as errors increase with the predicted price of the house. Hence why the transformed scales in the text are appropriate. If you are curious though, the figure below shows the prediction intervals on raw `Sale_Price` scale.
    
    ```{r}
    p + 
      scale_x_continuous(labels = scales::dollar)+
      scale_y_continuous(labels = scales::dollar)
    ```


# Checking Coverage

Let's now check what proportion of our holdout observations are actually "covered" by their prediction intervals^[I.e. the rate at which the *actual* value fall within the range of the prediction interval (coverage is essentially checking whether the range you expect from your prediction intervals is actually what is achieved).]. 

Hence for our 90% prediction intervals, we will expect coverage on a holdout set to be 90%.
```{r}
preds_intervals %>%
  mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
  summarise(n = n(),
            n_covered = sum(
              covered
            ),
            stderror = sd(covered) / sqrt(n),
            coverage_prop = n_covered / n) %>% 
  knitr::kable()
```

~93% of our holdout observations were actually covered^[The difference between 93% and the expected coverage of 90% may just be the result of random variability in the data.]. We can likely trust these intervals aren't too narrow^[Though they are perhaps slightly conservative. To get a better estimate for coverage, we could use cross validation as this will provide estimates for coverage across multiple iterations of model fitting.]. It is usually better to be on the slightly conservative than optimistic side regarding coverage (as models tend to perform worse than expected when put into production)^[I.e. it is betterthat our empirical coverage is greater than our target coverage than less than our target coverage -- even if it would be ideal for these to align.]. 

It may also helpful to review coverage across predictions, rather than just in aggregate. This distinction is related to the notion of "marginal" vs "conditional" coverage^["Marginal" coverage generally means "on average across observations", whereas "conditional" means conditioned on the specific attributes for an observation. Hence "90%" marginal coverage allows that coverage may be higher in some areas and lower in others. 90% conditional coverage means that the conditional rate should be 90% across observations. "Conditional" coverage is generally harder to get than "marginal" coverage and is actually impossible to achieve without some assumptions.].

The chart below splits the holdout data into five segments ordered by predicted price with equal number of observations^[Mostly equal -- there are 146 or 147 observations summarised in each row.] and checks the proportion covered over each quintile.

```{r}
preds_intervals %>%
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>% 
  mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
  group_by(price_grouped) %>% 
  summarise(n = n(),
            n_covered = sum(
              covered
            ),
            stderror = sd(covered) / sqrt(n),
            n_prop = n_covered / n) %>% 
  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %>% 
  separate(x_tmp, c("min", "max"), sep = ",") %>% 
  mutate(across(c(min, max), as.double)) %>% 
  ggplot(aes(x = forcats::fct_reorder(scales::dollar(max), max), y = n_prop))+
  geom_line(aes(group = 1))+
  geom_errorbar(aes(ymin = n_prop - 2 * stderror, ymax = n_prop + 2 * stderror))+
  coord_cartesian(ylim = c(0.70, 1.01))+
  # scale_x_discrete(guide = guide_axis(n.dodge = 2))+
  labs(x = "Max Predicted Price for Quintile",
       y = "Coverage at Quintile",
       title = "Coverage by Quintile of Predictions",
       subtitle = "On a holdout Set",
       caption = "Error bars represent {coverage} +/- 2 * {coverage standard error}")+
  theme_bw()
```

This suggests there may be slightly different levels of coverage at different quintiles (e.g. the cheapest predicted houses have less coverage than those in the 2nd quintile.)^[Not shown here, but it may also be valuable to review your coverage levels across key variables of interest.].

A test for checking whether variability in the categorical variable `covered` (covered / not-covered) does not vary across the quintile of predicted `Sale_Price` (1st, 2nd, 3rd, 4th, 5th) could be done using a [Chi-square statistical test](https://en.wikipedia.org/wiki/Chi-squared_test):

```{r}
preds_intervals %>% 
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>% 
  mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
  with(chisq.test(price_grouped, covered))
```

The results provide evidence that whether an observation is covered or not-covered may depend (to some extent) on which quintile for predicted `Sale_Price` the observation falls within^[More explicitly, this test shows that the distribution of `price_grouped by quintile` across `covered` would only occur ~3.4% of the time under the assumption of "no relationship". This is very unlikely, so we reject the NULL hypothesis of "no relationship".].

I will show in a follow-up post covering __Simulation Methods for Building Prediction Intervals__ intervals that seem to have more consistent coverage rates across predicted `Sale_Price`.

# Checking Interval Width

Another helpful metric for evaluating your prediction intervals is to review their width. More narrow bands suggest a more precise model. Remember from the notes in [Consider Variability] that the size of the errors will change with the size of the prediction. To adjust for this an appropriate metric can be to review 'interval width' around `Sale_Price` as a percentage of the prediction^[Alternatively could just have evaluated the interval widths in the `log(Sale_Price, 10)` terms outputted by the model.]. I will review these across the quintiles of our model.

```{r}
lm_interval_widths <- preds_intervals %>% 
  mutate(interval_width = .pred_upper - .pred_lower,
         interval_pred_ratio = interval_width / .pred) %>% 
  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>% 
  group_by(price_grouped) %>% 
  summarise(n = n(),
            mean_interval_width_percentage = mean(interval_pred_ratio),
            stdev = sd(interval_pred_ratio),
            stderror = stdev / sqrt(n)) %>% 
  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %>% 
  separate(x_tmp, c("min", "max"), sep = ",") %>% 
  mutate(across(c(min, max), as.double)) %>% 
  select(-price_grouped) 

lm_interval_widths %>% 
  knitr::kable()
```

The relative interval width is highly consistent across predicted `Sale_Price` and between observations^[Varying on average less than half a percent between observations.]. The 90% interval represents on average about ~54% of the predicted `Sale_Price`^[Methods in follow-up posts will show greater diversity of interval widths.]. For the analytic approach the only factor that contributes to differences in interval width across predictions is the distance of an observations from the centroid of the data (this has a relatively small impact compared to the sampling variance which is treated as constant)^[While the see-saw analogy does make a difference in causing some observations to have wider intervals than others, in aggregate you can see the difference is not *that* big -- values like the Mean Squared Error (MSE) or similar aggregate error metrics actually give a pretty close semblance of what you will expect to get when going through the process of creating prediction intervals specific to each observation. Note also however that the difference in interval width will still be greater for the more extreme observations. In future posts some of the methods I walk through will be more flexible or have other advantages (e.g. less strict reliance on distributional assumptions).]. 

# Closing Notes

*Upsides of prediction intervals using parametric method with linear regression:*

* Most common (largest number of people are familiar with)
* Straightforward, intuitive calculation
* Low computation costs

*Downsides:*

* Prediction intervals produced in this way have a variety of strong assumptions that they depend on, [resource](https://online.stat.psu.edu/stat501/lesson/3/3.3/#paragraph--766).
    * Generally this approach relies more heavily on model assumptions compared to alternatives. 
* Assumes you have the correct model (If your model overfits, the associated prediction intervals will likely be overly optimistic^[Though linear models are less likely to overfit compared to most model types.])

**Wrap-up:**

Prediction intervals provide an indicator for the uncertainty of individual observations. They are useful for generating a reasonable range you would expect for an observation. These ranges can be used to evaluate things like potential Sale Prices and have the advantages over more simple methods (e.g. comparing whether a deal is above or below expectated Sale Price) in that they take into account both the magnitude and level of variability in the data to provide a notion of a "reasonable" range of values for an observation.

Measures like *coverage* and *interval width* are helpful for evaluating your intervals. These metrics will be used in follow-up posts on __Building Prediction Intervals Using Quantile Regression__ and __Building Prediction Intervals Using Simulation__, where I walk through using more sophisticated methods for building prediction intervals.