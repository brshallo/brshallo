---
title: Feature Engineering with Sliding Windows and Lagged Inputs
author: Bryan Shalloway
date: '2020-10-12'
slug: window-functions-for-resampling
---


<div id="TOC">
<ul>
<li><a href="#load-data">Load data</a></li>
<li><a href="#feature-engineering-data-splits">Feature Engineering &amp; Data Splits</a><ul>
<li><a href="#lag-based-features-before-split-use-dplyr-or-similar">Lag Based Features (Before Split, use <code>dplyr</code> or similar)</a></li>
<li><a href="#data-splits">Data Splits</a></li>
<li><a href="#other-features-after-split-use-recipes">Other Features (After Split, use <code>recipes</code>)</a></li>
</ul></li>
<li><a href="#model-specification">Model Specification</a></li>
<li><a href="#model-evaluation">Model Evaluation</a></li>
<li><a href="#appendix">Appendix</a><ul>
<li><a href="#model-building-with-hyperparameter-tuning">Model Building with Hyperparameter Tuning</a></li>
<li><a href="#resources">Resources</a></li>
</ul></li>
</ul>
</div>

<p>The new <code>rsample::sliding_*()</code> functions bring the windowing approaches used in <a href="https://github.com/DavisVaughan/slider">slider</a> to the sampling procedures used in the <a href="https://github.com/tidymodels">tidymodels</a> framework<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. These functions make evaluation of models with time-dependent variables easier<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>For some problems you may want to take a traditional regression or classification based approach<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> while still accounting for the date/time-sensitive components of your data. In this post I will use the <code>tidymodels</code> suite of packages to:</p>
<ul>
<li>build lag based and non-lag based features</li>
<li>set-up appropriate time series cross-validation windows</li>
<li>evaluate performance of linear regression and random forest models on a regression problem</li>
</ul>
<p>For my example I will use data from Wake County food inspections and will try to predict the <code>SCORE</code> for upcoming restaurant food inspections.</p>
<div id="load-data" class="section level1">
<h1>Load data</h1>
<p>You can use Wake County’s open API (does not require a login/account) and the <a href="https://github.com/r-lib/httr">httr</a> and <a href="https://github.com/jeroen/jsonlite">jsonlite</a> packages to load in the data. You can also download the data directly from their <a href="https://data.wakegov.com/datasets/1b08c4eb32f44a198277c418b71b3a48_2">website</a>.</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(httr)
library(jsonlite)
library(tidymodels)</code></pre>
<p><em>Get food inspections data:</em></p>
<pre class="r"><code>r_insp &lt;- GET(&quot;https://opendata.arcgis.com/datasets/ebe3ae7f76954fad81411612d7c4fb17_1.geojson&quot;)

inspections &lt;- content(r_insp, &quot;text&quot;) %&gt;% 
  fromJSON() %&gt;% 
  .$features %&gt;%
  .$properties %&gt;% 
  as_tibble()

inspections_clean &lt;- inspections %&gt;% 
  mutate(date = ymd_hms(DATE_) %&gt;% as.Date()) %&gt;% 
  select(-c(DATE_, DESCRIPTION, OBJECTID))</code></pre>
<p><em>Get food locations data:</em></p>
<pre class="r"><code>r_rest &lt;- GET(&quot;https://opendata.arcgis.com/datasets/124c2187da8c41c59bde04fa67eb2872_0.geojson&quot;) #json

restauraunts &lt;- content(r_rest, &quot;text&quot;) %&gt;% 
  fromJSON() %&gt;% 
  .$features %&gt;%
  .$properties %&gt;% 
  as_tibble() %&gt;% 
  select(-OBJECTID)

restauraunts &lt;- restauraunts %&gt;% 
  mutate(RESTAURANTOPENDATE = ymd_hms(RESTAURANTOPENDATE) %&gt;% as.Date())</code></pre>
<p><em>Further prep:</em></p>
<ul>
<li>Join the <code>inspections</code> and <code>restaurants</code> datasets<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></li>
<li>Filter out extreme outliers in <code>SCORE</code> (likely data entry errors)</li>
<li>Filter to only locations of <code>TYPE</code> restaurant<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></li>
<li>It’s important to consider which fields should be excluded for ethical reasons. For our problem, we will say that any restaurant name or location information must be excluded<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.</li>
</ul>
<pre class="r"><code>inspections_restaurants &lt;- inspections_clean %&gt;% 
  left_join(restauraunts, by = c(&quot;HSISID&quot;, &quot;PERMITID&quot;)) %&gt;% 
  filter(SCORE &gt; 50, FACILITYTYPE == &quot;Restaurant&quot;) %&gt;% 
  select(-c(FACILITYTYPE, PERMITID)) %&gt;% 
  select(-c(NAME, contains(&quot;ADDRESS&quot;), CITY, STATE, POSTALCODE, PHONENUMBER, X, Y, GEOCODESTATUS))</code></pre>
<pre class="r"><code>inspections_restaurants %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 24,276
## Columns: 6
## $ HSISID             &lt;chr&gt; &quot;04092017542&quot;, &quot;04092017542&quot;, &quot;04092017542&quot;, &quot;04...
## $ SCORE              &lt;dbl&gt; 94.5, 92.0, 95.0, 93.5, 93.0, 93.5, 92.5, 96.0, ...
## $ TYPE               &lt;chr&gt; &quot;Inspection&quot;, &quot;Inspection&quot;, &quot;Inspection&quot;, &quot;Inspe...
## $ INSPECTOR          &lt;chr&gt; &quot;Anne-Kathrin Bartoli&quot;, &quot;Laura McNeill&quot;, &quot;Laura ...
## $ date               &lt;date&gt; 2017-04-07, 2017-11-08, 2018-03-23, 2018-09-07,...
## $ RESTAURANTOPENDATE &lt;date&gt; 2017-03-01, 2017-03-01, 2017-03-01, 2017-03-01,...</code></pre>
</div>
<div id="feature-engineering-data-splits" class="section level1">
<h1>Feature Engineering &amp; Data Splits</h1>
<p>Discussion on issue <a href="https://github.com/tidymodels/rsample/pull/168">#168</a> suggests that some features (those depending on prior observations) should be created before the data is split<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. The first and last sub-sections:</p>
<ul>
<li><a href="#lag-based-features-before-split-use-dplyr-or-similar">Lag Based Features (Before Split, use <code>dplyr</code> or similar)</a></li>
<li><a href="#other-features-after-split-use-recipes">Other Features (After Split, use <code>recipes</code>)</a></li>
</ul>
<p>provide examples of the types of features that should be created before and after splitting your data. Lag-based features can, in some ways, be thought of as ‘raw inputs’ as they should be created prior to a <code>recipes</code> step<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<div id="lag-based-features-before-split-use-dplyr-or-similar" class="section level2">
<h2>Lag Based Features (Before Split, use <code>dplyr</code> or similar)</h2>
<p>Lag based features should generally be computed prior to splitting your data into “training” / “testing” (or “analysis” / “assessment”<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>) sets. This is because calculation of these features may depend on observations in prior splits<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. Let’s build a few features where this is the case:</p>
<ul>
<li>Prior <code>SCORE</code> for <code>HSISID</code></li>
<li>Average of prior 3 years of <code>SCORE</code> for <code>HSISISD</code></li>
<li>Overall recent prior average <code>SCORE</code></li>
<li>Days since <code>RESTAURANTOPENDATE</code></li>
<li>Days since last inspection <code>date</code></li>
</ul>
<pre class="r"><code>data_time_feats &lt;- inspections_restaurants %&gt;% 
  arrange(date) %&gt;% 
  mutate(SCORE_yr_overall = slider::slide_index_dbl(SCORE, 
                                                    .i = date, 
                                                    .f = mean, 
                                                    na.rm = TRUE, 
                                                    .before = lubridate::days(365), 
                                                    .after = -lubridate::days(1))
         ) %&gt;% 
  group_by(HSISID) %&gt;% 
  mutate(SCORE_lag = lag(SCORE),
         SCORE_recent = slider::slide_index_dbl(SCORE, 
                                                date, 
                                                mean, 
                                                na.rm = TRUE, 
                                                .before = lubridate::days(365*3), 
                                                .after = -lubridate::days(1), 
                                                .complete = FALSE),
         days_since_open = (date - RESTAURANTOPENDATE) / ddays(1),
         days_since_last = (date - lag(date)) / ddays(1)) %&gt;% 
  ungroup() %&gt;% 
  arrange(date)</code></pre>
<p>The use of <code>.after = -lubridate::days(1)</code> prevents data leakage by ensuring that this feature does not include information from the current day in its calculation<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> <a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>.</p>
</div>
<div id="data-splits" class="section level2">
<h2>Data Splits</h2>
<p><strong>Some Filters:</strong></p>
<p>We will presume that the model is only intended for restaurants that have previous inspections on record<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> and will use only the most recent five years of data.</p>
<pre class="r"><code>data_time_feats &lt;- data_time_feats %&gt;% 
  filter(date &gt;= (max(date) - years(5)), !is.na(SCORE_lag))</code></pre>
<p><strong>Initial Split:</strong></p>
<p>After creating our lag based features, we can split our data into training and testing splits.</p>
<pre class="r"><code>initial_split &lt;- rsample::initial_time_split(data_time_feats, prop = .8)
train &lt;- rsample::training(initial_split)
test &lt;- rsample::testing(initial_split)</code></pre>
<p><strong>Resampling (Time Series Cross-Validation):</strong></p>
<p>For this problem we should evaluate our models using time series cross-validation<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>. This entails creating multiple ordered subsets of the training data where each set has a different assignment of observations into “analysis” or “assessment” data<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>.</p>
<p>Ideally the resampling scheme used for model evaluation mirrors how the model will be built and evaluated in production. For example, if the production model will be updated once every three months it makes sense that the “assessment” set be this length. We can use <code>rsample::sliding_period()</code> to set this up.</p>
<p>For each set, we will use three years of “analysis” data for training and then three months of “assessment” data for evaluation.</p>
<pre class="r"><code>resamples &lt;- rsample::sliding_period(train, 
                                     index = date, 
                                     period = &quot;month&quot;, 
                                     lookback = 36, 
                                     assess_stop = 3)</code></pre>
<p>I will load in some functions I made to review our resampling windows<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.</p>
<pre class="r"><code>devtools::source_gist(&quot;https://gist.github.com/brshallo/7d180bde932628a151a4d935ffa586a5&quot;)

resamples  %&gt;% 
  extract_dates_rset() %&gt;% 
  print() %&gt;% 
  plot_dates_rset()</code></pre>
<pre><code>## # A tibble: 13 x 6
##    splits         id     analysis_min analysis_max assessment_min assessment_max
##    &lt;list&gt;         &lt;chr&gt;  &lt;date&gt;       &lt;date&gt;       &lt;date&gt;         &lt;date&gt;        
##  1 &lt;split [9K/1K~ Slice~ 2015-10-08   2018-10-31   2018-11-01     2019-01-31    
##  2 &lt;split [9.1K/~ Slice~ 2015-11-02   2018-11-30   2018-12-03     2019-02-28    
##  3 &lt;split [9.3K/~ Slice~ 2015-12-01   2018-12-31   2019-01-02     2019-03-31    
##  4 &lt;split [9.5K/~ Slice~ 2016-01-04   2019-01-31   2019-02-01     2019-04-30    
##  5 &lt;split [9.7K/~ Slice~ 2016-02-01   2019-02-28   2019-03-01     2019-05-31    
##  6 &lt;split [9.8K/~ Slice~ 2016-03-01   2019-03-31   2019-04-01     2019-06-28    
##  7 &lt;split [9.9K/~ Slice~ 2016-04-01   2019-04-30   2019-05-01     2019-07-31    
##  8 &lt;split [10.1K~ Slice~ 2016-05-02   2019-05-31   2019-06-03     2019-08-30    
##  9 &lt;split [10.2K~ Slice~ 2016-06-01   2019-06-28   2019-07-01     2019-09-30    
## 10 &lt;split [10.4K~ Slice~ 2016-07-01   2019-07-31   2019-08-01     2019-10-31    
## 11 &lt;split [10.6K~ Slice~ 2016-08-01   2019-08-30   2019-09-03     2019-11-27    
## 12 &lt;split [10.7K~ Slice~ 2016-09-01   2019-09-30   2019-10-01     2019-12-31    
## 13 &lt;split [10.8K~ Slice~ 2016-10-03   2019-10-31   2019-11-01     2020-01-16</code></pre>
<p><img src="/post/2020-10-12-window-functions-for-resampling_files/figure-html/check-resampling-splits-1.png" width="672" /></p>
<p>For purposes of <a href="#model-evaluation">Model Evaluation</a>, performance across each period will be weighted equally when reviewing overall performance (regardless of number of observations in a period)<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> <a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>.</p>
</div>
<div id="other-features-after-split-use-recipes" class="section level2">
<h2>Other Features (After Split, use <code>recipes</code>)</h2>
<p>Where possible, features should be created using the <a href="https://github.com/tidymodels/recipes">recipes</a> package<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> (i.e. those that do not depend on lagged values). <code>recipes</code> makes pre-processing convenient and helps prevent data leakage into the assessment set.</p>
<p><em>Some features / transformations I’ll make with <code>recipes</code>:</em></p>
<p>Note that it is OK to modify or transform a previously created lag-based feature in a <code>recipes</code> step, as assuming you created the raw input as well as your resampling windows in an appropriate manner, you should be safe from data leakage issues<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>.</p>
<ul>
<li>collapse rare values for <code>INSPECTOR</code> and <code>TYPE</code></li>
<li>log transform <code>days_since_open</code> and <code>days_since_last</code></li>
<li>add calendar based features</li>
</ul>
<pre class="r"><code>rec_general &lt;- recipes::recipe(SCORE ~ ., data = train) %&gt;% 
  step_rm(RESTAURANTOPENDATE) %&gt;% 
  update_role(HSISID, new_role = &quot;ID&quot;) %&gt;% 
  step_other(INSPECTOR, TYPE, threshold = 50) %&gt;% 
  step_string2factor(one_of(&quot;TYPE&quot;, &quot;INSPECTOR&quot;)) %&gt;%
  step_novel(one_of(&quot;TYPE&quot;, &quot;INSPECTOR&quot;)) %&gt;%
  step_log(days_since_open, days_since_last) %&gt;% 
  step_date(date, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% 
  update_role(date, new_role = &quot;ID&quot;) %&gt;% 
  step_zv(all_predictors())</code></pre>
<p>Let’s peak at what these features will look look like:</p>
<pre class="r"><code>prep(rec_general, data = train) %&gt;% 
  juice() %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 13,960
## Columns: 12
## $ HSISID           &lt;fct&gt; 04092016286, 04092016487, 04092016210, 04092011828...
## $ TYPE             &lt;fct&gt; Inspection, Inspection, Inspection, Inspection, In...
## $ INSPECTOR        &lt;fct&gt; Laura McNeill, Angela Myers, Jennifer Edwards, And...
## $ date             &lt;date&gt; 2015-10-08, 2015-10-08, 2015-10-08, 2015-10-08, 2...
## $ SCORE_yr_overall &lt;dbl&gt; 95.50647, 95.50647, 95.50647, 95.50647, 95.50647, ...
## $ SCORE_lag        &lt;dbl&gt; 94.5, 99.0, 96.0, 95.0, 94.0, 95.5, 92.5, 96.5, 98...
## $ SCORE_recent     &lt;dbl&gt; 94.30000, 99.00000, 93.42857, 95.55556, 96.20000, ...
## $ days_since_open  &lt;dbl&gt; 6.976348, 6.665684, 7.090077, 8.884472, 8.200288, ...
## $ days_since_last  &lt;dbl&gt; 5.476464, 5.075174, 5.176150, 4.276666, 5.129899, ...
## $ SCORE            &lt;dbl&gt; 96.0, 95.5, 94.0, 98.0, 95.5, 92.5, 95.0, 97.0, 98...
## $ date_dow         &lt;fct&gt; Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, ...
## $ date_month       &lt;fct&gt; Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, ...</code></pre>
</div>
</div>
<div id="model-specification" class="section level1">
<h1>Model Specification</h1>
<p><strong>Simple linear regression model:</strong></p>
<pre class="r"><code>lm_mod &lt;- parsnip::linear_reg() %&gt;% 
  set_engine(&quot;lm&quot;) %&gt;% 
  set_mode(&quot;regression&quot;)

lm_workflow_rs &lt;- workflows::workflow() %&gt;% 
  add_model(lm_mod) %&gt;% 
  add_recipe(rec_general) %&gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))</code></pre>
<p><strong><code>ranger</code> Random Forest model using default hyper-parameter values:</strong></p>
<pre class="r"><code>rand_mod &lt;- parsnip::rand_forest() %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  set_mode(&quot;regression&quot;)
  
set.seed(1234)
rf_workflow_rs &lt;- workflow() %&gt;% 
  add_model(rand_mod) %&gt;% 
  add_recipe(rec_general) %&gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))</code></pre>
<p><strong>A <code>parsnip::null_model</code>:</strong></p>
<p>The NULL model will be helpful as a baseline Root Mean Square Error (RMSE) comparison.</p>
<pre class="r"><code>null_mod &lt;- parsnip::null_model(mode = &quot;regression&quot;) %&gt;% 
  set_engine(&quot;parsnip&quot;)

null_workflow_rs &lt;- workflow() %&gt;% 
  add_model(null_mod) %&gt;% 
  add_formula(SCORE ~ NULL) %&gt;%
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))</code></pre>
<p>See code in <a href="#model-building-with-hyperparameter-tuning">Model Building with Hyperparameter Tuning</a> for more sophisticated examples that include hyperparameter tuning for <code>glmnet</code><a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> and <code>ranger</code> models.</p>
</div>
<div id="model-evaluation" class="section level1">
<h1>Model Evaluation</h1>
<p>The next several code chunks extract the <em>average</em> performance across “assessment” sets of samples or extract the performance across each of the individual “assessment” sets.</p>
<pre class="r"><code>mod_types &lt;- list(&quot;lm&quot;, &quot;rf&quot;, &quot;null&quot;)

avg_perf &lt;- map(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
                collect_metrics) %&gt;% 
  map2(mod_types, ~mutate(.x, source = .y)) %&gt;% 
  bind_rows()</code></pre>
<pre class="r"><code>extract_splits_metrics &lt;- function(rs_obj, name){
  
  rs_obj %&gt;% 
    select(id, .metrics) %&gt;% 
    unnest(.metrics) %&gt;% 
    mutate(source = name)
}

splits_perf &lt;- map2(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
     mod_types, 
     extract_splits_metrics) %&gt;% 
  bind_rows()</code></pre>
<p>The overall performance as well as the performance within splits suggests that both models were better than the baseline (the mean within the analysis set)<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> and that the linear model outperformed the random forest model.</p>
<pre class="r"><code>splits_perf %&gt;% 
  mutate(id = forcats::fct_rev(id)) %&gt;% 
  ggplot(aes(x = .estimate, y = id, colour = source))+
  geom_vline(aes(xintercept = mean, colour = source), 
           alpha = 0.5,
           data = avg_perf)+
  geom_point()+
  facet_wrap(~.metric, scales = &quot;free_x&quot;)+
  xlim(c(0, NA))+
  theme_bw()+
  labs(caption = &quot;Vertical lines are average performance as captured by `collect_metrics()`&quot;)</code></pre>
<p><img src="/post/2020-10-12-window-functions-for-resampling_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We could use a paired sample t-test to formally compare the random forest and linear models’ out-of-sample RMSE performance.</p>
<pre class="r"><code>t.test(
  filter(splits_perf, source == &quot;lm&quot;, .metric == &quot;rmse&quot;) %&gt;% pull(.estimate),
  filter(splits_perf, source == &quot;rf&quot;, .metric == &quot;rmse&quot;) %&gt;% pull(.estimate),
  paired = TRUE
) %&gt;% 
  broom::tidy() %&gt;% 
  mutate(across(where(is.numeric), round, 4)) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">estimate</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
<th align="right">parameter</th>
<th align="right">conf.low</th>
<th align="right">conf.high</th>
<th align="left">method</th>
<th align="left">alternative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-0.0758</td>
<td align="right">-4.9392</td>
<td align="right">3e-04</td>
<td align="right">12</td>
<td align="right">-0.1092</td>
<td align="right">-0.0423</td>
<td align="left">Paired t-test</td>
<td align="left">two.sided</td>
</tr>
</tbody>
</table>
<p>This suggests the better performance by the linear model <em>is</em> statistically significant.</p>
<p><strong>Other potential steps:</strong></p>
<p>There is lots more we could do from here<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>. However the purpose of this post was to provide a short <code>tidymodels</code> example that incorporated some of the sliding functions in <code>rsample</code> and <code>slider</code>. For more resources on modeling and the <code>tidymodels</code> framework, see <a href="https://www.tidymodels.org/">tidymodels.org</a>, <a href="https://www.tmwr.org/">Tidy Modeling with R</a> or check-out the <a href="#resources">Resources</a> section in the <a href="#appendix">Appendix</a>.</p>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="model-building-with-hyperparameter-tuning" class="section level2">
<h2>Model Building with Hyperparameter Tuning</h2>
<p>Below is code for tuning a <code>glmnet</code> linear regression model (use <code>tune</code> to optimize the L1/L2 penalty)<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>.</p>
<pre class="r"><code>rec_glmnet &lt;- rec_general %&gt;% 
  step_dummy(all_predictors(), -all_numeric()) %&gt;%
  step_normalize(all_predictors(), -all_nominal()) %&gt;% 
  step_zv(all_predictors())

glmnet_mod &lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&gt;% 
  set_engine(&quot;glmnet&quot;) %&gt;% 
  set_mode(&quot;regression&quot;)

glmnet_workflow &lt;- workflow::workflow() %&gt;% 
  add_model(glmnet_mod) %&gt;% 
  add_recipe(rec_glmnet)

glmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 
    0.2, 0.4, 0.6, 0.8, 1))

glmnet_tune &lt;- tune::tune_grid(glmnet_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = glmnet_grid)</code></pre>
<p>And code to tune a <code>ranger</code> Random Forest model, tuning the <code>mtry</code> and <code>min_n</code> parameters<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>.</p>
<pre class="r"><code>rand_mod &lt;- parsnip::rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  set_mode(&quot;regression&quot;)
  
rf_workflow &lt;- workflow() %&gt;% 
  add_model(rand_mod) %&gt;% 
  add_recipe(rec_general)

cores &lt;- parallel::detectCores()

set.seed(1234)
rf_tune &lt;- tune_grid(rf_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = 25)</code></pre>
</div>
<div id="resources" class="section level2">
<h2>Resources</h2>
<ul>
<li>Link on doing regressions in slider: <a href="https://twitter.com/dvaughan32/status/1247270052782637056?s=20" class="uri">https://twitter.com/dvaughan32/status/1247270052782637056?s=20</a></li>
<li>Rstudio lightning talk on <code>slider</code>: <a href="https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/" class="uri">https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/</a></li>
<li><code>modeltime</code> package that applies <code>tidymodels</code> suite to time series and forecasting problems: <a href="https://business-science.github.io/modeltime/" class="uri">https://business-science.github.io/modeltime/</a> (business-science course has more fully developed training materials on this topic as well)</li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>These were announced with version <a href="https://github.com/tidymodels/rsample/blob/master/NEWS.md">0.0.8</a>.The help pages for <code>rsample</code> (as well as the <code>slider</code> package) are helpful resources for understanding the three types of sliding you can use, briefly these are:</p>
<ul>
<li><code>sliding_window()</code>: only takes into account order / position of dates</li>
<li><code>sliding_index()</code>: slide according to an index</li>
<li><code>sliding_period()</code>: slide according to an index and set k split points based on period (and other function arguments)</li>
</ul>
<p><code>rsample::sliding_index()</code> and <code>rsample::sliding_period()</code> are maybe the most useful additions as they allow you to do resampling based on a date/time index. For <code>sliding_index()</code>, you usually want to make use of the <code>step</code> argument (otherwise it defaults to having a split for every observation).</p>
<p>I found <code>rsample::sliding_period()</code> easier to get acquantied with<code>than rsample::sliding_index()</code>. However within the <code>slider</code> package I found <code>slider::sliding_index()</code> easier to use than <code>slider::sliding_period()</code>. Perhaps this makes sense as when setting sampling windows you are usually trying to return an object with far fewer rows, that is, collapsed to k number of rows (unless you are doing Leave-One-Out cross-validation). On the other hand, the <code>slider</code> package is often used in a <code>mutate()</code> step where you often want to output the same number of observations as are inputted. Perhaps then it is unsurprising the different scenarios when the <code>index</code> vs <code>period</code> approach feels more intuitive.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Previously users would have needed to use <code>rsample::rolling_origin()</code>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>As opposed to a more specialized time-series modeling approach.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>There is also “violations” dataset available, which may have additional useful features, but which I will ignore for this example.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>For this example I’m pretending that we only care about predicting <code>SCORE</code> for restaurants… as opposed to food trucks or other entities that may receive inspections.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>In some cases you may need to be more careful than this and exclude information that are proxies for inappropriate fields as well. For example, pretend that the <code>INSPECTOR</code>s are assigned based on region. In this case, <code>INSPECTOR</code> would be a proxy for geographic information and perhaps warranting exclusion as well (in certain cases).<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Into training / testing sets or analysis / assessment sets.<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>As discussed by Davis Vaughn at the end of this <a href="https://gist.github.com/DavisVaughan/433dbdceb439c9be30ddcc78d836450d">gist</a>.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of <a href="http://www.feat.engineering/resampling.html">Feature Engineering and Selection…</a> for further explanation.]<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>It is important that these features be created in a way that does not cause data leakage.<a href="#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Which would not be available at the time of prediction.<a href="#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>I’m a fan of the ability to use negative values in the <code>.after</code> argument:</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
This is a fairly obscure feature in {slider}, but I love it. Don’t want the current day in your rolling window? Set a negative <code>.after</code> value to shift the end of the window backwards. For example:<br><br>On day 5<br>.before = days(3)<br>.after = -days(1)<br><br>Includes days:<br>[2, 4] <a href="https://t.co/rG0IGuTj1c">https://t.co/rG0IGuTj1c</a>
</p>
— Davis Vaughan (<span class="citation">@dvaughan32</span>) <a href="https://twitter.com/dvaughan32/status/1233116713010573312?ref_src=twsrc%5Etfw">February 27, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<a href="#fnref12" class="footnote-back">↩</a></li>
<li id="fn13"><p>If I did not make this assumption, I would need to impute the time based features at this point.<a href="#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>Two helpful resources for understanding time series cross-validation:</p>
<ol style="list-style-type: decimal">
<li>From <a href="https://eng.uber.com/forecasting-introduction/">uber engineering</a></li>
<li>From <a href="https://otexts.com/fpp3/tscv.html">Forecasting Principles and Practices</a></li>
</ol>
<a href="#fnref14" class="footnote-back">↩</a></li>
<li id="fn15"><p>An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of <a href="http://www.feat.engineering/resampling.html">Feature Engineering and Selection…</a> for further explanation.]<a href="#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>I’ve tweeted previously about helper functions for reviewing your resampling scheme:</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
❤️new <code>rsample::sliding_*()</code> funs by <a href="https://twitter.com/dvaughan32?ref_src=twsrc%5Etfw"><span class="citation">@dvaughan32</span></a>. It can take a minute to check that all arguments are set correctly. Here are helper funs I've used to check that my resampling windows are constructed as intended: <a href="https://t.co/HhSjuRzAsB">https://t.co/HhSjuRzAsB</a> may make into an <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://twitter.com/hashtag/shiny?src=hash&amp;ref_src=twsrc%5Etfw">#shiny</a> dashboard. <a href="https://t.co/sNloHfkh4a">pic.twitter.com/sNloHfkh4a</a>
</p>
— Bryan Shalloway (<span class="citation">@brshallo</span>) <a href="https://twitter.com/brshallo/status/1314720234373287937?ref_src=twsrc%5Etfw">October 10, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<a href="#fnref16" class="footnote-back">↩</a></li>
<li id="fn17"><p>Note that using <code>rsample::sliding_period()</code> is likely to produce different numbers of observations between splits.<a href="#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>It could also make sense to weight performance metrics by number of observations. One way to do this, would be to use a control function to extract the predictions, and then evaluate the performance across the predictions. In my examples below I do keep the predictions, but end-up not making use of them…<a href="#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>For each split, this will then build the features for the assessment set based on each analysis set.<a href="#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>Although I just do a simple <code>step_log()</code> transform here, more sophisticated steps would also be kosher, e.g. <code>step_pca()</code>.<a href="#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>Our number of observations is realtively high compared to the number of features, so there is a good chance we will have relatively low penalties. While working interactively, I did not see any substantive difference in performance.<a href="#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>There is no baseline performance for Rsquared because the metric itself is based off amount of variance that is explained compared to the baseline (i.e. the mean).<a href="#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>You would likely iterate on the model building process (e.g. perform exploratory data analysis, review outliers in initial models, etc.) and eventually get to a final set of models to evaluate on the test set.<a href="#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>Our number of observations is realtively high compared to the number of features, so there is a good chance we will have relatively low penalties.<a href="#fnref24" class="footnote-back">↩</a></p></li>
<li id="fn25"><p>This was taking a <em>long</em> time and is part of why I decided to move the tuned examples to teh <a href="#appendix">Appendix</a>.<a href="#fnref25" class="footnote-back">↩</a></p></li>
</ol>
</div>
