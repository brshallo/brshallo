<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>programming on Bryan Shalloway&#39;s Blog</title>
    <link>/categories/programming/</link>
    <description>Recent content in programming on Bryan Shalloway&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Undersampling Will Change the Base Rates of Your Model&#39;s Predictions</title>
      <link>/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#create-data&#34;&gt;Create Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-of-feature-and-target&#34;&gt;Association of ‘feature’ and ‘target’&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resample&#34;&gt;Resample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-models&#34;&gt;Build Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rescale-predictions-to-predicted-probabilities&#34;&gt;Rescale Predictions to Predicted Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;Density Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lift-plot&#34;&gt;Lift Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-scaling-methods&#34;&gt;Comparing Scaling Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!-- This chunk allows more than three levels for the Table of Contents --&gt;
&lt;!-- &lt;script&gt; --&gt;
&lt;!--     $(document).ready(function() { --&gt;
&lt;!--       $items = $(&#39;div#TOC li&#39;); --&gt;
&lt;!--       $items.each(function(idx) { --&gt;
&lt;!--         num_ul = $(this).parentsUntil(&#39;#TOC&#39;).length; --&gt;
&lt;!--         $(this).css({&#39;text-indent&#39;: num_ul * 10, &#39;padding-left&#39;: 0}); --&gt;
&lt;!--       }); --&gt;
&lt;!--     }); --&gt;
&lt;!-- &lt;/script&gt; --&gt;
&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; In classification problems, under and over sampling&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; techniques shift the distribution of predicted probabilities towards the minority class. If your problem requires accurate probabilities you will need to adjust your predictions in some way during post-processing (or at another step) to account for this&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;People new to predictive modeling may rush into using sampling procedures without understanding what these procedures are doing. They then sometimes get confused when their predictions appear way off (from those that would be expected according to the base rates in their data). I decided to write this vignette to briefly walk through an example of the implications of under or over sampling procedures on the base rates of predictions&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My examples will appear obvious to individuals with experience in predictive modeling with imbalanced classes. The code is pulled largely from a few emails I sent in early to mid 2018&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; to individuals new to data science. Like my other posts, you can view the source code on &lt;a href=&#34;https://github.com/brshallo/brshallo&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that this post is not about &lt;em&gt;what&lt;/em&gt; resampling procedures are or &lt;em&gt;why&lt;/em&gt; you might want to them&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, it is meant &lt;em&gt;only&lt;/em&gt; to demonstrate that such procedures change the base rates of your predictions (unless adjusted for).&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The proportion of TRUE to FALSE cases of the target in binary classification problems largely determines the base rate of the predictions produced by the model. Therefore if you use sampling techniques that change this proportion (e.g. to go from 5-95 to 50-50 TRUE-FALSE ratios) there is a good chance you will want to rescale / calibrate&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; your predictions before using them in the wild (if you care about things other than simply ranking your observations&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(modelr)
library(ggplot2)
library(gridExtra)
library(purrr)

theme_set(theme_bw())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;create-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create Data&lt;/h1&gt;
&lt;p&gt;Generate classification data with substantial class imbalance&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert log odds to probability
convert_lodds &amp;lt;- function(log_odds) exp(log_odds) / (1 + exp(log_odds))

set.seed(123)

minority_data &amp;lt;- tibble(rand_lodds = rnorm(1000, log(.03 / (1 - .03)), sd = 1),
       rand_probs = convert_lodds(rand_lodds)) %&amp;gt;% 
  mutate(target = map(.x = rand_probs, ~rbernoulli(100, p = .x))) %&amp;gt;% 
  unnest() %&amp;gt;% 
  mutate(id = row_number())

# Change the name of the same of the variables to make the dataset more
# intuitive to follow.
example &amp;lt;- minority_data %&amp;gt;% 
  select(id, target, feature = rand_lodds)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;In this dataset we have a class imbalance where our &lt;code&gt;target&lt;/code&gt; is composed of ~5% positive (&lt;code&gt;TRUE&lt;/code&gt;) cases and ~95% negative (&lt;code&gt;FALSE&lt;/code&gt;) cases.&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example %&amp;gt;% 
  count(target) %&amp;gt;% 
  mutate(proportion = round(n / sum(n), 3)) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;target&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;proportion&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.954&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Make 80-20 train - test split&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
train &amp;lt;- example %&amp;gt;% 
  sample_frac(0.80)

test &amp;lt;- example %&amp;gt;% 
  anti_join(train, by = &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;association-of-feature-and-target&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Association of ‘feature’ and ‘target’&lt;/h1&gt;
&lt;p&gt;We have one important input to our model named &lt;code&gt;feature&lt;/code&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;% 
  ggplot(aes(feature, fill = target))+
  geom_histogram()+
  labs(title = &amp;quot;Distribution of values of &amp;#39;feature&amp;#39;&amp;quot;,
       subtitle = &amp;quot;Greater values of &amp;#39;feature&amp;#39; associate with higher likelihood &amp;#39;target&amp;#39; = TRUE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;resample&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resample&lt;/h2&gt;
&lt;p&gt;Make a new sample &lt;code&gt;train_downsamp&lt;/code&gt; that keeps all positive cases in the training set and an equal number of randomly sampled negative cases so that the split is no longer 5-95 but becomes 50-50.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;minority_class_size &amp;lt;- sum(train$target)

set.seed(1234)

train_downsamp &amp;lt;- train %&amp;gt;% 
  group_by(target) %&amp;gt;% 
  sample_n(minority_class_size) %&amp;gt;% 
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See below for what the distribution of &lt;code&gt;feature&lt;/code&gt; looks like in the down-sampled dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_downsamp %&amp;gt;% 
  ggplot(aes(feature, fill = target))+
  geom_histogram()+
  labs(title = &amp;quot;Distribution of values of &amp;#39;feature&amp;#39; (down-sampled)&amp;quot;,
       subtitle = &amp;quot;Greater values of &amp;#39;feature&amp;#39; associate with higher likelihood &amp;#39;target&amp;#39; = TRUE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build Models&lt;/h2&gt;
&lt;p&gt;Train a logistic regression model to predict positive cases for &lt;code&gt;target&lt;/code&gt; based on &lt;code&gt;feature&lt;/code&gt; using the training dataset without any changes in the sample (i.e. with the roughly 5-95 class imbalance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_5_95 &amp;lt;- glm(target ~ feature, family = binomial(&amp;quot;logit&amp;quot;), data = train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Train a model with the down-sampled (i.e. 50-50) dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_50_50 &amp;lt;- glm(target ~ feature, family = binomial(&amp;quot;logit&amp;quot;), data = train_downsamp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the predictions from each of these models&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; onto our test set (and convert log-odd predictions to probabilities).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds &amp;lt;- test %&amp;gt;% 
  gather_predictions(mod_5_95, mod_50_50) %&amp;gt;% 
  mutate(pred_prob = convert_lodds(pred))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize distributions of predicted probability of the positive and negative cases for each model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_histogram()+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The predicted probabilities for the model built with the down-sampled 50-50 dataset are much higher than those built with the original 5-95 dataset. For example, let’s look at the predictions between these models for a particular observation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds %&amp;gt;% 
  filter(id == 1828) %&amp;gt;%
  arrange(id) %&amp;gt;% 
  select(-pred) %&amp;gt;% 
  knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;target&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;feature&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_prob&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mod_5_95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mod_50_50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This shows that when &lt;code&gt;feature&lt;/code&gt; is equal to -2.77, the model built without undersampling produces a prediction of 6% whereas the model built from the undersampled data would predict 56%. The former can be thought of as the predicted probability of the event whereas the latter would first need to be rescaled.&lt;/p&gt;
&lt;p&gt;If picking a decision threshold for the predictions, the model built from the undersampled dataset would have far more predictions of &lt;code&gt;TRUE&lt;/code&gt; compared to the rate of &lt;code&gt;TRUE&lt;/code&gt;s from the model built from the original training dataset&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rescale-predictions-to-predicted-probabilities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rescale Predictions to Predicted Probabilities&lt;/h2&gt;
&lt;p&gt;Isotonic Regression&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; or Platt scaling&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt; could be used. Such methods are used to calibrate outputted predictions and ensure they align with &lt;em&gt;actual&lt;/em&gt; probabilities. Recalibration techniques are typically used when you have models that may not output well-calibrated probabilities&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;. However these methods can also be used to rescale your outputs (as in this case). (In the case of linear models, there are also simpler approaches available&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_50_50_rescaled_calibrated &amp;lt;- train %&amp;gt;% 
  add_predictions(mod_50_50) %&amp;gt;% 
  glm(target ~ pred, family = binomial(&amp;quot;logit&amp;quot;), data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted &amp;lt;- test %&amp;gt;% 
  spread_predictions(mod_5_95, mod_50_50) %&amp;gt;% 
  rename(pred = mod_50_50) %&amp;gt;% 
  spread_predictions(mod_50_50_rescaled_calibrated) %&amp;gt;% 
  select(-pred) %&amp;gt;% 
  gather(mod_5_95, mod_50_50_rescaled_calibrated, key = &amp;quot;model&amp;quot;, value = &amp;quot;pred&amp;quot;) %&amp;gt;% 
  mutate(pred_prob = convert_lodds(pred)) 

test_with_preds_adjusted %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_histogram()+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the predictions have been calibrated according to their underlying base rate, you can see the distributions of the predictions between the models are essentially the same.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Density Plots&lt;/h2&gt;
&lt;p&gt;Rebuilding plots but using density distributions by class (rather than histograms based on counts).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_density(alpha = 0.3)+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_density(alpha = 0.3)+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lift-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lift Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted %&amp;gt;% 
  mutate(target = factor(target, c(&amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;))) %&amp;gt;% 
  filter(model == &amp;quot;mod_5_95&amp;quot;) %&amp;gt;%
  yardstick::lift_curve(target, pred) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-scaling-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing Scaling Methods&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Added after publishing&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://andrewpwheeler.com/&#34;&gt;Andrew Wheeler&lt;/a&gt; for his helpful disqus comment referencing another method for rescaling which prompted me to create a quick &lt;a href=&#34;https://gist.github.com/brshallo/24338a87b33e5d2ac98d200b1ccecfc5&#34;&gt;gist&lt;/a&gt; comparing platt scaling against using an offset/adjustment approach for rescaling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/4f6e2dee86039f3cbd23980414cac318cabd364459714b3520ffe00b870d13a4/68747470733a2f2f692e696d6775722e636f6d2f613245496e69392e706e67&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In the title I just mention Undersamping for brevities sake. Upsampling and downsampling are synonyms you may hear as well&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I expect the audience for this post may be rather limited.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I wrote this example After having conversations related to this a few times (and participants not grasping points that would become clear with demonstration).&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;before I started using &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;or any of a myriad of topics related to this.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;There are often pretty easy built-in ways to accommodate this.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;There are also other reasons you may not want to rescale your predictions… but in many cases you will want to.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Could have been more precise here…&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;no need for validation for this example&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;The higher incidence of TRUE values in the target at higher scores demonstrates the features predictive value.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;One built with 5-95 split the other with a downsampled 50-50 split.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;Of course you could just use different decision thresholds for the predictions as well.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Decision tree based approach&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;Logistic regression based approach&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;E.g. when using Support Vector Machines&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;In this case we are starting with a linear model hence we could also have just changed the intercept value to get the same affect. Rescaling methods act on the &lt;em&gt;predictions&lt;/em&gt; rather than the model parameters. Hence these scaling methods have the advantage of being generalizable as they are agnostic to model type.&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Feature Engineering with Sliding Windows and Lagged Inputs</title>
      <link>/2020/10/12/window-functions-for-resampling/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/12/window-functions-for-resampling/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;Load data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering-data-splits&#34;&gt;Feature Engineering &amp;amp; Data Splits&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-based-features-before-split-use-dplyr-or-similar&#34;&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-splits&#34;&gt;Data Splits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-features-after-split-use-recipes&#34;&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-specification-and-training&#34;&gt;Model Specification and Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-building-with-hyperparameter-tuning&#34;&gt;Model Building with Hyperparameter Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;The new &lt;code&gt;rsample::sliding_*()&lt;/code&gt; functions bring the windowing approaches used in &lt;a href=&#34;https://github.com/DavisVaughan/slider&#34;&gt;slider&lt;/a&gt; to the sampling procedures used in the &lt;a href=&#34;https://github.com/tidymodels&#34;&gt;tidymodels&lt;/a&gt; framework&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. These functions make evaluation of models with time-dependent variables easier&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For some problems you may want to take a traditional regression or classification based approach&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; while still accounting for the date/time-sensitive components of your data. In this post I will use the &lt;code&gt;tidymodels&lt;/code&gt; suite of packages to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;build lag based and non-lag based features&lt;/li&gt;
&lt;li&gt;set-up appropriate time series cross-validation windows&lt;/li&gt;
&lt;li&gt;evaluate performance of linear regression and random forest models on a regression problem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For my example I will use data from Wake County food inspections. I will try to predict the &lt;code&gt;SCORE&lt;/code&gt; for upcoming restaurant food inspections.&lt;/p&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load data&lt;/h1&gt;
&lt;p&gt;You can use Wake County’s open API (does not require a login/account) and the &lt;a href=&#34;https://github.com/r-lib/httr&#34;&gt;httr&lt;/a&gt; and &lt;a href=&#34;https://github.com/jeroen/jsonlite&#34;&gt;jsonlite&lt;/a&gt; packages to load in the data. You can also download the data directly from the Wake County &lt;a href=&#34;https://data.wakegov.com/datasets/1b08c4eb32f44a198277c418b71b3a48_2&#34;&gt;website&lt;/a&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(httr)
library(jsonlite)
library(tidymodels)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Get food inspections data:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_insp &amp;lt;- GET(&amp;quot;https://opendata.arcgis.com/datasets/ebe3ae7f76954fad81411612d7c4fb17_1.geojson&amp;quot;)

inspections &amp;lt;- content(r_insp, &amp;quot;text&amp;quot;) %&amp;gt;% 
  fromJSON() %&amp;gt;% 
  .$features %&amp;gt;%
  .$properties %&amp;gt;% 
  as_tibble()

inspections_clean &amp;lt;- inspections %&amp;gt;% 
  mutate(date = ymd_hms(DATE_) %&amp;gt;% as.Date()) %&amp;gt;% 
  select(-c(DATE_, DESCRIPTION, OBJECTID))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Get food locations data:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_rest &amp;lt;- GET(&amp;quot;https://opendata.arcgis.com/datasets/124c2187da8c41c59bde04fa67eb2872_0.geojson&amp;quot;) #json

restauraunts &amp;lt;- content(r_rest, &amp;quot;text&amp;quot;) %&amp;gt;% 
  fromJSON() %&amp;gt;% 
  .$features %&amp;gt;%
  .$properties %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  select(-OBJECTID)

restauraunts &amp;lt;- restauraunts %&amp;gt;% 
  mutate(RESTAURANTOPENDATE = ymd_hms(RESTAURANTOPENDATE) %&amp;gt;% as.Date())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Further prep:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Join the &lt;code&gt;inspections&lt;/code&gt; and &lt;code&gt;restaurants&lt;/code&gt; datasets&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Filter out extreme outliers in &lt;code&gt;SCORE&lt;/code&gt; (likely data entry errors)&lt;/li&gt;
&lt;li&gt;Filter to only locations of &lt;code&gt;TYPE&lt;/code&gt; restaurant&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Filter out potential duplicate entries&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It’s important to consider which fields should be excluded for ethical reasons. For our problem, we will say that any restaurant name or location information must be excluded&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspections_restaurants &amp;lt;- inspections_clean %&amp;gt;% 
  left_join(restauraunts, by = c(&amp;quot;HSISID&amp;quot;, &amp;quot;PERMITID&amp;quot;)) %&amp;gt;% 
  filter(SCORE &amp;gt; 50, FACILITYTYPE == &amp;quot;Restaurant&amp;quot;) %&amp;gt;% 
  distinct(HSISID, date, .keep_all = TRUE) %&amp;gt;% 
  select(-c(FACILITYTYPE, PERMITID)) %&amp;gt;% 
  select(-c(NAME, contains(&amp;quot;ADDRESS&amp;quot;), CITY, STATE, POSTALCODE, PHONENUMBER, X, Y, GEOCODESTATUS))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspections_restaurants %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 24,294
## Columns: 6
## $ HSISID             &amp;lt;chr&amp;gt; &amp;quot;04092017542&amp;quot;, &amp;quot;04092017542&amp;quot;, &amp;quot;04092017542&amp;quot;, &amp;quot;04...
## $ SCORE              &amp;lt;dbl&amp;gt; 94.5, 92.0, 95.0, 93.5, 93.0, 93.5, 92.5, 94.0, ...
## $ TYPE               &amp;lt;chr&amp;gt; &amp;quot;Inspection&amp;quot;, &amp;quot;Inspection&amp;quot;, &amp;quot;Inspection&amp;quot;, &amp;quot;Inspe...
## $ INSPECTOR          &amp;lt;chr&amp;gt; &amp;quot;Anne-Kathrin Bartoli&amp;quot;, &amp;quot;Laura McNeill&amp;quot;, &amp;quot;Laura ...
## $ date               &amp;lt;date&amp;gt; 2017-04-07, 2017-11-08, 2018-03-23, 2018-09-07,...
## $ RESTAURANTOPENDATE &amp;lt;date&amp;gt; 2017-03-01, 2017-03-01, 2017-03-01, 2017-03-01,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-data-splits&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feature Engineering &amp;amp; Data Splits&lt;/h1&gt;
&lt;p&gt;Discussion on issue &lt;a href=&#34;https://github.com/tidymodels/rsample/pull/168&#34;&gt;#168&lt;/a&gt; suggests that some features (those depending on prior observations) should be created before the data is split&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. The first and last sub-sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-based-features-before-split-use-dplyr-or-similar&#34;&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-features-after-split-use-recipes&#34;&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;provide examples of the types of features that should be created before and after splitting your data respectively. Lag based features can, in some ways, be thought of as ‘raw inputs’ as they should be created prior to building a &lt;code&gt;recipe&lt;/code&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;lag-based-features-before-split-use-dplyr-or-similar&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/h2&gt;
&lt;p&gt;Lag based features should generally be computed prior to splitting your data into “training” / “testing” (or “analysis” / “assessment”&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;) sets. This is because calculation of these features may depend on observations in prior splits&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;. Let’s build a few features where this is the case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prior &lt;code&gt;SCORE&lt;/code&gt; for &lt;code&gt;HSISID&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Average of prior 3 years of &lt;code&gt;SCORE&lt;/code&gt; for &lt;code&gt;HSISISD&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overall recent (year) prior average &lt;code&gt;SCORE&lt;/code&gt; (across &lt;code&gt;HSISISD&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Days since &lt;code&gt;RESTAURANTOPENDATE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Days since last inspection &lt;code&gt;date&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_time_feats &amp;lt;- inspections_restaurants %&amp;gt;% 
  arrange(date) %&amp;gt;% 
  mutate(SCORE_yr_overall = slider::slide_index_dbl(SCORE, 
                                                    .i = date, 
                                                    .f = mean, 
                                                    na.rm = TRUE, 
                                                    .before = lubridate::days(365), 
                                                    .after = -lubridate::days(1))
         ) %&amp;gt;% 
  group_by(HSISID) %&amp;gt;% 
  mutate(SCORE_lag = lag(SCORE),
         SCORE_recent = slider::slide_index_dbl(SCORE, 
                                                date, 
                                                mean, 
                                                na.rm = TRUE, 
                                                .before = lubridate::days(365*3), 
                                                .after = -lubridate::days(1), 
                                                .complete = FALSE),
         days_since_open = (date - RESTAURANTOPENDATE) / ddays(1),
         days_since_last = (date - lag(date)) / ddays(1)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  arrange(date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The use of &lt;code&gt;.after = -lubridate::days(1)&lt;/code&gt; prevents data leakage by ensuring that this feature does not include information from the current day in its calculation&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-splits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Splits&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Additional Filtering:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will presume that the model is only intended for restaurants that have previous inspections on record&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt; and will use only the most recent seven years of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_time_feats &amp;lt;- data_time_feats %&amp;gt;% 
  filter(date &amp;gt;= (max(date) - years(7)), !is.na(SCORE_lag))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Initial Split:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After creating our lag based features, we can split our data into training and testing splits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_split &amp;lt;- rsample::initial_time_split(data_time_feats, prop = .8)
train &amp;lt;- rsample::training(initial_split)
test &amp;lt;- rsample::testing(initial_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Resampling (Time Series Cross-Validation):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this problem we should evaluate our models using time series cross-validation&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;. This entails creating multiple ordered subsets of the training data where each set has a different assignment of observations into “analysis” or “assessment” data&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ideally the resampling scheme used for model evaluation mirrors how the model will be built and evaluated in production. For example, if the production model will be updated once every three months it makes sense that the “assessment” sets be this length. We can use &lt;code&gt;rsample::sliding_period()&lt;/code&gt; to set things up.&lt;/p&gt;
&lt;p&gt;For each set, we will use three years of “analysis” data for training a model and then three months of “assessment” data for evaluation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resamples &amp;lt;- rsample::sliding_period(train, 
                                     index = date, 
                                     period = &amp;quot;month&amp;quot;, 
                                     lookback = 36, 
                                     assess_stop = 3, 
                                     step = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will load in some helper functions I created for reviewing the dates of our resampling windows&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::source_gist(&amp;quot;https://gist.github.com/brshallo/7d180bde932628a151a4d935ffa586a5&amp;quot;)

resamples  %&amp;gt;% 
  extract_dates_rset() %&amp;gt;% 
  print() %&amp;gt;% 
  plot_dates_rset() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 6
##    splits         id     analysis_min analysis_max assessment_min assessment_max
##    &amp;lt;list&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;date&amp;gt;       &amp;lt;date&amp;gt;       &amp;lt;date&amp;gt;         &amp;lt;date&amp;gt;        
##  1 &amp;lt;split [6.2K/~ Slice~ 2013-10-09   2016-10-31   2016-11-01     2017-01-31    
##  2 &amp;lt;split [6.3K/~ Slice~ 2014-01-02   2017-01-31   2017-02-01     2017-04-28    
##  3 &amp;lt;split [6.6K/~ Slice~ 2014-04-01   2017-04-28   2017-05-01     2017-07-31    
##  4 &amp;lt;split [7.1K/~ Slice~ 2014-07-01   2017-07-31   2017-08-01     2017-10-31    
##  5 &amp;lt;split [7.5K/~ Slice~ 2014-10-01   2017-10-31   2017-11-01     2018-01-31    
##  6 &amp;lt;split [7.9K/~ Slice~ 2015-01-02   2018-01-31   2018-02-01     2018-04-30    
##  7 &amp;lt;split [8.3K/~ Slice~ 2015-04-01   2018-04-30   2018-05-01     2018-07-31    
##  8 &amp;lt;split [8.6K/~ Slice~ 2015-07-01   2018-07-31   2018-08-01     2018-10-31    
##  9 &amp;lt;split [9K/1K~ Slice~ 2015-10-01   2018-10-31   2018-11-01     2019-01-31    
## 10 &amp;lt;split [9.5K/~ Slice~ 2016-01-04   2019-01-31   2019-02-01     2019-04-30    
## 11 &amp;lt;split [9.9K/~ Slice~ 2016-04-01   2019-04-30   2019-05-01     2019-07-31    
## 12 &amp;lt;split [10.4K~ Slice~ 2016-07-01   2019-07-31   2019-08-01     2019-10-31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-12-window-functions-for-resampling_files/figure-html/check-resampling-splits-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For purposes of overall &lt;a href=&#34;#model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;, performance across each period will be weighted equally (regardless of number of observations in a period)&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-features-after-split-use-recipes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;Where possible, features should be created using the &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; package&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;. &lt;code&gt;recipes&lt;/code&gt; makes pre-processing convenient and helps prevent data leakage.&lt;/p&gt;
&lt;p&gt;It is OK to modify or transform a previously created lag based feature in a &lt;code&gt;recipes&lt;/code&gt; step. Assuming that you created the lag based input as well as your resampling windows in an appropriate manner, you should be safe from data leakage issues when modifying the variables during later feature engineering steps&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Some features / transformations I’ll make with &lt;code&gt;recipes&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collapse rare values for &lt;code&gt;INSPECTOR&lt;/code&gt; and &lt;code&gt;TYPE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;log transform &lt;code&gt;days_since_open&lt;/code&gt; and &lt;code&gt;days_since_last&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;add calendar based features&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec_general &amp;lt;- recipes::recipe(SCORE ~ ., data = train) %&amp;gt;% 
  step_rm(RESTAURANTOPENDATE) %&amp;gt;% 
  update_role(HSISID, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_other(INSPECTOR, TYPE, threshold = 50) %&amp;gt;% 
  step_string2factor(one_of(&amp;quot;TYPE&amp;quot;, &amp;quot;INSPECTOR&amp;quot;)) %&amp;gt;%
  step_novel(one_of(&amp;quot;TYPE&amp;quot;, &amp;quot;INSPECTOR&amp;quot;)) %&amp;gt;%
  # note that log transformations are completely superfluous for the random
  # forest model fit (is only valuable for the linear mod)
  step_log(days_since_open, days_since_last) %&amp;gt;% 
  step_date(date, features = c(&amp;quot;dow&amp;quot;, &amp;quot;month&amp;quot;)) %&amp;gt;% 
  update_role(date, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_zv(all_predictors()) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s peak at the features we will be passing into the model building step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prep(rec_general, data = train) %&amp;gt;% 
  juice() %&amp;gt;% 
  glimpse() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 17,048
## Columns: 12
## $ HSISID           &amp;lt;fct&amp;gt; 04092016152, 04092014520, 04092014483, 04092012102...
## $ TYPE             &amp;lt;fct&amp;gt; Inspection, Inspection, Inspection, Inspection, In...
## $ INSPECTOR        &amp;lt;fct&amp;gt; David Adcock, Naterra McQueen, Andrea Anover, othe...
## $ date             &amp;lt;date&amp;gt; 2013-10-09, 2013-10-09, 2013-10-09, 2013-10-09, 2...
## $ SCORE_yr_overall &amp;lt;dbl&amp;gt; 96.22766, 96.22766, 96.22766, 96.22766, 96.22766, ...
## $ SCORE_lag        &amp;lt;dbl&amp;gt; 96.0, 95.5, 97.0, 94.5, 97.5, 99.0, 96.0, 96.0, 10...
## $ SCORE_recent     &amp;lt;dbl&amp;gt; 96.75000, 95.75000, 97.50000, 95.25000, 96.75000, ...
## $ days_since_open  &amp;lt;dbl&amp;gt; 6.410175, 7.926964, 7.959276, 8.682029, 8.970432, ...
## $ days_since_last  &amp;lt;dbl&amp;gt; 4.709530, 4.941642, 4.934474, 4.875197, 5.117994, ...
## $ SCORE            &amp;lt;dbl&amp;gt; 98.5, 96.0, 96.0, 93.0, 95.0, 93.5, 95.0, 92.0, 98...
## $ date_dow         &amp;lt;fct&amp;gt; Wed, Wed, Wed, Wed, Wed, Wed, Wed, Wed, Wed, Thu, ...
## $ date_month       &amp;lt;fct&amp;gt; Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-specification-and-training&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Specification and Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Simple linear regression model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_mod &amp;lt;- parsnip::linear_reg() %&amp;gt;% 
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)

lm_workflow_rs &amp;lt;- workflows::workflow() %&amp;gt;% 
  add_model(lm_mod) %&amp;gt;% 
  add_recipe(rec_general) %&amp;gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;ranger&lt;/code&gt; Random Forest model (using defaults):&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rand_mod &amp;lt;- parsnip::rand_forest() %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)
  
set.seed(1234)
rf_workflow_rs &amp;lt;- workflow() %&amp;gt;% 
  add_model(rand_mod) %&amp;gt;% 
  add_recipe(rec_general) %&amp;gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;parsnip::null_model&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The NULL model will be helpful as a baseline Root Mean Square Error (RMSE) comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null_mod &amp;lt;- parsnip::null_model(mode = &amp;quot;regression&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;parsnip&amp;quot;)

null_workflow_rs &amp;lt;- workflow() %&amp;gt;% 
  add_model(null_mod) %&amp;gt;% 
  add_formula(SCORE ~ NULL) %&amp;gt;%
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See code in &lt;a href=&#34;#model-building-with-hyperparameter-tuning&#34;&gt;Model Building with Hyperparameter Tuning&lt;/a&gt; for more sophisticated examples that include hyperparameter tuning for &lt;code&gt;glmnet&lt;/code&gt;&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; and &lt;code&gt;ranger&lt;/code&gt; models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Evaluation&lt;/h1&gt;
&lt;p&gt;The next several code chunks extract the &lt;em&gt;average&lt;/em&gt; performance across “assessment” sets&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt; or extract the performance across each of the individual “assessment” sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_types &amp;lt;- list(&amp;quot;lm&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;null&amp;quot;)

avg_perf &amp;lt;- map(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
                collect_metrics) %&amp;gt;% 
  map2(mod_types, ~mutate(.x, source = .y)) %&amp;gt;% 
  bind_rows() &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_splits_metrics &amp;lt;- function(rs_obj, name){
  
  rs_obj %&amp;gt;% 
    select(id, .metrics) %&amp;gt;% 
    unnest(.metrics) %&amp;gt;% 
    mutate(source = name)
}

splits_perf &amp;lt;- map2(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
     mod_types, 
     extract_splits_metrics) %&amp;gt;% 
  bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall performance as well as the performance across splits suggests that both models were better than the baseline (the mean within the analysis set)&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; and that the linear model outperformed the random forest model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits_perf %&amp;gt;% 
  mutate(id = forcats::fct_rev(id)) %&amp;gt;% 
  ggplot(aes(x = .estimate, y = id, colour = source))+
  geom_vline(aes(xintercept = mean, colour = fct_relevel(source, c(&amp;quot;lm&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;null&amp;quot;))), 
           alpha = 0.4,
           data = avg_perf)+
  geom_point()+
  facet_wrap(~.metric, scales = &amp;quot;free_x&amp;quot;)+
  xlim(c(0, NA))+
  theme_bw()+
  labs(caption = &amp;quot;Vertical lines are average performance as captured by `tune::collect_metrics()`&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-12-window-functions-for-resampling_files/figure-html/plot-performance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could use a paired sample t-test to formally compare the random forest and linear models’ out-of-sample RMSE performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(
  filter(splits_perf, source == &amp;quot;lm&amp;quot;, .metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% pull(.estimate),
  filter(splits_perf, source == &amp;quot;rf&amp;quot;, .metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% pull(.estimate),
  paired = TRUE
) %&amp;gt;% 
  broom::tidy() %&amp;gt;% 
  mutate(across(where(is.numeric), round, 4)) %&amp;gt;% 
  knitr::kable() &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.high&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;alternative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.0839&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-3.7277&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0033&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1334&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0343&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Paired t-test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;two.sided&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This suggests the better performance by the linear model &lt;em&gt;is&lt;/em&gt; statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other potential steps:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is lots more we could do from here&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;. However the purpose of this post was to provide a short &lt;code&gt;tidymodels&lt;/code&gt; example that incorporates window functions from &lt;code&gt;rsample&lt;/code&gt; and &lt;code&gt;slider&lt;/code&gt; on a regression problem. For more resources on modeling and the &lt;code&gt;tidymodels&lt;/code&gt; framework, see &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels.org&lt;/a&gt; or &lt;a href=&#34;https://www.tmwr.org/&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;model-building-with-hyperparameter-tuning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Building with Hyperparameter Tuning&lt;/h2&gt;
&lt;p&gt;Below is code for tuning a &lt;code&gt;glmnet&lt;/code&gt; linear regression model (use &lt;code&gt;tune&lt;/code&gt; to optimize the L1/L2 penalty)&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec_glmnet &amp;lt;- rec_general %&amp;gt;% 
  step_dummy(all_predictors(), -all_numeric()) %&amp;gt;%
  step_normalize(all_predictors(), -all_nominal()) %&amp;gt;% 
  step_zv(all_predictors())

glmnet_mod &amp;lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&amp;gt;% 
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)

glmnet_workflow &amp;lt;- workflow::workflow() %&amp;gt;% 
  add_model(glmnet_mod) %&amp;gt;% 
  add_recipe(rec_glmnet)

glmnet_grid &amp;lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 
    0.2, 0.4, 0.6, 0.8, 1))

glmnet_tune &amp;lt;- tune::tune_grid(glmnet_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = glmnet_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And code to tune a &lt;code&gt;ranger&lt;/code&gt; Random Forest model, tuning the &lt;code&gt;mtry&lt;/code&gt; and &lt;code&gt;min_n&lt;/code&gt; parameters&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rand_mod &amp;lt;- parsnip::rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)
  
rf_workflow &amp;lt;- workflow() %&amp;gt;% 
  add_model(rand_mod) %&amp;gt;% 
  add_recipe(rec_general)

cores &amp;lt;- parallel::detectCores()

set.seed(1234)
rf_tune &amp;lt;- tune_grid(rf_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Link on doing regressions in slider: &lt;a href=&#34;https://twitter.com/dvaughan32/status/1247270052782637056?s=20&#34; class=&#34;uri&#34;&gt;https://twitter.com/dvaughan32/status/1247270052782637056?s=20&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rstudio lightning talk on &lt;code&gt;slider&lt;/code&gt;: &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/&#34; class=&#34;uri&#34;&gt;https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeltime&lt;/code&gt; package that applies &lt;code&gt;tidymodels&lt;/code&gt; suite to time series and forecasting problems: &lt;a href=&#34;https://business-science.github.io/modeltime/&#34; class=&#34;uri&#34;&gt;https://business-science.github.io/modeltime/&lt;/a&gt; (business-science course has more fully developed training materials on this topic as well)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;These were announced with version &lt;a href=&#34;https://github.com/tidymodels/rsample/blob/master/NEWS.md&#34;&gt;0.0.8&lt;/a&gt;. The help pages for &lt;code&gt;rsample&lt;/code&gt; (as well as the &lt;code&gt;slider&lt;/code&gt; package) are helpful resources for understanding the three types of sliding you can use, briefly these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sliding_window()&lt;/code&gt;: only takes into account order / position of dates&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sliding_index()&lt;/code&gt;: slide according to an index&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sliding_period()&lt;/code&gt;: slide according to an index and set k split points based on period (and other function arguments)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;rsample::sliding_index()&lt;/code&gt; and &lt;code&gt;rsample::sliding_period()&lt;/code&gt; are maybe the most useful additions as they allow you to do resampling based on a date/time index. For &lt;code&gt;sliding_index()&lt;/code&gt;, you usually want to make use of the &lt;code&gt;step&lt;/code&gt; argument (otherwise it defaults to having a split for every observation).&lt;/p&gt;
&lt;p&gt;I found &lt;code&gt;rsample::sliding_period()&lt;/code&gt; easier to get acquantied with than &lt;code&gt;rsample::sliding_index()&lt;/code&gt;. However within the &lt;code&gt;slider&lt;/code&gt; package I found &lt;code&gt;slider::sliding_index()&lt;/code&gt; easier to use than &lt;code&gt;slider::sliding_period()&lt;/code&gt;. Perhaps this makes sense as when setting sampling windows you are usually trying to return an object with far fewer rows, that is, collapsed to k number of rows (unless you are doing Leave-One-Out cross-validation). On the other hand, the &lt;code&gt;slider&lt;/code&gt; package is often used in a &lt;code&gt;mutate()&lt;/code&gt; step where you often want to output the same number of observations as are inputted. Perhaps then it is unsurprising the different scenarios when the &lt;code&gt;index&lt;/code&gt; vs &lt;code&gt;period&lt;/code&gt; approach feels more intuitive.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Previously users would have needed to use &lt;code&gt;rsample::rolling_origin()&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;As opposed to a more specialized time-series modeling approach.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This dataset is updated on an ongoing basis as Food Inspections are conducted. This makes it a poor choice as an example dataset (because results will vary if running in the future when more data has been collected). I used it because I am familiar with the dataset, it made for a good example, and because I wanted a publicly documented example of pulling in data using an API (even a simple one).&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;There is also “violations” dataset available, which may have additional useful features, but which I will ignore for this example.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;For this example I’m pretending that we only care about predicting &lt;code&gt;SCORE&lt;/code&gt; for restaurants… as opposed to food trucks or other entities that may receive inspections.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Or at least cases where historical data is claiming there were multiple inspections on the same day.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;In some cases you may need to be more careful than this and exclude information that are proxies for inappropriate fields as well. For example, pretend that the &lt;code&gt;INSPECTOR&lt;/code&gt;s are assigned based on region. In this case, &lt;code&gt;INSPECTOR&lt;/code&gt; would be a proxy for geographic information and perhaps warranting exclusion as well (in certain cases).&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Into training / testing sets or analysis / assessment sets.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;As discussed by Davis Vaughn at the end of this &lt;a href=&#34;https://gist.github.com/DavisVaughan/433dbdceb439c9be30ddcc78d836450d&#34;&gt;gist&lt;/a&gt;.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of &lt;a href=&#34;http://www.feat.engineering/resampling.html&#34;&gt;Feature Engineering and Selection…&lt;/a&gt; for further explanation.]&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;It is important that these features be created in a way that does not cause data leakage.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Which would not be available at the time of prediction.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;I’m a fan of the ability to use negative values in the &lt;code&gt;.after&lt;/code&gt; argument:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
This is a fairly obscure feature in {slider}, but I love it. Don’t want the current day in your rolling window? Set a negative &lt;code&gt;.after&lt;/code&gt; value to shift the end of the window backwards. For example:&lt;br&gt;&lt;br&gt;On day 5&lt;br&gt;.before = days(3)&lt;br&gt;.after = -days(1)&lt;br&gt;&lt;br&gt;Includes days:&lt;br&gt;[2, 4] &lt;a href=&#34;https://t.co/rG0IGuTj1c&#34;&gt;https://t.co/rG0IGuTj1c&lt;/a&gt;
&lt;/p&gt;
— Davis Vaughan (&lt;span class=&#34;citation&#34;&gt;@dvaughan32&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/dvaughan32/status/1233116713010573312?ref_src=twsrc%5Etfw&#34;&gt;February 27, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;If I did not make this assumption, I would need to impute the time based features at this point.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;Two helpful resources for understanding time series cross-validation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;From &lt;a href=&#34;https://eng.uber.com/forecasting-introduction/&#34;&gt;uber engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From &lt;a href=&#34;https://otexts.com/fpp3/tscv.html&#34;&gt;Forecasting Principles and Practices&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of &lt;a href=&#34;http://www.feat.engineering/resampling.html&#34;&gt;Feature Engineering and Selection…&lt;/a&gt; for further explanation.]&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;I’ve tweeted previously about helper functions for reviewing your resampling scheme:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
❤️new &lt;code&gt;rsample::sliding_*()&lt;/code&gt; funs by &lt;a href=&#34;https://twitter.com/dvaughan32?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@dvaughan32&lt;/span&gt;&lt;/a&gt;. It can take a minute to check that all arguments are set correctly. Here are helper funs I&#39;ve used to check that my resampling windows are constructed as intended: &lt;a href=&#34;https://t.co/HhSjuRzAsB&#34;&gt;https://t.co/HhSjuRzAsB&lt;/a&gt; may make into an &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/shiny?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#shiny&lt;/a&gt; dashboard. &lt;a href=&#34;https://t.co/sNloHfkh4a&#34;&gt;pic.twitter.com/sNloHfkh4a&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1314720234373287937?ref_src=twsrc%5Etfw&#34;&gt;October 10, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Note that using &lt;code&gt;rsample::sliding_period()&lt;/code&gt; is likely to produce different numbers of observations between splits.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;It could also make sense to weight performance metrics by number of observations. One way to do this, would be to use a control function to extract the predictions, and then evaluate the performance across the predictions. In my examples below I do keep the predictions, but end-up not doing anything with them. Alternatively you could weight the performance metric by number of observations. The justification for weighting periods of different number of observations equally is that noise may vary consistently across time windows – weighting by observations may allow an individual time period too much influence (simply because it happened to be that there were a greater proportion of inspections at that period).&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;For each split, this will then build the features for the assessment set based on each analysis set.&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Although I just do a simple &lt;code&gt;step_log()&lt;/code&gt; transform below, more sophisticated steps on lag based inputs would also be kosher, e.g. &lt;code&gt;step_pca()&lt;/code&gt;. However there is a good argument that many of these should be done prior to a &lt;code&gt;recipes&lt;/code&gt; step. For example, say you have missing values for some of the lag based inputs – in that case it may make sense to use a lag based method for imputation, which may work better than say a mean imputation using the training set. So, like many things, just be thoughtful and constantly ask youself what will be the ideal method while &lt;em&gt;being careful&lt;/em&gt; that, to the question of “will this data be available prior to the prediction?” that you can answer in the affirmitive.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;Our number of observations is relatively high compared to the number of features, so there is a good chance we will have relatively low penalties. While working interactively, I did not see any substantive difference in performance.&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;Remember that this is not weighted by observations, so each assessment set impacts the overall performance equally, regardless of small differences in number of observations.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;There is no baseline performance for Rsquared because the metric itself is based off amount of variance that is explained compared to the baseline (i.e. the mean).&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;You would likely iterate on the model building process (e.g. perform exploratory data analysis, review outliers in initial models, etc.) and eventually get to a final set of models to evaluate on the test set.&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;I added a few other links to the &lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt; section in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Our number of observations is relatively high compared to the number of features, so there is a good chance we will have relatively low penalties.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;This was taking a &lt;em&gt;long&lt;/em&gt; time and is part of why I decided to move the tuned examples to the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;.&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Short Examples of Best Practices When Writing Functions That Call dplyr Verbs</title>
      <link>/2020/06/25/using-across-to-build-functions-with-dplyr-with-notes-on-legacy-approaches/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/25/using-across-to-build-functions-with-dplyr-with-notes-on-legacy-approaches/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#function-expecting-one-column&#34;&gt;Function expecting one column&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions-allowing-multiple-columns&#34;&gt;Functions allowing multiple columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#older-approaches&#34;&gt;Older approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tidyverse/dplyr&#34;&gt;dplyr&lt;/a&gt;, the foundational &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; package, makes a trade-off between being easy to code in interactively at the expense of being more difficult to create functions with. The source of the trade-off is in how &lt;code&gt;dplyr&lt;/code&gt; evaluates column names (specifically, allowing for unquoted column names as argument inputs). Tidy evaluation has been under major development the last couple of years in order to make &lt;a href=&#34;https://dplyr.tidyverse.org/articles/programming.html&#34;&gt;programming with dplyr&lt;/a&gt; easier.&lt;/p&gt;
&lt;p&gt;During this development, there have been a variety of proposed methods for programming with &lt;code&gt;dplyr&lt;/code&gt;. In this post, I will document the current ‘best-practices’ with &lt;code&gt;dplyr&lt;/code&gt; 1.0.0. In the &lt;a href=&#34;#older-approaches&#34;&gt;Older approaches&lt;/a&gt; section I provide analogous examples that someone (i.e. myself) might have used during this maturation period.&lt;/p&gt;
&lt;p&gt;For a more full discussion on this topic see &lt;code&gt;dplyr&lt;/code&gt;’s documentation at &lt;a href=&#34;https://dplyr.tidyverse.org/articles/programming.html&#34;&gt;programming with dplyr&lt;/a&gt; and the various links referenced there.&lt;/p&gt;
&lt;div id=&#34;function-expecting-one-column&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Function expecting one column&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretend we want to create a function that calculates the sum of a given variable in a dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  
  summarise(df, {{var}} := sum({{var}}))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, cty)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to edit the variable in place and avoid using the special assignment operator &lt;code&gt;:=&lt;/code&gt;, you could use the new (in &lt;code&gt;dplyr&lt;/code&gt; 1.0.0) &lt;code&gt;across()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise(df, across({{vars}}, sum))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions-allowing-multiple-columns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functions allowing multiple columns&lt;/h1&gt;
&lt;p&gt;Using the &lt;code&gt;across()&lt;/code&gt; approach also allows you to input more than one variable, e.g. a user could call the following to get summaries on both &lt;code&gt;cty&lt;/code&gt; and &lt;code&gt;hwy&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, c(cty, hwy))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to compute multiple column summaries with different functions and you wanted to glue the function name onto your outputted column names&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, you could instead pass a named list of functions into the &lt;code&gt;.fns&lt;/code&gt; argument of &lt;code&gt;across()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise(df, across({{vars}}, list(sum = sum, mean = mean)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might want to create a function that can take in multiple sets of columns, e.g. the function below allows you to &lt;code&gt;group_by()&lt;/code&gt; one set of variables and &lt;code&gt;summarise()&lt;/code&gt; another set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_vars &amp;lt;- function(df, group_vars, sum_vars){
  df %&amp;gt;% 
    group_by(across({{group_vars}})) %&amp;gt;% 
    summarise(across({{sum_vars}}, list(sum = sum, mean = mean)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How a user would run &lt;code&gt;sum_group_vars()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_vars(mpg,
               c(model, year), 
               c(hwy, cty))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re feeling fancy, you could also make the input to &lt;code&gt;.fns&lt;/code&gt; an argument to &lt;code&gt;sum_group_vars()&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;older-approaches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Older approaches&lt;/h1&gt;
&lt;p&gt;Generally, I find the new &lt;code&gt;across()&lt;/code&gt; approaches introduced in &lt;code&gt;dplyr&lt;/code&gt; 1.0.0 are easier and more consistent to use than the methods that preceded them. However the methods in this section still work and are supported. They are just no longer the ‘recommended’ or most ‘modern’ approach available for creating functions that pass column names into &lt;code&gt;dplyr&lt;/code&gt; verbs.&lt;/p&gt;
&lt;p&gt;Prior to the introduction of the &lt;em&gt;bracket-bracket&lt;/em&gt;, &lt;code&gt;{{}}&lt;/code&gt;, you would have used the &lt;em&gt;&lt;code&gt;enquo()&lt;/code&gt; + bang-bang&lt;/em&gt; approach&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. The function below is equivalent to the &lt;code&gt;sum_var()&lt;/code&gt; function shown at the start of this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  var_quo &amp;lt;- enquo(var)
  summarise(df, !!var_quo := sum(!!var_quo))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To modify variables in-place you would have used the &lt;code&gt;*_at()&lt;/code&gt;, &lt;code&gt;*_if()&lt;/code&gt; or &lt;code&gt;*_all()&lt;/code&gt; function variants (which are now superseded by &lt;code&gt;across()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise_at(df, {{vars}}, sum)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to using &lt;code&gt;across()&lt;/code&gt; this method allows multiple variables being input. However what is weird about this function is that it requires the user wrapping the variable names in &lt;code&gt;vars()&lt;/code&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Hence to use the previously created function, a user would run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, vars(hwy, cty))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you could have the variable name inputs be character vectors by modifying the function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, vars){
  
  summarise_at(df, vars(one_of(vars)), sum)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which could be called by a user as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var(mpg, c(&amp;quot;hwy&amp;quot;, &amp;quot;cty&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These &lt;code&gt;*_at()&lt;/code&gt; variants also support inputting a list of functions, e.g. the below function would output both the sums and means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  
  summarise_at(df, vars(one_of(var)), list(sum = sum, mean = mean))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For multiple grouping variables and multiple variables to be summarised you could create:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groupsum &amp;lt;- function(df, group_vars, sum_vars){
  df %&amp;gt;% 
    group_by_at(vars(one_of(group_vars))) %&amp;gt;% 
    summarise_at(vars(one_of(sum_vars)), list(sum = sum, mean = mean))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which would be called by a user:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var(mpg, 
        c(&amp;quot;model&amp;quot;, &amp;quot;year&amp;quot;), 
        c(&amp;quot;hwy&amp;quot;, &amp;quot;cty&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a variety of similar spins you might take on handling tidy evaluation when creating these or similar types of functions.&lt;/p&gt;
&lt;p&gt;One other older approach perhaps worth mentioning (presented &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2019/working-with-names-and-expressions-in-your-tidy-eval-code/&#34;&gt;here&lt;/a&gt;) is “passing the dots”. Here is an example for if we want to &lt;code&gt;group_by()&lt;/code&gt; multiple columns and then &lt;code&gt;summarise()&lt;/code&gt; on just one column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_var &amp;lt;- function(df, sum_var, ...){
  df %&amp;gt;% 
    group_by(...) %&amp;gt;% 
    summarise({{sum_var}} := sum({{sum_var}}))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The limitation with this approach is that only one set of your inputs can have more than one variable in it, i.e. wherever you pass the &lt;code&gt;...&lt;/code&gt; in your function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;Image shared on social media was created using &lt;code&gt;xaringan&lt;/code&gt; and &lt;code&gt;flair&lt;/code&gt;. See &lt;a href=&#34;https://github.com/brshallo/dplyr-1.0.0-example&#34;&gt;dplyr-1.0.0-example&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/brshallo/dplyr-1.0.0-example/blob/master/dplyr-example-cropped.png?raw=true&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; 1.0.0 also now has &lt;a href=&#34;https://www.tidyverse.org/blog/2020/02/glue-strings-and-tidy-eval/&#34;&gt;support for using the glue&lt;/a&gt; package syntax for modifying variable names.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Doing this doesn’t require any tidy evaluation knowledge&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;There is also the &lt;code&gt;rlang::enquos()&lt;/code&gt; and &lt;code&gt;!!!&lt;/code&gt; operator for when the input has length greater than one.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;A niche function specific to tidy evaluation (which users might not think of).&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Use Flipbooks to Explain Your Code and Thought Process</title>
      <link>/2020/06/24/use-flipbooks-to-explain-your-code-and-thought-process/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/24/use-flipbooks-to-explain-your-code-and-thought-process/</guid>
      <description>


&lt;div id=&#34;learning-rs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Learning R’s &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Using the pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) is one of my favorite things about coding in R and the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;. However when it was first shown to me, I couldn’t understand what the &lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typed_query&#34;&gt;#rstats&lt;/a&gt; nut describing it was &lt;em&gt;so enthusiastic&lt;/em&gt; about. They tried to explain, “It means &lt;em&gt;and then&lt;/em&gt; do the next operation.” When that didn’t click for me, they continued (while becoming ever more excited) “It &lt;em&gt;passes the previous steps output into the first argument&lt;/em&gt; of the next function,” still… 😐😐😕.
Self-evident verbs in their code like &lt;code&gt;select()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;summarise()&lt;/code&gt; helped me nod along, partly following the operations. Though it wasn’t until I evaluated the code &lt;em&gt;line-by-line&lt;/em&gt; that I recognized the pipe’s elegance, power, beauty, simplicity 😄!&lt;/p&gt;
&lt;p&gt;Now, a few years and reads through &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt; later&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I will often share my work by keeping the code and output together and showing, line-by-line, what I am building towards. For example when…&lt;/p&gt;
&lt;p&gt;… giving a 2019 talk on &lt;em&gt;“Managing objects in analytics workflows, using lists as columns in dataframes”&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/gme4Fb9JVjk?start=258&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;… giving a 2017 talk on &lt;em&gt;“Getting started with ‘tidy’ data science in R”&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/eeCELJNWEuw?start=474&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;… promoting a recent blog post on &lt;em&gt;“Tidy pairwise operations”&lt;/em&gt; (though in this case I removed the code):&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
What is your &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; (or other &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; ) approach for doing arbitrary pairwise operations across variables? Mine is frequently something like:&lt;br&gt;&lt;br&gt;I. nest…&lt;br&gt;II. expand combos… &lt;br&gt;III. filter…&lt;br&gt;IV. map fun(s)…&lt;br&gt;…&lt;br&gt;&lt;br&gt;I wrote a post walking through this: &lt;a href=&#34;https://t.co/xRnRf5yh3m&#34;&gt;https://t.co/xRnRf5yh3m&lt;/a&gt; &lt;a href=&#34;https://t.co/Zvxey2gm3H&#34;&gt;pic.twitter.com/Zvxey2gm3H&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1271194908477591553?ref_src=twsrc%5Etfw&#34;&gt;June 11, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;p&gt;However each of these examples were built using PowerPoint (and a lot of copy and pasting of code + output). The series of images cannot be easily reproduced. In this post I’ll point to resources on how to create these sorts of code communication materials in ways that &lt;em&gt;are reproducible&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flipbooks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Flipbooks&lt;/h1&gt;
&lt;p&gt;When I started writing this post, I planned to call this type of output a “&lt;strong&gt;LEXPREX&lt;/strong&gt;” for “&lt;strong&gt;L&lt;/strong&gt;ine-by-line &lt;strong&gt;EX&lt;/strong&gt;ecution with &lt;strong&gt;PR&lt;/strong&gt;inted &lt;strong&gt;EX&lt;/strong&gt;amples” (and a name evocative of the inspiring &lt;a href=&#34;https://github.com/tidyverse/reprex&#34;&gt;reprex&lt;/a&gt; package by &lt;a href=&#34;https://twitter.com/JennyBryan%5D&#34;&gt;Jenny Bryan&lt;/a&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;). But, thankfully, an excellent solution containing thorough explanations (and a much better name) already existed, &lt;em&gt;flipbooks&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As described in the &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;flipbookr documentation&lt;/a&gt;, “flipbooks are tools that present side-by-side, aligned, incremental code-output.”&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/brshallo/flipbookr-gifs-examples/raw/master/example-r4ds.gif?raw=true&#34; alt=&#34;(Example inspired by ‘Many Models’ chapter of ‘R For Data Science’ by Grolemund &amp;amp; Wickham.)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;(Example inspired by ‘Many Models’ chapter of ‘R For Data Science’ by Grolemund &amp;amp; Wickham.)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At this point you should stop reading this blog and instead go learn about &lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr&#34;&gt;flipbookr&lt;/a&gt;. My post was largely written before I learned about this package. Hence, starting at &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2020/flipbooks-evangeline-reynolds/&#34;&gt;this presentation&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/EvaMaeRey&#34;&gt;Gina Reynolds&lt;/a&gt; or &lt;code&gt;flipbookr&lt;/code&gt;’s &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;about page&lt;/a&gt; will generally be a more productive use of your time. The remainder of this post discusses either tools adjacent to flipbooks or describes workflows that can also be found within &lt;code&gt;flipbookr&lt;/code&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-with-xaringan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example with xaringan&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt; package for making slideshows contains highlighting features (and is what &lt;code&gt;flipbookr&lt;/code&gt; is built-on). For highlighting &lt;em&gt;code&lt;/em&gt; you can use the trailing comment &lt;code&gt;#&amp;lt;&amp;lt;&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. For highlighting &lt;em&gt;output&lt;/em&gt; there is the &lt;code&gt;highlight.output&lt;/code&gt; code chunk option.&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe src=&#34;https://slides.yihui.org/xaringan/#31&#34; style=&#34;width: 560px; height: 315px;&#34;&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/mitchoharawild&#34;&gt;Mitchell O’Hara-Wild&lt;/a&gt;’s 2019 presentation on &lt;em&gt;“Flexible futures for &lt;a href=&#34;https://github.com/tidyverts/fable&#34;&gt;fable&lt;/a&gt; functionality”&lt;/em&gt; contains a helpful example where he uses these features to walk-through &lt;a href=&#34;https://github.com/mitchelloharawild/fable-combinations-2019/blob/6a55628e1ad156c0040676b7881a799f7f75370a/user2019/index.Rmd&#34;&gt;his code&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/DhDOTxojQ3k?start=554&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;See &lt;a href=&#34;#more-sophisticated-highlighting&#34;&gt;More sophisticated highlighting&lt;/a&gt; if your use-case requires more than line-level highlighting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animating-a-flipbook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Animating a flipbook&lt;/h1&gt;
&lt;p&gt;I sometimes want to convert a flipbook into a gif, e.g. when sharing an example in a README or a snippet of a concept on social media. If you ignored my prior entreaty, this is a second reminder to stop and go read about &lt;code&gt;flipbookr&lt;/code&gt;. The &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;template file&lt;/a&gt; now shows how to create gifs using &lt;code&gt;flipbookr&lt;/code&gt; (html) –&amp;gt; &lt;code&gt;pagedown&lt;/code&gt; (pdf) –&amp;gt; &lt;code&gt;magick&lt;/code&gt; (gif). I also describe this workflow and provide examples &lt;a href=&#34;https://github.com/brshallo/flipbookr-gifs-examples&#34;&gt;here&lt;/a&gt;, e.g.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/brshallo/flipbookr-gifs-examples/raw/master/example-riddler-solution.gif&#34; alt=&#34;(Example from a prior blog post, “Riddler Solutions: Pedestrian Puzzles”)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;(Example from a prior blog post, “Riddler Solutions: Pedestrian Puzzles”)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Closing note&lt;/h1&gt;
&lt;p&gt;I recommend exploring the &lt;a href=&#34;https://education.rstudio.com/blog/&#34;&gt;Rstudio Education blog&lt;/a&gt;. The site contains helpful resources for improving your technical communication. It was here that I stumbled on the post &lt;a href=&#34;https://education.rstudio.com/blog/2020/05/flair/&#34;&gt;Decorate your R code with flair&lt;/a&gt;. Reading this inspired me to make a first attempt at building a reproducible animation of line-by-line execution of R code (something I’d been wanting to do for ages). The positive response &amp;amp; feedback to my initial tweet led me to learn about &lt;code&gt;flipbookr&lt;/code&gt; and motivated additional actions (described in &lt;a href=&#34;#engagement-contributions&#34;&gt;Engagement &amp;amp; contributions&lt;/a&gt;) including the review and completion of this blog post.&lt;/p&gt;
&lt;p&gt;Finally, please go enjoy the beautiful examples you can find at the &lt;code&gt;flipbookr&lt;/code&gt; &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;about page&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;&lt;img src=&#34;/post/2020-06-16-use-flipbooks-to-explain-your-code-and-thought-process_files/flipbookr-example.gif&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;more-sophisticated-highlighting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More sophisticated highlighting&lt;/h2&gt;
&lt;p&gt;For more sophisticated highlighting of &lt;em&gt;code&lt;/em&gt;, use the &lt;a href=&#34;https://github.com/kbodwin/flair&#34;&gt;flair package&lt;/a&gt;. I’m not sure what to recommend for highlighting changes in &lt;em&gt;output&lt;/em&gt; to the console… perhaps &lt;a href=&#34;https://github.com/brodieG/diffobj&#34;&gt;diffobj&lt;/a&gt; would be an option. You could also just explicitly format the output, e.g. using &lt;a href=&#34;https://github.com/rstudio/gt&#34;&gt;gt&lt;/a&gt; or &lt;a href=&#34;https://github.com/haozhu233/kableExtra&#34;&gt;kableExtra&lt;/a&gt; for tabular outputs, or using geoms, annotations, etc. in &lt;a href=&#34;https://github.com/tidyverse/ggplot2&#34;&gt;ggplot&lt;/a&gt;s. And, of course, you can always dive into the html.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;engagement-contributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Engagement &amp;amp; contributions&lt;/h2&gt;
&lt;p&gt;Blogging is time consuming. Reaching out to package maintainers or making contributions (even small ones) on open-source software projects can be intimidating. As a &lt;em&gt;tiny&lt;/em&gt; success story, I documented actions that stemmed (in some part) from engaging with the #rstats online communities while working on this blog post topic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While this post was in draft form, I tweeted out my initial approach (that used the &lt;a href=&#34;https://github.com/kbodwin/flair&#34;&gt;flair&lt;/a&gt; package).
&lt;ul&gt;
&lt;li&gt;The next step might have been trying to improve upon this. Thankfully, instead, &lt;a href=&#34;https://twitter.com/KellyBodwin&#34;&gt;Kelly Bodwin&lt;/a&gt; pointed me to &lt;code&gt;flipbookr&lt;/code&gt;!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
P.S. &lt;br&gt;&lt;br&gt;The &lt;code&gt;flair_lines()&lt;/code&gt; function lets you highlight whole line(s) if you want! &lt;br&gt;&lt;br&gt;{flipbookr} is a better option for making gifs/slides like this, but {flair} + {pagedown} + {magick} might help if you want specialty or layered highlighting.
&lt;/p&gt;
— Kelly Bodwin (&lt;span class=&#34;citation&#34;&gt;@KellyBodwin&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/KellyBodwin/status/1272741205365764097?ref_src=twsrc%5Etfw&#34;&gt;June 16, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Kelly also created an &lt;a href=&#34;https://github.com/kbodwin/flair/issues/15&#34;&gt;issue&lt;/a&gt; to further discuss possible integrations between &lt;code&gt;flair&lt;/code&gt; and &lt;code&gt;flipbookr&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I remade my initial example using &lt;code&gt;flipbookr&lt;/code&gt; (&lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr/issues/22&#34;&gt;see issue&lt;/a&gt;).
&lt;ul&gt;
&lt;li&gt;I first created an &lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr/issues/21&#34;&gt;issue&lt;/a&gt; showing how to print &lt;code&gt;xaringan&lt;/code&gt; slides incrementally using &lt;code&gt;pagedown::chrome_print()&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;Which helped to close a related &lt;a href=&#34;https://github.com/rstudio/pagedown/issues/110&#34;&gt;issue&lt;/a&gt; on &lt;code&gt;xaringan&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Gina Reynolds made a variety of updates to &lt;code&gt;flipbookr&lt;/code&gt;, one of which included adding the html –&amp;gt; pdf –&amp;gt; gif workflow to the template 😄.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Big thanks to &lt;a href=&#34;https://twitter.com/grrrck?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@grrrck&lt;/span&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/statsgen?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@statsgen&lt;/span&gt;&lt;/a&gt; for helps and &lt;a href=&#34;https://twitter.com/xieyihui?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@xieyihui&lt;/span&gt;&lt;/a&gt; because {xaringan}! And to &lt;a href=&#34;https://twitter.com/brshallo?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/KellyBodwin?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@KellyBodwin&lt;/span&gt;&lt;/a&gt; for new ideas about how to share flipbooks, html -&amp;gt; pdf -&amp;gt; gif. Guidance now included in template update on this - this gif created w/ that workflow!🙏🤩
&lt;/p&gt;
— Gina Reynolds (&lt;span class=&#34;citation&#34;&gt;@EvaMaeRey&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/EvaMaeRey/status/1274837474460626945?ref_src=twsrc%5Etfw&#34;&gt;June 21, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;See my notes and solutions &lt;a href=&#34;https://brshallo.github.io/r4ds_solutions/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I also considered names such as &lt;code&gt;pexprex&lt;/code&gt;, &lt;code&gt;sexprex&lt;/code&gt;, &lt;code&gt;pripex&lt;/code&gt;, … I’ll let the reader guess at the acronyms.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Which I prefer over the alternatives of using the leading &lt;code&gt;*&lt;/code&gt; or wrapping the message in&lt;code&gt;{{}}&lt;/code&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Pairwise Operations</title>
      <link>/2020/06/03/tidy-2-way-column-combinations/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/03/tidy-2-way-column-combinations/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ii.-expand-combinations&#34;&gt;II. Expand combinations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iii.-filter-redundancies&#34;&gt;III. Filter redundancies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iv.-map-functions&#34;&gt;IV. Map function(s)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#v.-return-to-normal-dataframe&#34;&gt;V. Return to normal dataframe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vi.-bind-back-to-data&#34;&gt;VI. Bind back to data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functionalize&#34;&gt;Functionalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-creating-evaluating-features&#34;&gt;Example creating &amp;amp; evaluating features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-is-this-approach-inappropriate&#34;&gt;When is this approach inappropriate?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interactions-example-tidymodels&#34;&gt;Interactions example, tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expand-via-join&#34;&gt;Expand via join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nested-tibbles&#34;&gt;Nested tibbles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pivot-and-then-summarise&#34;&gt;Pivot and then summarise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gif-for-social-media&#34;&gt;Gif for social media&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tweets&#34;&gt;Tweets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Say you want to map an operation or list of operations across all two-way&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; combinations of a set of variables/columns in a dataframe. For example, you may be doing feature engineering and want to create a set of interaction terms, ratios, etc&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. You may be interested in computing a summary statistic across all pairwise combinations of a given set of variables&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. In some cases there may be a pairwise implementation already available, e.g. R’s &lt;code&gt;cor()&lt;/code&gt; function for computing correlations&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. In other cases one may not exist or is not easy to use&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. In this post I’ll walk through an example&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; explaining code and steps for setting-up arbitrary pairwise operations across sets of variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I’ll break my approach down into several steps:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I. Nest and pivot&lt;br /&gt;
II. Expand combinations&lt;br /&gt;
III. Filter redundancies&lt;br /&gt;
IV. Map function(s)&lt;br /&gt;
V. Return to normal dataframe&lt;br /&gt;
VI. Bind back to data&lt;/p&gt;
&lt;p&gt;If your interest is only in computing summary statistics (as opposed to modifying an existing dataframe with new columns / features), then only steps I - IV are needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevant software and style:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I will primarily be using R’s &lt;code&gt;tidyverse&lt;/code&gt; packages. I make frequent use of lists as columns within dataframes – if you are new to these, see my previous &lt;a href=&#34;https://www.youtube.com/watch?v=gme4Fb9JVjk&#34;&gt;talk&lt;/a&gt; and the resources&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; I link to in the description.&lt;/p&gt;
&lt;p&gt;Throughout this post, wherever I write “dataframe” I really mean “tibble” (a dataframe with minor changes to default options and printing behavior). Also note that I am using &lt;code&gt;dplyr&lt;/code&gt; 0.8.3 rather than the newly released 1.0.0&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other resources and open issues (updated 2020-06-14):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In particular, the comments in issue &lt;a href=&#34;https://github.com/tidymodels/corrr/issues/44&#34;&gt;44&lt;/a&gt; for the &lt;code&gt;corrr&lt;/code&gt; package contain excellent solutions for doing pairwise operations (the subject of this post)&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. Issue &lt;a href=&#34;https://github.com/tidymodels/corrr/issues/94&#34;&gt;94&lt;/a&gt; also features discussion on this topic. Throughout this post I will reference other alternative code/approaches (especially in the footnotes and the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’ll use the ames housing dataset across examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames &amp;lt;- AmesHousing::make_ames()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifically, I’ll focus on ten numeric columns that, based on a random sample of 1000 rows, show the highest correlation with &lt;code&gt;Sale_Price&lt;/code&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(2020)
ames_cols &amp;lt;- ames %&amp;gt;% 
  select_if(is.numeric) %&amp;gt;% 
  sample_n(1000) %&amp;gt;% 
  corrr::correlate() %&amp;gt;% 
  corrr::focus(Sale_Price) %&amp;gt;% 
  arrange(-abs(Sale_Price)) %&amp;gt;% 
  head(10) %&amp;gt;% 
  pull(term)

ames_subset &amp;lt;- select(ames, ames_cols) %&amp;gt;% 
  # Could normalize data or do other prep 
  # but is not pertinent for examples
  mutate_all(as.double)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;i.-nest-and-pivot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I. Nest and pivot&lt;/h2&gt;
&lt;p&gt;There are a variety of ways to make lists into columns within a dataframe. In the example below, I first use &lt;code&gt;summarise_all(.tbl = ames_subset, .funs = list)&lt;/code&gt; to create a one row dataframe where each column is a list containing a single element and each individual element corresponds with a numeric vector of length 2930.&lt;/p&gt;
&lt;p&gt;After nesting, I pivot&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; the columns leaving a dataframe with two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;var&lt;/code&gt; the variable names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vector&lt;/code&gt; a list where each element contains the associated vector&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists &amp;lt;- ames_subset %&amp;gt;% 
  summarise_all(list) %&amp;gt;% 
  pivot_longer(cols = everything(), 
               names_to = &amp;quot;var&amp;quot;, 
               values_to = &amp;quot;vector&amp;quot;) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    var            vector       
##    &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;       
##  1 Gr_Liv_Area    &amp;lt;dbl [2,930]&amp;gt;
##  2 Garage_Cars    &amp;lt;dbl [2,930]&amp;gt;
##  3 Garage_Area    &amp;lt;dbl [2,930]&amp;gt;
##  4 Total_Bsmt_SF  &amp;lt;dbl [2,930]&amp;gt;
##  5 First_Flr_SF   &amp;lt;dbl [2,930]&amp;gt;
##  6 Year_Built     &amp;lt;dbl [2,930]&amp;gt;
##  7 Full_Bath      &amp;lt;dbl [2,930]&amp;gt;
##  8 Year_Remod_Add &amp;lt;dbl [2,930]&amp;gt;
##  9 TotRms_AbvGrd  &amp;lt;dbl [2,930]&amp;gt;
## 10 Fireplaces     &amp;lt;dbl [2,930]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;a href=&#34;#pivot-and-then-summarise&#34;&gt;Pivot and then summarise&lt;/a&gt; for a nearly identical approach with just an altered order of steps. Also see &lt;a href=&#34;#nested-tibbles&#34;&gt;Nested tibbles&lt;/a&gt; for how you could create a list-column of dataframes&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; rather than vectors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if my variables are across rows not columns?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, pretend you want to see if &lt;code&gt;Sale_Price&lt;/code&gt; is different across &lt;code&gt;Mo_Sold&lt;/code&gt;. Perhaps you started by doing an F-test, found that to be significant, and now want to do pairwise t-tests across the samples of &lt;code&gt;Sale_Price&lt;/code&gt; for each &lt;code&gt;Mo_Sold&lt;/code&gt;. To set this up, you will want a &lt;code&gt;group_by()&lt;/code&gt; rather than a &lt;code&gt;pivot_longer()&lt;/code&gt; step. E.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames %&amp;gt;% 
  group_by(Mo_Sold) %&amp;gt;% 
  summarise(Sale_Price = list(Sale_Price)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 2
##    Mo_Sold Sale_Price 
##  *   &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;     
##  1       1 &amp;lt;int [123]&amp;gt;
##  2       2 &amp;lt;int [133]&amp;gt;
##  3       3 &amp;lt;int [232]&amp;gt;
##  4       4 &amp;lt;int [279]&amp;gt;
##  5       5 &amp;lt;int [395]&amp;gt;
##  6       6 &amp;lt;int [505]&amp;gt;
##  7       7 &amp;lt;int [449]&amp;gt;
##  8       8 &amp;lt;int [233]&amp;gt;
##  9       9 &amp;lt;int [161]&amp;gt;
## 10      10 &amp;lt;int [173]&amp;gt;
## 11      11 &amp;lt;int [143]&amp;gt;
## 12      12 &amp;lt;int [104]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At which point your data is in fundamentally the same form as was created in the previous code chunk (at least for if we only care about computing summary metrics that don’t require vectors of equal length&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;) so you can move onto &lt;a href=&#34;#ii.-expand-combinations&#34;&gt;II. Expand combinations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If the variables needed for your combinations of interest are across both rows and columns, you may want to use both &lt;code&gt;pivot_longer()&lt;/code&gt; and &lt;code&gt;group_by()&lt;/code&gt; steps and may need to make a few small modifications.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ii.-expand-combinations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;II. Expand combinations&lt;/h2&gt;
&lt;p&gt;I then use &lt;code&gt;tidyr::nesting()&lt;/code&gt; within &lt;code&gt;tidyr::expand()&lt;/code&gt; to make all 2-way combinations of our rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists_comb &amp;lt;- expand(df_lists,
                        nesting(var, vector),
                        nesting(var2 = var, vector2 = vector)) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 4
##    var        vector        var2           vector2      
##    &amp;lt;chr&amp;gt;      &amp;lt;list&amp;gt;        &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;       
##  1 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Fireplaces     &amp;lt;dbl [2,930]&amp;gt;
##  2 Fireplaces &amp;lt;dbl [2,930]&amp;gt; First_Flr_SF   &amp;lt;dbl [2,930]&amp;gt;
##  3 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Full_Bath      &amp;lt;dbl [2,930]&amp;gt;
##  4 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Garage_Area    &amp;lt;dbl [2,930]&amp;gt;
##  5 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Garage_Cars    &amp;lt;dbl [2,930]&amp;gt;
##  6 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Gr_Liv_Area    &amp;lt;dbl [2,930]&amp;gt;
##  7 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Total_Bsmt_SF  &amp;lt;dbl [2,930]&amp;gt;
##  8 Fireplaces &amp;lt;dbl [2,930]&amp;gt; TotRms_AbvGrd  &amp;lt;dbl [2,930]&amp;gt;
##  9 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Year_Built     &amp;lt;dbl [2,930]&amp;gt;
## 10 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Year_Remod_Add &amp;lt;dbl [2,930]&amp;gt;
## # ... with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;a href=&#34;#expand-via-join&#34;&gt;Expand via join&lt;/a&gt; for an alternative approach using the &lt;code&gt;dplyr::*_join()&lt;/code&gt; operations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You could make a strong case that this step should be after &lt;a href=&#34;#iii.-filter-redundancies&#34;&gt;III. Filter redundancies&lt;/a&gt;&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;. However putting it beforehand makes the required code easier to write and to read.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iii.-filter-redundancies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;III. Filter redundancies&lt;/h2&gt;
&lt;p&gt;Filter-out redundant columns, sort the rows, better organize the columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists_comb &amp;lt;- df_lists_comb %&amp;gt;% 
  filter(var != var2) %&amp;gt;% 
  arrange(var, var2) %&amp;gt;% 
  mutate(vars = paste0(var, &amp;quot;.&amp;quot;, var2)) %&amp;gt;% 
  select(contains(&amp;quot;var&amp;quot;), everything()) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 90 x 5
##    var          var2           vars                    vector       vector2     
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                   &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;      
##  1 Fireplaces   First_Flr_SF   Fireplaces.First_Flr_SF &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  2 Fireplaces   Full_Bath      Fireplaces.Full_Bath    &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  3 Fireplaces   Garage_Area    Fireplaces.Garage_Area  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  4 Fireplaces   Garage_Cars    Fireplaces.Garage_Cars  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  5 Fireplaces   Gr_Liv_Area    Fireplaces.Gr_Liv_Area  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  6 Fireplaces   Total_Bsmt_SF  Fireplaces.Total_Bsmt_~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  7 Fireplaces   TotRms_AbvGrd  Fireplaces.TotRms_AbvG~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  8 Fireplaces   Year_Built     Fireplaces.Year_Built   &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  9 Fireplaces   Year_Remod_Add Fireplaces.Year_Remod_~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
## 10 First_Flr_SF Fireplaces     First_Flr_SF.Fireplaces &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
## # ... with 80 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your operation of interest is associative&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;, apply a filter to remove additional redundant combinations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c_sort_collapse &amp;lt;- function(...){
  c(...) %&amp;gt;% 
    sort() %&amp;gt;% 
    str_c(collapse = &amp;quot;.&amp;quot;)
}

df_lists_comb_as &amp;lt;- df_lists_comb %&amp;gt;% 
  mutate(vars = map2_chr(.x = var, 
                         .y = var2, 
                         .f = c_sort_collapse)) %&amp;gt;%
  distinct(vars, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;iv.-map-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IV. Map function(s)&lt;/h2&gt;
&lt;p&gt;Each row of your dataframe now contains the relevant combinations of variables and is ready to have any arbitrary function(s) mapped across them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example with summary statistic&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, let’s say we want to compute the p-value of the correlation coefficient for each pair&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs_cor_pvalues &amp;lt;- df_lists_comb_as %&amp;gt;% 
  mutate(cor_pvalue = map2(vector, vector2, cor.test) %&amp;gt;% map_dbl(&amp;quot;p.value&amp;quot;),
         vars = fct_reorder(vars, -cor_pvalue)) %&amp;gt;% 
  arrange(cor_pvalue) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45 x 6
##    var        var2         vars                vector     vector2     cor_pvalue
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;               &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 First_Flr~ Total_Bsmt_~ First_Flr_SF.Total~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  2 Full_Bath  Gr_Liv_Area  Full_Bath.Gr_Liv_A~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  3 Garage_Ar~ Garage_Cars  Garage_Area.Garage~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  4 Gr_Liv_Ar~ TotRms_AbvG~ Gr_Liv_Area.TotRms~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  5 Year_Built Year_Remod_~ Year_Built.Year_Re~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  7.85e-301
##  6 First_Flr~ Gr_Liv_Area  First_Flr_SF.Gr_Li~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  8.17e-244
##  7 Garage_Ca~ Year_Built   Garage_Cars.Year_B~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  1.57e-219
##  8 Full_Bath  TotRms_AbvG~ Full_Bath.TotRms_A~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  1.24e-210
##  9 First_Flr~ Garage_Area  First_Flr_SF.Garag~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  8.16e-178
## 10 Garage_Ca~ Gr_Liv_Area  Garage_Cars.Gr_Liv~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  4.80e-175
## # ... with 35 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For fun, let’s plot the most significant associations onto a bar graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs_cor_pvalues %&amp;gt;% 
  head(15) %&amp;gt;% 
  mutate(cor_pvalue_nlog = -log(cor_pvalue)) %&amp;gt;% 
  ggplot(aes(x = vars, 
             y = cor_pvalue_nlog, 
             fill = is.infinite(cor_pvalue_nlog) %&amp;gt;% factor(c(T, F))))+
  geom_col()+
  coord_flip()+
  theme_bw()+
  labs(title = &amp;quot;We are confident that garage area and # of garage cars are correlated&amp;quot;,
       y = &amp;quot;Negative log of p-value of correlation coefficient&amp;quot;,
       x = &amp;quot;Variable combinations&amp;quot;,
       fill = &amp;quot;Too high to\nmeaningfully\ndifferentiate:&amp;quot;)+
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You could use this approach to calculate any pairwise summary statistic. For example, see &lt;a href=&#34;https://gist.github.com/brshallo/dc3c1f2f34519ca2a8a68024bc3a22e5&#34;&gt;gist&lt;/a&gt; where I calculate the K-S statistic across each combination of a group of distributions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you only care about computing summary statistics on your pairwise combinations, (and not adding new columns onto your original dataframe) you can stop here.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example with transformations&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Back to the feature engineering example, perhaps we want to create new features of the difference and quotient of each combination of our variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features_prep1 &amp;lt;- df_lists_comb %&amp;gt;% 
  mutate(difference = map2(vector, vector2, `-`),
         ratio = map2(vector, vector2, `/`))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;v.-return-to-normal-dataframe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;V. Return to normal dataframe&lt;/h2&gt;
&lt;p&gt;The next set of steps will put our data back into a more traditional form consistent with our starting dataframe/tibble.&lt;/p&gt;
&lt;p&gt;First let’s revert our data to a form similar to where it was at the end of &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt; where we had two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one with our variable names&lt;/li&gt;
&lt;li&gt;a second containing a list-column of vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features_prep2 &amp;lt;- new_features_prep1 %&amp;gt;% 
  pivot_longer(cols = c(difference, ratio)) %&amp;gt;% # 1
  mutate(name_vars = str_c(var, name, var2, sep = &amp;quot;.&amp;quot;)) %&amp;gt;% # 2
  select(name_vars, value) # 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of each line of code above is a number corresponding with the following explanations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;if we had done just one operation, this step would not be needed, but we did multiple operations, created multiple list-columns (&lt;code&gt;difference&lt;/code&gt; and &lt;code&gt;ratio&lt;/code&gt;) which we need to get into a single list-column&lt;/li&gt;
&lt;li&gt;create new variable name that combines constituent variable names with name of transformation&lt;/li&gt;
&lt;li&gt;remove old columns&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next we simply apply the inverse of those operations performed in &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features &amp;lt;- new_features_prep2 %&amp;gt;% 
  pivot_wider(values_from = value,
              names_from = name_vars) %&amp;gt;%
  unnest(cols = everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new features will add a good number of columns onto our original dataset&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(new_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2930  180&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;vi.-bind-back-to-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;VI. Bind back to data&lt;/h2&gt;
&lt;p&gt;I then bind the new features back onto the original subsetted dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_data_features &amp;lt;- bind_cols(ames_subset, new_features)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At which point I could do further exploring, feature engineering, model building, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;functionalize&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functionalize&lt;/h1&gt;
&lt;p&gt;I put these steps into a few (unpolished) functions found at &lt;a href=&#34;https://gist.github.com/brshallo/f92a5820030e21cfed8f823a6e1d56e1&#34;&gt;this gist&lt;/a&gt;&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::source_gist(&amp;quot;https://gist.github.com/brshallo/f92a5820030e21cfed8f823a6e1d56e1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in get(genname, envir = envir) : object &amp;#39;testthat_print&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mutate_pairwise()&lt;/code&gt; takes in your dataframe, the set of numeric columns to create pairwise combinations from, and a list of functions&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt; to apply.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-creating-evaluating-features&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example creating &amp;amp; evaluating features&lt;/h1&gt;
&lt;p&gt;Let’s use the new &lt;code&gt;mutate_pairwise()&lt;/code&gt; function to create new columns for the differences and quotients between all pairwise combinations of our variables of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_data_features_example &amp;lt;- mutate_pairwise(
  df = mutate_if(ames, is.numeric, as.double),
  one_of(ames_cols),
  funs = list(&amp;quot;/&amp;quot;, &amp;quot;-&amp;quot;),
  funs_names = list(&amp;quot;ratio&amp;quot;, &amp;quot;difference&amp;quot;),
  associative = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps you want to calculate some measure of association between your features and a target of interest. To keep things simple, I’ll remove any columns that contain any NA’s or infinite values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;features_keep &amp;lt;- ames_data_features_example %&amp;gt;% 
  keep(is.numeric) %&amp;gt;% 
  keep(~sum(is.na(.) | is.infinite(.)) == 0) %&amp;gt;% 
  colnames()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe, for some reason, you want to see the statistical significance of the correlation of each feature with &lt;code&gt;Sale_Price&lt;/code&gt; when weighting by &lt;code&gt;Lot_Area&lt;/code&gt;. I’ll calculate these across variables (and a random sample of 1500 observations) then plot them on a histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
ames_data_features_example %&amp;gt;% 
  sample_n(1500) %&amp;gt;% 
  summarise_at(
    .vars = features_keep[!(features_keep %in% c(&amp;quot;Sale_Price&amp;quot;, &amp;quot;Lot_Area&amp;quot;))],
    .funs = ~weights::wtd.cor(., Sale_Price, weight = Lot_Area)[1]) %&amp;gt;% 
  gather() %&amp;gt;% # gather() is an older version of pivot_longer() w/ fewer parameters
  ggplot(aes(x = value))+
  geom_vline(xintercept = 0, colour = &amp;quot;lightgray&amp;quot;, size = 2)+
  geom_histogram()+
  scale_x_continuous(labels = scales::comma)+
  labs(title = &amp;quot;Distribution of correlations with Sale_Price&amp;quot;,
       subtitle = &amp;quot;Weighted by Lot Area&amp;quot;,
       x = &amp;quot;Weighted correlation coefficient&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If doing predictive modeling or inference you may want to fit any transformations and analysis into a &lt;code&gt;tidymodels&lt;/code&gt; pipeline or other framework. For some brief notes on this see &lt;a href=&#34;#interactions-example-tidymodels&#34;&gt;Interactions example, tidymodels&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-is-this-approach-inappropriate&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;When is this approach inappropriate?&lt;/h1&gt;
&lt;p&gt;Combinatorial growth is very fast&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;. As you increase either the number of variables in your pool or the size of each set, you will quickly bump into computational limitations.&lt;/p&gt;
&lt;p&gt;Tidyverse packages are optimized to be efficient. However operations with matrices or other specialized formats&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; are generally faster&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt; than with dataframes/tibbles. If you are running into computational challenges but prefer to stick with a tidyverse aesthetic (which uses dataframes as a cornerstone), you might:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use heuristics to reduce the number of variables or operations you need to perform (e.g. take a sample, use a preliminary filter, a step-wise like iteration, etc.)&lt;/li&gt;
&lt;li&gt;Look for packages that abstract the storage and computationally heavy operations away&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; and then return back an output in a convenient form&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improve the efficiency of your code (e.g. filter redundancies before rather than after expanding combinations)&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consider parralelizing&lt;/li&gt;
&lt;li&gt;Use matrices&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is sometimes an urge to do &lt;em&gt;everything&lt;/em&gt; in a tidy way, which is not necessary. For example, you &lt;em&gt;could&lt;/em&gt; use an approach like the one I walk through to calculate pairwise correlations between each of your variables. However, the &lt;code&gt;cor()&lt;/code&gt; function would do this much more efficiently if called on a matrix or traditional dataframe without list-columns (though you could also use the &lt;code&gt;corrr&lt;/code&gt; package within the &lt;code&gt;tidymodels&lt;/code&gt; suite which calls &lt;code&gt;cor()&lt;/code&gt; in the back-end&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;However, for many operations…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there may not be an efficient pairwise implementation available / accessible&lt;/li&gt;
&lt;li&gt;the slower computation may not matter or can be mitigated in some way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These situations&lt;a href=&#34;#fn30&#34; class=&#34;footnote-ref&#34; id=&#34;fnref30&#34;&gt;&lt;sup&gt;30&lt;/sup&gt;&lt;/a&gt; are where the approach I walked through is most appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;interactions-example-tidymodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions example, tidymodels&lt;/h2&gt;
&lt;p&gt;A good example for creating and evaluating interaction terms&lt;a href=&#34;#fn31&#34; class=&#34;footnote-ref&#34; id=&#34;fnref31&#34;&gt;&lt;sup&gt;31&lt;/sup&gt;&lt;/a&gt; is in &lt;a href=&#34;http://www.feat.engineering/complete-enumeration.html#complete-enumeration-simple-screening&#34;&gt;The Brute-Force Approach to Identifying Predictive Interactions, Simple Screening&lt;/a&gt; section of &lt;em&gt;Max Kuhn&lt;/em&gt; and &lt;em&gt;Kjell Johnson’s&lt;/em&gt; (free) online book “Feature Engineering and Selection: A Practical Approach for Predictive Models”.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/topepo/FES/blob/master/07_Detecting_Interaction_Effects/7_04_The_Brute-Force_Approach_to_Identifying_Predictive_Interactions/ames_pairwise.R&#34;&gt;source code&lt;/a&gt; shows another approach for combining variables. The author uses…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;combn()&lt;/code&gt; to create all combinations of variable names which are then…&lt;/li&gt;
&lt;li&gt;turned into formulas and passed into &lt;code&gt;recipes::step_interact()&lt;/code&gt;, specifying the new columns to be created&lt;a href=&#34;#fn32&#34; class=&#34;footnote-ref&#34; id=&#34;fnref32&#34;&gt;&lt;sup&gt;32&lt;/sup&gt;&lt;/a&gt;…&lt;/li&gt;
&lt;li&gt;for each interaction term…&lt;/li&gt;
&lt;li&gt;in each associated model being evaluated&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The example uses a mix of packages and styles and is not a purely tidy approach – &lt;code&gt;tidymodels&lt;/code&gt; has also gone through a lot of development since “Feature Engineering and Selection…” was published in 2019&lt;a href=&#34;#fn33&#34; class=&#34;footnote-ref&#34; id=&#34;fnref33&#34;&gt;&lt;sup&gt;33&lt;/sup&gt;&lt;/a&gt;. Section 11.2 on &lt;a href=&#34;http://www.feat.engineering/greedy-simple-filters.html&#34;&gt;Greedy Search Methods, Simple Filters&lt;/a&gt; is also highly relevant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expand-via-join&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expand via join&lt;/h2&gt;
&lt;p&gt;You can take advantage of join&lt;a href=&#34;#fn34&#34; class=&#34;footnote-ref&#34; id=&#34;fnref34&#34;&gt;&lt;sup&gt;34&lt;/sup&gt;&lt;/a&gt; behavior to create all possible row combinations. In this case, the output will be the same as shown when using &lt;code&gt;expand()&lt;/code&gt; (except row order will be different).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(mutate(df_lists, id = 1),
          mutate(df_lists, id = 1) %&amp;gt;% rename_at(vars(-one_of(&amp;quot;id&amp;quot;)), paste0, &amp;quot;2&amp;quot;)) %&amp;gt;%
  select(-id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nested-tibbles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nested tibbles&lt;/h2&gt;
&lt;p&gt;Creates list of tibbles rather than list of vectors – typically the first way lists as columns in dataframes is introduced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_subset %&amp;gt;% 
  pivot_longer(everything(), names_to = &amp;quot;var&amp;quot;, values_to = &amp;quot;list&amp;quot;) %&amp;gt;% 
  group_by(var) %&amp;gt;% 
  nest()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pivot-and-then-summarise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pivot and then summarise&lt;/h2&gt;
&lt;p&gt;(Almost) equivalent to the example in &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;. Steps just run in a different order (row order will also be different).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_test %&amp;gt;% 
  pivot_longer(cols = everything(), 
             names_to = &amp;quot;var&amp;quot;, 
             values_to = &amp;quot;vector&amp;quot;) %&amp;gt;% 
  group_by(var) %&amp;gt;% 
  summarise_all(list)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gif-for-social-media&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gif for social media&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AmesHousing::make_ames() %&amp;gt;% 
  select(Year = Year_Sold, Price = Sale_Price) %&amp;gt;% 
  # I.
  group_by(Year) %&amp;gt;% 
  summarise(Price = list(Gr_Liv_Area)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  # II.
  expand(nesting(Year, Price),
         nesting(Year2 = Year, Price2 = Price)
  ) %&amp;gt;%
  # III.
  filter(Year != Year2) %&amp;gt;% 
  mutate(Years = map2_chr(.x = Year, 
                          .y = Year2, 
                          .f = c_sort_collapse)) %&amp;gt;%
  distinct(Years, .keep_all = TRUE) %&amp;gt;% 
  select(-Years) %&amp;gt;% 
  #IV.
  mutate(ks_test = map2(Price, 
                        Price2, 
                        stats::ks.test) %&amp;gt;% map_dbl(&amp;quot;p.value&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/pairwise-comparison-gif-edit.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Actual gif was created by embedding above code into a presentation and exporting it as a gif and then making a few minor edits.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tweets&lt;/h2&gt;
&lt;p&gt;A few tweets as documentation of thinking. &lt;em&gt;Many of these were added after publishing this post.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Original tweet + R bloggers tweet:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
What is your &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; (or other &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; ) approach for doing arbitrary pairwise operations across variables? Mine is frequently something like:&lt;br&gt;&lt;br&gt;I. nest…&lt;br&gt;II. expand combos… &lt;br&gt;III. filter…&lt;br&gt;IV. map fun(s)…&lt;br&gt;…&lt;br&gt;&lt;br&gt;I wrote a post walking through this: &lt;a href=&#34;https://t.co/xRnRf5yh3m&#34;&gt;https://t.co/xRnRf5yh3m&lt;/a&gt; &lt;a href=&#34;https://t.co/Zvxey2gm3H&#34;&gt;pic.twitter.com/Zvxey2gm3H&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1271194908477591553?ref_src=twsrc%5Etfw&#34;&gt;June 11, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Tidy Pairwise Operations {&lt;a href=&#34;https://t.co/mI5r2e5ttN&#34;&gt;https://t.co/mI5r2e5ttN&lt;/a&gt;} &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DataScience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DataScience&lt;/a&gt;
&lt;/p&gt;
— R-bloggers (&lt;span class=&#34;citation&#34;&gt;@Rbloggers&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Rbloggers/status/1307007573611155456?ref_src=twsrc%5Etfw&#34;&gt;September 18, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet with link to gist of other example applying this approach:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/W_R_Chase?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@W_R_Chase&lt;/span&gt;&lt;/a&gt; alludes to using &lt;code&gt;expand()&lt;/code&gt; for a solution but takes a different approach. I wrote a short gist that fleshes in what a &lt;code&gt;tidyr::expand()&lt;/code&gt; approach to this problem could look like: &lt;a href=&#34;https://t.co/agloPgJR1r&#34;&gt;https://t.co/agloPgJR1r&lt;/a&gt; (2/3)
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1272411480193974273?ref_src=twsrc%5Etfw&#34;&gt;June 15, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet about &lt;code&gt;widyr&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Will add &lt;code&gt;widyr&lt;/code&gt; to my set of tools for tidy pairwise operations: &lt;a href=&#34;https://t.co/NSxNC3nehK&#34;&gt;https://t.co/NSxNC3nehK&lt;/a&gt; !&lt;br&gt;&lt;br&gt;Seems to overlap some w/ tidymodels, eg &lt;code&gt;corrr&lt;/code&gt;📦 (&lt;a href=&#34;https://twitter.com/thisisdaryn?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@thisisdaryn&lt;/span&gt;&lt;/a&gt; ) or could imagine widely_kmeans() as a &lt;code&gt;recipe&lt;/code&gt;/&lt;code&gt;embed&lt;/code&gt; step…&lt;a href=&#34;https://twitter.com/drob?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@drob&lt;/span&gt;&lt;/a&gt; any tips on when/how you use these in combination? &lt;a href=&#34;https://t.co/dAsJtNW7Vo&#34;&gt;https://t.co/dAsJtNW7Vo&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1313966803488583686?ref_src=twsrc%5Etfw&#34;&gt;October 7, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet with more efficient approach (for case when just combining multiple columns and returning output of equal number of rows, i.e. mutating rather than summarising):&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
How you can use &lt;code&gt;dplyr::mutate()&lt;/code&gt; to return a dataframe consisting of all combinations of arbitrary pairwise operations across a selection of columns: &lt;a href=&#34;https://t.co/RxwtbmWqap&#34;&gt;https://t.co/RxwtbmWqap&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; … &lt;a href=&#34;https://t.co/UpJw0pGPUd&#34;&gt;pic.twitter.com/UpJw0pGPUd&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1316851879658356736?ref_src=twsrc%5Etfw&#34;&gt;October 15, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet with approach using &lt;code&gt;corrr&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/mattwrkntn?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@mattwrkntn&lt;/span&gt;&lt;/a&gt; solution for grouped, pairwise operations in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; is excellent: &lt;a href=&#34;https://t.co/wjbO4fRqxP&#34;&gt;https://t.co/wjbO4fRqxP&lt;/a&gt; . Just substitute in the new corrr::colpair_map for corrr::correlate and could use for any pairwise summarizing operation. &lt;a href=&#34;https://t.co/0Wf5KpQ4th&#34;&gt;pic.twitter.com/0Wf5KpQ4th&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1355622893200289793?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Will focus on two-way example in this post, but could use similar methods to make more generalizable solution across n-way examples. If I were to do this, the code below would change. E.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to use &lt;code&gt;pmap*()&lt;/code&gt; operations over &lt;code&gt;map2*()&lt;/code&gt; operations&lt;/li&gt;
&lt;li&gt;I’d need to make some functions that make it so I can remove all the places where I have &lt;code&gt;var&lt;/code&gt; and &lt;code&gt;var2&lt;/code&gt; type column names hard-coded&lt;/li&gt;
&lt;li&gt;Alternatively, I might shift approaches and make better use of &lt;code&gt;combn()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Though this “throw everything and the kitchen-sink” approach may not always be a good idea.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I’ve done this type of operation in a variety of ways. Sometimes without any really good reason as to why I used one approach or another. It isn’t completely clear (at least to me) the recommended way of doing these type of operations within the tidyverse – hence the diversity of my approaches in the past and deciding to document the typical steps in the approach I take… via writing this post.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Or the tidymodels implementation &lt;code&gt;corrr::correlate()&lt;/code&gt; in the &lt;a href=&#34;https://corrr.tidymodels.org/&#34;&gt;corrr&lt;/a&gt; package.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;or is not in a style you prefer&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;I’ll also reference related approaches / small tweaks (putting those materials in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;. This is by no means an exhaustive list (e.g. don’t have an example with a &lt;code&gt;for&lt;/code&gt; loop or with a &lt;code&gt;%do%&lt;/code&gt; operator). The source code of my post on &lt;a href=&#34;https://www.bryanshalloway.com/2020/02/13/fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs/&#34;&gt;Ambiguous Absolute Value&lt;/a&gt; signs shows a related but more complex / messy approach on a combinatorics problem.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;In particular, the chapters on “Iteration” and “Many Models” in &lt;a href=&#34;https://r4ds.had.co.nz/iteration.html&#34;&gt;R for Data Science&lt;/a&gt;. I would also recommend Rebecca Barter’s &lt;a href=&#34;http://www.rebeccabarter.com/blog/2019-08-19_purrr/&#34;&gt;Learn to purrr&lt;/a&gt; blog post.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;The new &lt;code&gt;dplyr&lt;/code&gt; 1.0.0. contains new functions that would have been potentially useful for several of these operations. I highly recommend checking these updates out in the various &lt;a href=&#34;https://www.tidyverse.org/tags/dplyr-1-0-0/&#34;&gt;recent posts&lt;/a&gt; by Hadley Wickham. Some of the major updates (potentially relevant to the types of operations I’ll be discussing in my post):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;new approach for across-column operations (replacing &lt;code&gt;_at()&lt;/code&gt;, &lt;code&gt;_if()&lt;/code&gt;, &lt;code&gt;_all()&lt;/code&gt; variants with &lt;code&gt;across()&lt;/code&gt; function)&lt;/li&gt;
&lt;li&gt;brought-back rowwise operations&lt;/li&gt;
&lt;li&gt;emphasize ability to output tibbles / multiple columns in core &lt;code&gt;dplyr&lt;/code&gt; verbs. This is something I had only taken advantage of occassionally in the past (&lt;a href=&#34;https://stackoverflow.com/a/54725732/9059865&#34;&gt;example&lt;/a&gt;), but will look to use more going forward.&lt;/li&gt;
&lt;/ul&gt;
&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;f I’d spotted this issue initially I’m not sure I would have written this post. However what this post offers is a more verbose treatment of the problem which may be useful for people newer to pairwise operations or the tidyverse.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;For technical reasons, I also converted all integer types to doubles – was getting integer overflow problems in later operations before changing. &lt;a href=&#34;https://stackoverflow.com/questions/8804779/what-is-integer-overflow-in-r-and-how-can-it-happen&#34;&gt;Thread&lt;/a&gt; on integer overflow in R. In this post I’m not taking a disciplined approach to feature engineering. For example it may make sense to normalize the variables so that variable combinations would be starting on a similar scale. This could be done using &lt;code&gt;recipes::step_normalize()&lt;/code&gt; or with code like &lt;code&gt;dplyr::mutate_all(df, ~(. - mean(.)) / sd(.))&lt;/code&gt; .&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Note that this part of the problem is one where I actually find using &lt;code&gt;tidyr::gather()&lt;/code&gt; easier – but I’ve been forcing myself to switch over to using the &lt;code&gt;pivot_()&lt;/code&gt; functions over &lt;code&gt;spread()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt;.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;The more common approach.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;If your variables are across rows you are likely concerned with getting summary metrics rather than creating new features – as if your data is across rows there is nothing guaranteeing you have the same number of observations or that they are lined-up appropriately. If you &lt;em&gt;are&lt;/em&gt; interested in creating new features, you should probably have first reshaped your data to ensure each column represented a variable.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;As switching these would be more computationally efficient – see &lt;a href=&#34;#when-is-this-approach-inappropriate&#34;&gt;When is this approach inappropriate?&lt;/a&gt; for notes related to this. Switching the order here would suggest using approaches with the&lt;code&gt;combn()&lt;/code&gt; function.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;I.e. has the same output regardless of the order of the variables. E.g. multiplication or addition but not subtraction or division.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;Function(s) that output vectors of length 1 (or less than length of input vectors).&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;Note that the pairwise implementation &lt;code&gt;psych::corr.test()&lt;/code&gt; could have been used on your original subsetted dataframe, see &lt;a href=&#34;https://stackoverflow.com/questions/13112238/a-matrix-version-of-cor-test&#34;&gt;stack overflow thread&lt;/a&gt;.&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;Function(s) that output vector of length equal to length of input vectors.&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Did not print this output because cluttered-up page with so many column names.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;Steps I - III and V &amp;amp; VI are essentially direct copies of the code above. The approach I took with Step IV may take more effort to follow as it requires understanding a little &lt;code&gt;rlang&lt;/code&gt; and could likely have been done more simply.&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;Must have two vectors as input, but do not need to be infix functions.&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Non-technical article discussing combinatorial explosion in context of company user growth targets: &lt;a href=&#34;https://medium.com/@TorBair/exponential-growth-isn-t-cool-combinatorial-growth-is-85a0b1fdb6a5&#34;&gt;Exponential Growth Isn’t Cool. Combinatorial Growth Is.&lt;/a&gt;.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;E.g. &lt;a href=&#34;https://github.com/Rdatatable/data.table&#34;&gt;data.table&lt;/a&gt; dataframes&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;Hence, if you are doing operations across combinations of lots of variables it may not make sense to do the operations directly within dataframes.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;Much (if not most) of the &lt;code&gt;tidyverse&lt;/code&gt; (and the R programming language generally) is about creating a smooth interface between the analyst/scientist and the back-end complexity of the operations they are performing. Projects like &lt;a href=&#34;https://spark.rstudio.com/&#34;&gt;sparklyr&lt;/a&gt;, &lt;a href=&#34;https://db.rstudio.com/dbi/&#34;&gt;DBI&lt;/a&gt;, &lt;a href=&#34;https://github.com/rstudio/reticulate&#34;&gt;reticulate&lt;/a&gt;, &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;, and &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; (to name a few) represent cases where this &lt;em&gt;interface&lt;/em&gt; role of R is most apparent.&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;For tidyverse packages, this is often returned into or in the form of a dataframe.&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;Could make better use of &lt;code&gt;combn()&lt;/code&gt; function to help.&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Depending on the complexity may just need to brush-up on your linear algebra.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;&lt;code&gt;corrr&lt;/code&gt; can also be used to run the operation on databases that may have larger data than you could fit on your computer.&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn30&#34;&gt;&lt;p&gt;Likely more common for many, if not most, analysts and data scientists.&lt;a href=&#34;#fnref30&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn31&#34;&gt;&lt;p&gt;I.e. multiplying two variables together&lt;a href=&#34;#fnref31&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn32&#34;&gt;&lt;p&gt;Created upon the recipe being &lt;em&gt;baked&lt;/em&gt; or &lt;em&gt;juiced&lt;/em&gt; – if you have not checked it out, &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; is AWESOME!&lt;a href=&#34;#fnref32&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn33&#34;&gt;&lt;p&gt;Maybe at a future date I’ll make a post writing out the example here using the newer approaches now available in &lt;code&gt;tidymodels&lt;/code&gt;. &lt;a href=&#34;https://gist.github.com/brshallo/674ff06608c1a55fefb8d5dc49896d65&#34;&gt;Gist&lt;/a&gt; of &lt;code&gt;combn_ttible()&lt;/code&gt;… starting place for if I ever get to that write-up.&lt;a href=&#34;#fnref33&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn34&#34;&gt;&lt;p&gt;Could also have used &lt;code&gt;right_join()&lt;/code&gt; or &lt;code&gt;full_join()&lt;/code&gt;.&lt;a href=&#34;#fnref34&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>animatrixr &amp; Visualizing Matrix Transformations pt. 2</title>
      <link>/2020/02/24/animatrixr-visualizing-matrix-transformations-pt-2/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/24/animatrixr-visualizing-matrix-transformations-pt-2/</guid>
      <description>


&lt;p&gt;This post is a continuation on my post from last week on &lt;a href=&#34;https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/&#34;&gt;Visualizing Matrix Transformations with gganimate&lt;/a&gt;. Both posts are largely inspired by &lt;a href=&#34;https://twitter.com/3blue1brown&#34;&gt;Grant Sanderson’s&lt;/a&gt; beautiful video series &lt;a href=&#34;https://www.youtube.com/watch?v=kYB8IZa5AuE&amp;amp;list=PL_w8oSr1JpVCZ5pKXHKz6PkjGCbPbSBYv&amp;amp;index=4&#34;&gt;The Essence of Linear Algebra&lt;/a&gt; and wanting to continue messing around with &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;Thomas Lin Peterson’s&lt;/a&gt; fantastic &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; package in R.&lt;/p&gt;
&lt;p&gt;As with the last post, I’ll describe trying to (very loosely) recreate a &lt;em&gt;small&lt;/em&gt; part of the visualizations showing the geometry of matrix multiplication and changing basis vectors (using &lt;code&gt;gganimate&lt;/code&gt; in R). (Once again, just in the 2x2 case.)&lt;/p&gt;
&lt;p&gt;If you are &lt;em&gt;really&lt;/em&gt; interested in building visualizations like the ones shown on 3Blue1Brown, you should check-out the associated &lt;a href=&#34;https://github.com/3b1b/manim&#34;&gt;manim&lt;/a&gt; project on github.&lt;/p&gt;
&lt;div id=&#34;topics-to-cover&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Topics to cover&lt;/h1&gt;
&lt;p&gt;I had two major sections in the Appendix of last week’s post:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Multiple matrix transformations”&lt;/li&gt;
&lt;li&gt;“Potential improvements” (where I mostly describe limitations around visualizing rotations)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post expands on these topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animatrixr-and-multiple-matrix-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;animatrixr&lt;/code&gt; and multiple matrix transformations&lt;/h1&gt;
&lt;p&gt;Sanderson discusses the value in sometimes decomposing a matrix transformation and thinking of its parts sequentially. I created a &lt;strong&gt;toy&lt;/strong&gt; package &lt;code&gt;animatrixr&lt;/code&gt; for building chained matrix transformations that can then be animated using &lt;code&gt;gganimate&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;animatrixr::add_transformation()&lt;/code&gt; lets you chain together matrix transformations with R’s pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, let’s consider three matrix transformations: horizontal sheer –&amp;gt; vertical sheer –&amp;gt; reflection across x-axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

if (!requireNamespace(&amp;quot;animatrixr&amp;quot;)) devtools::install_github(&amp;#39;brshallo/animatrixr&amp;#39;)
library(animatrixr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sheer_horizontal &amp;lt;- tribble(~ x, ~ y,
                      1, 0.5,
                      0, 1) %&amp;gt;%
  as.matrix()

sheer_vertical &amp;lt;- tribble(~ x, ~ y,
                      1, 0,
                      0.5, 1) %&amp;gt;%
  as.matrix()

reflect_x &amp;lt;- tribble(~ x, ~ y,
                      1, 0,
                      0, -1) %&amp;gt;%
  as.matrix() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s visualize the transformations being applied sequentially:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(sheer_horizontal) %&amp;gt;% 
  add_transformation(sheer_vertical) %&amp;gt;% 
  add_transformation(reflect_x, 
                     seq_fun = animatrixr::seq_matrix_l,
                     n_frames = 40) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/vsheer-hsheer-reflect-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;add_transformation()&lt;/code&gt; explicitly creates in-between frames for a given transformation. The &lt;code&gt;seq_fun&lt;/code&gt; argument allows you to define the interpolation method, for example whether the coordinates should (during the animation) follow a linear path (default) or the angle of a rotation.&lt;/p&gt;
&lt;p&gt;It would be nice to add-in functionality where the final transformation object could then be added to layers of a ggplot (though I’ve done nothing towards this except add an argument in &lt;code&gt;animatrixr::animate_matrix()&lt;/code&gt; for displaying the &lt;a href=&#34;https://github.com/lockedata/datasauRus&#34;&gt;datasauRus&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;(Warning: &lt;code&gt;animatrixr&lt;/code&gt; is severely limited, as discussed in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; and in package documentation. However you can find it at the “brshallo/animatrixr” repo on &lt;a href=&#34;https://github.com/brshallo/animatrixr&#34;&gt;my github page&lt;/a&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-rotations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing rotations&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;seq_fun&lt;/code&gt; argument within &lt;code&gt;add_transformation()&lt;/code&gt; specifies frames in-between the start and end states after a matrix transformation. By default it uses &lt;code&gt;animatrixr::seq_matrix_l&lt;/code&gt; which changes in-between coordinates linearly (as does &lt;code&gt;gganimate&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s look at a rotation where the in-between coordinates are interpolated linearly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rotate_90 &amp;lt;- tribble(~ x, ~ y,
                        cos(pi / 2), -sin(pi / 2),
                        sin(pi / 2), cos(pi / 2)) %&amp;gt;%
  as.matrix()

matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(rotate_90) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-linear-1.gif&#34; width=&#34;71%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Linear interpolation makes the rotation transformation appear scrunched during the animation (from how we intuitively think of a rotation) as the coordinate points take a straight line path to their positions after applying the transformation&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To make the in-between coordinates instead follow the angle of rotation we could change the &lt;code&gt;seq_fun&lt;/code&gt; from &lt;code&gt;animatrixr::seq_matrix_l&lt;/code&gt; to &lt;code&gt;animatrixr::seq_matrix_lp&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(rotate_90, seq_fun = animatrixr::seq_matrix_lp) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-polar-sheer-linear-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;During the rotation portion of the animation &lt;code&gt;gganimate&lt;/code&gt; is still tweening images linearly, however the frames &lt;code&gt;add_transformation()&lt;/code&gt; creates are now following along the angle of rotation of the transformation. Hence the animation ends-up approximating a curved path.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;seq_matrix_lp()&lt;/code&gt; needs improvement and was just set-up to work for toy examples – it really only looks ‘right’ if doing rotations off of &lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} 1 &amp;amp; 0\\0  &amp;amp; 1 \end{array}\right)\]&lt;/span&gt; See &lt;a href=&#34;#showing-rotations&#34;&gt;Showing rotations&lt;/a&gt; in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; for additional detail on how this is set-up and the various limitations with &lt;code&gt;animatrixr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Happy animatrixing!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# animatrixr::rotation_matrix() is helper function for creating matrix
# transformations of rotations
matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(animatrixr::rotation_matrix(pi / 2),
                     seq_fun = animatrixr::seq_matrix_lp) %&amp;gt;% 
  add_transformation(matrix(c(1, 0.5, 0, 1), nrow = 2)) %&amp;gt;% 
  add_transformation(matrix(c(1, 0, 0, -1), nrow = 2)) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/unnamed-chunk-1-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;using-animatrixr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;animatrixr&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;This is a toy package (very hastily written). I have not put effort into thinking about making it usable for others. Also, some parts just don’t really work or aren’t set-up quite right… (as noted in the README and elsewhere in the package). But feel free to check-it out / improve it / make something better! Let me know if you do!&lt;/p&gt;
&lt;p&gt;This has been a fun dabble into thinking (at least surface level) about animation. Though I don’t have any plans to add onto this (or write any more posts on this topic). If I do add anything, it will most likely just be cleaning-up the decomposition methods in the &lt;code&gt;seq_matrix*()&lt;/code&gt; functions. But no plans&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes-on-seq-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes on seq functions&lt;/h2&gt;
&lt;p&gt;Below are additional notes on the &lt;code&gt;animatrixr::seq_matrix*&lt;/code&gt; functions. They need some work, but here is a description of how they are currently set-up.&lt;/p&gt;
&lt;div id=&#34;showing-rotations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Showing rotations&lt;/h3&gt;
&lt;p&gt;To animate the rotation of a transformation, &lt;code&gt;add_transformation(m = matrix(c(0, 1, -1, 0), nrow = 2), seq_fun = seq_matrix_lp)&lt;/code&gt; explicitly creates in-between frames on the path the points would follow if they were instead following polar coordinates along the angle of rotation. In the next few sections I’ll discuss the process for doing this (again, this is not necessarily an ideal set-up).&lt;/p&gt;
&lt;p&gt;Given any 2x2 matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} a &amp;amp; b\\ c &amp;amp; d \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;you can use the equation &lt;code&gt;atan2(c, a)&lt;/code&gt; to extract the angle of rotation from the matrix&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and then create a sequence from the starting angle of rotation to the final angle of rotation.&lt;/p&gt;
&lt;p&gt;For example, if my start angle is &lt;span class=&#34;math inline&#34;&gt;\(0^\circ\)&lt;/span&gt;, and final angle of rotation is at &lt;span class=&#34;math inline&#34;&gt;\(38^\circ\)&lt;/span&gt; and I have 20 frames, then my sequence would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0^\circ, 2^\circ, ... 38^\circ\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A rotation matrix is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} cos(\theta) &amp;amp; -sin(\theta)\\ sin(\theta) &amp;amp; cos(\theta) \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence I can convert my sequence of angles into a sequence of matrices that define the rotations applied for each explicit in-between frame.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(\begin{array}{cc} cos(0^\circ) &amp;amp; -sin(0^\circ)\\ sin(0^\circ) &amp;amp; cos(0^\circ) \end{array}\right), 
\left(\begin{array}{cc} cos(2^\circ) &amp;amp; -sin(2^\circ)\\ sin(2^\circ) &amp;amp; cos(2^\circ) \end{array}\right)...
\left(\begin{array}{cc} cos(28^\circ) &amp;amp; -sin(28^\circ)\\ sin(28^\circ) &amp;amp; cos(28^\circ) \end{array}\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seq_matrix_lp-applied-on-non-standard-unit-basis-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;seq_matrix_lp&lt;/code&gt; applied on non-standard unit basis vectors&lt;/h3&gt;
&lt;p&gt;If you input a matrix transformation into &lt;code&gt;seq_matrix_lp&lt;/code&gt; that is not a pure rotation from the unit vectors it will decompose the matrix into a &lt;em&gt;rotation&lt;/em&gt; component and &lt;em&gt;other&lt;/em&gt; component&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, the &lt;em&gt;other&lt;/em&gt; component creates a sequence of matrices that have the in-between frames interpolated linearly. The sequence of &lt;em&gt;rotation&lt;/em&gt; and &lt;em&gt;other&lt;/em&gt; matrices are then recomposed to provide the final sequence.&lt;/p&gt;
&lt;p&gt;This approach means that non-pure rotations on the unit vectors, etc. will not really look like rotations. I would need to factor in other components (e.g. scale) to improve this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-rotation-first&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Show rotation first&lt;/h3&gt;
&lt;p&gt;Beyond &lt;code&gt;seq_matrip_l()&lt;/code&gt; and &lt;code&gt;seq_matrix_lp()&lt;/code&gt;, I made another seq_matrix* function: &lt;code&gt;seq_matrix_rotate_first&lt;/code&gt; which (like &lt;code&gt;seq_matrix_lp&lt;/code&gt;) also decomposes a matrix into rotation and other components. Rather than interpolating these separately and then recomposing them (as &lt;code&gt;seq_matrix_lp&lt;/code&gt; does) &lt;code&gt;seq_matrix_rotate_first&lt;/code&gt; works by interpolating them separately and then applying the decomposed sequences sequentially – so the entire rotation component of the transformation will be animated and then the ‘other’ component will be animated (this makes for twice as many frames when there is a ‘rotation’ and ‘other’ component in the transformation matrix).&lt;/p&gt;
&lt;p&gt;I.e. starting from our identity matrix and applying a single matrix transformation, it will automatically decompose this and animate the decomposed parts in two steps, &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; –&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and then from &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; –&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. Below is an example of the animation for the transformation matrix:
&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} 0 &amp;amp; -1\\1  &amp;amp; -0.5 \end{array}\right)\]&lt;/span&gt;
(which could be decomposed into a rotation and a sheer part).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transformation_matrix &amp;lt;- sheer_vertical %*% animatrixr::rotation_matrix(pi/4)

matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(transformation_matrix, seq_fun = seq_matrix_rotate_first) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-sheer-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;There are (especially) a lot of problems with this function currently and I don’t recommend using it e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;only works (at all correctly) if starting from standard unit vectors (hence cannot really be combined into a chain of matrix transformations)&lt;/li&gt;
&lt;li&gt;rotation component extracted will vary depending on what ‘other’ is within M
E.g. if M = {rotation}{vertical sheer} vs. M = {rotation}{horizontal sheer} – rotation component will look different&lt;/li&gt;
&lt;li&gt;I defaulted the amount of frames given to the rotation component to be the same as the amount of frames given to other component. If the size of the rotation is small relative to the other part of the transformation (or vice versa) the timing will feel slow/jumpy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Provides a cleaner approach for doing this compared to the clunky method I walked through in my post last week.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;All visualizations from last week used this linear interpolation method.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I discuss this at more length in my previous post – see the sub-section in the “Appendix”, “Problem of squeezing during rotation”.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;However I also hadn’t planned on writing a follow-up post… so who knows…&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://computergraphics.stackexchange.com/questions/3932/animating-a-smooth-linear-transformation&#34;&gt;post&lt;/a&gt; referencing this.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;To find the ‘other’ component of a matrix transformation… say &lt;em&gt;M&lt;/em&gt; represents the overall matrix transformation, in &lt;a href=&#34;#showing-rotations&#34;&gt;Showing rotations&lt;/a&gt; I described how to calculate &lt;em&gt;R&lt;/em&gt; (the rotation component), hence to calculate &lt;em&gt;A&lt;/em&gt;, ‘other’, I do:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[AR = M\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ARR^{-1} = MR^{-1}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[A = MR^{-1}\]&lt;/span&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>