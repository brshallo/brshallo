<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>business on Bryan Shalloway&#39;s Blog</title>
    <link>/tags/business/</link>
    <description>Recent content in business on Bryan Shalloway&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/business/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weighting Confusion Matrices by Outcomes and Observations</title>
      <link>/2020/12/08/weighting-classification-outcomes/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/08/weighting-classification-outcomes/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-performance-metrics&#34;&gt;Model Performance Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lending-data-example&#34;&gt;Lending Data Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#starter-code&#34;&gt;Starter Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighting-by-classification-outcomes&#34;&gt;Weighting by Classification Outcomes&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#metrics-across-decision-thresholds&#34;&gt;Metrics Across Decision Thresholds&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighting-by-observations&#34;&gt;Weighting by Observations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing-note&#34;&gt;Closing note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#weights-of-observations-during-and-prior-to-modeling&#34;&gt;Weights of Observations During and Prior to Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notes-on-cost-sensitive-classification&#34;&gt;Notes on Cost Sensitive Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighted-classification-metrics&#34;&gt;Weighted Classification Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions-on-cost-sensitive-classification&#34;&gt;Questions on Cost Sensitive Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#arriving-at-weights&#34;&gt;Arriving at Weights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Weighting in predictive modeling may take multiple forms and occur at different steps in the model building process.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When selecting observations to be used in model training&lt;/li&gt;
&lt;li&gt;During model training&lt;/li&gt;
&lt;li&gt;After model training, during model evaluation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The focus of this post is on the last stage&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. I will describe two types of weighting that can be applied in late stage model evaluation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#weighting-by-classification-outcomes&#34;&gt;Weighting by Classification Outcomes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighting-by-observations&#34;&gt;Weighting by Observations&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Specifically with the aim of identifying ideal cut-points for making class predictions.&lt;/p&gt;
&lt;p&gt;(See &lt;a href=&#34;#weights-of-observations-during-and-prior-to-modeling&#34;&gt;Weights of Observations During and Prior to Modeling&lt;/a&gt; in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; for a brief discussion on forms of weighting applied at other steps in predictive modeling.)&lt;/p&gt;
&lt;div id=&#34;model-performance-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Performance Metrics&lt;/h1&gt;
&lt;p&gt;Most common metrics used in classification problems – e.g. accuracy, precision, recall/sensitivity, specificity, Area Under the ROC curve&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; (AUC) or Precision-Recall curve – come down to the relationship between the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) at a particular decision threshold&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; or across all thresholds&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Initial model evaluation and selection should generally start by reviewing metrics that capture general performance &lt;em&gt;across all&lt;/em&gt; thresholds. Max Kuhn and Kjell Johnson explain this in their book &lt;a href=&#34;http://www.feat.engineering/measuring-performance.html&#34;&gt;Feature Engineering and Selection…&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“During the initial phase of model building, a good strategy for data sets with two classes is to focus on the AUC statistics from these curves instead of metrics based on hard class predictions. Once a reasonable model is found, the ROC or precision-recall curves can be carefully examined to find a reasonable cutoff for the data and then qualitative prediction metrics can be used.” - 3.2.2 &lt;em&gt;Classification Metrics&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once a performant model has been selected, the analyst can then identify an ideal decision threshold/cut-point/cutoff for making class predictions. In this post I will largely be skipping this initial phase in model building. Instead I will focus on methods for identifying optimal cutoffs. In particular, I will use weights on predictions and outcomes (on a hold-out-dataset) to determine which decision thresholds would maximize expected value for a selected model&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Every Cut-Point Has an Associated Confusion Matrix&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The frequency of classification outcomes (TP, TN, FP, FN) at a specific decision threshold are often represented by a confusion matrix.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png&#34; alt=&#34;Confusion Matrix for Classification&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;a href=&#34;https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&#34;&gt;Confusion Matrix for Classification&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each cell of a confusion matrix may represent a more or less valuable outcome depending on the particular problem. In the case of giving out loans, a &lt;em&gt;false positive&lt;/em&gt; may be more costly than a &lt;em&gt;false negative&lt;/em&gt;&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; while in problems concerning security threats the reverse may be true&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;. When you have a sense of the value associated with the classification outcomes, you can use this information to weight the confusion matrices, calculate the expected value of each, identify which maximizes expected value and select the associated decision threshold for use when deploying your model.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that weighting applied in the evaluation stage (as discussed throughout this post) often relies on the predicted probabilities of your model being accurate (and not just ranked correctly)&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lending-data-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lending Data Example&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lendingclub.com/auth/login?login_url=%2Fstatistics%2Fadditional-statistics%3F&#34;&gt;Lending Club&lt;/a&gt; data is used for examples throughout this post. I will be predicting the binary target &lt;code&gt;Class&lt;/code&gt;, which defines whether loans are in good or bad standing (i.e. whether the recipient is or is not in default on their loan).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Load packages, set theme, load data:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(parsnip)
library(probably)
library(rsample)
library(modeldata)
library(yardstick)
library(tidyverse)

theme_set(theme_minimal())

data(&amp;quot;lending_club&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;starter-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starter Code&lt;/h2&gt;
&lt;p&gt;This section provides a brief example of building a model and calculating a confusion matrix at a particular decision threshold. Most of the code in this section is copied from a &lt;a href=&#34;https://probably.tidymodels.org/articles/where-to-use.html&#34;&gt;vignette&lt;/a&gt; for the &lt;a href=&#34;https://probably.tidymodels.org/index.html&#34;&gt;probably&lt;/a&gt;&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; package and will serve as starter code for my examples. I provide brief descriptions of the code chunks but recommend reading the source for explanations on the steps.&lt;/p&gt;
&lt;p&gt;(If you are familiar with &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt; and modeling in classification problems you might skip to &lt;a href=&#34;#weighting-by-classification-outcomes&#34;&gt;Weighting by Classification Outcomes&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Minor transformations and select relevant columns:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lending_club &amp;lt;- lending_club %&amp;gt;%
  mutate(Class = relevel(Class, &amp;quot;good&amp;quot;)) %&amp;gt;% 
  select(Class, annual_inc, verification_status, sub_grade, funded_amnt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Training / Testing split:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
split &amp;lt;- rsample::initial_split(lending_club, prop = 0.75)

lending_train &amp;lt;- rsample::training(split)
lending_test  &amp;lt;- rsample::testing(split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Specify and build model:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logi_reg &amp;lt;- logistic_reg()
logi_reg_glm &amp;lt;- logi_reg %&amp;gt;% 
  set_engine(&amp;quot;glm&amp;quot;)

logi_reg_fit &amp;lt;- fit(
  logi_reg_glm,
  formula = Class ~ annual_inc + verification_status + sub_grade,
  data = lending_train
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Add predictions to test set:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- logi_reg_fit %&amp;gt;%
  predict(new_data = lending_test, type = &amp;quot;prob&amp;quot;)

lending_test_pred &amp;lt;- bind_cols(predictions, lending_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Use &lt;code&gt;probably::make_two_class_pred()&lt;/code&gt; to make hard predictions at a threshold of 0.50:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hard_pred_0.5 &amp;lt;- lending_test_pred %&amp;gt;%
  mutate(.pred = probably::make_two_class_pred(.pred_good,
                                               levels(Class),
                                               threshold = 0.5) %&amp;gt;% 
           as.factor(c(&amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;))
         ) %&amp;gt;%
  select(Class, contains(&amp;quot;.pred&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After we’ve made class predictions we can make a confusion matrix&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. &lt;em&gt;Use &lt;code&gt;yardstick::conf_mat()&lt;/code&gt; to get a confusion matrix for the class predictions:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat_0.5 &amp;lt;- yardstick::conf_mat(hard_pred_0.5, Class, .pred)

conf_mat_0.5 %&amp;gt;% autoplot(type = &amp;quot;heatmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could also call &lt;code&gt;summary(conf_mat_0.5)&lt;/code&gt; to calculate common metrics at this decision threshold.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom function:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I will load in a function &lt;code&gt;conf_mat_weighted()&lt;/code&gt; that works similarly to &lt;code&gt;yardsitck::conf_mat()&lt;/code&gt; but can handle observation weights (which will come into play in &lt;a href=&#34;#weighting-by-observations&#34;&gt;Weighting by Observations&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# source conf_mat_weighted() which is similar to yardstick::conf_mat() but also
# has the possibility of handling a weights column
devtools::source_gist(&amp;quot;https://gist.github.com/brshallo/37d524b82541c2f8540eab39f991830a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting-by-classification-outcomes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighting by Classification Outcomes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;For the purposes of this post I will bypass common steps in model evaluation and selection and go straight to using weighting techniques to identify appropriate decision thresholds for a selected model&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For our loan problem we will use the following weighting scheme for our potential classification outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP: 0.14 (predict a loan is good when it is indeed good)&lt;/li&gt;
&lt;li&gt;FP: 3.10 (predict a loan is good when it is actually bad)&lt;/li&gt;
&lt;li&gt;TN: 0.02 (predict a loan is bad when it is indeed bad)&lt;/li&gt;
&lt;li&gt;FN: 0.06 (predict a loan is bad when it is actually good)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Weights = \left(\begin{array}{cc} 0.14 &amp;amp; 3.10\\0.06  &amp;amp; 0.02 \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There may be asymmetries in business processes associated with the ‘actual’ and the ‘predicted’ states that might explain differences in the value associated with each cell in the confusion matrix&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I put the classification outcome weights into a matrix&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outcome_weights &amp;lt;- matrix(
          c(0.14, 3.1,
          0.06, 0.02),
          nrow = 2,
          byrow = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I apply these weights to the cells of the confusion matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight_cells &amp;lt;- function(confusion_matrix, weights = matrix(rep(1, 4), nrow = 2)){
  
  confusion_matrix_output &amp;lt;- confusion_matrix
  confusion_matrix_output$table &amp;lt;- confusion_matrix$table * weights
  confusion_matrix_output
}

conf_mat_0.5_weighted &amp;lt;- weight_cells(conf_mat_0.5, outcome_weights)

conf_mat_0.5_weighted %&amp;gt;% autoplot(type = &amp;quot;heatmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how different these values are from those calculated without weighting (shown in &lt;a href=&#34;#starter-code&#34;&gt;Starter Code&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;An earlier version of this post included a variety of other performance metrics calculated after weighting by classification outcomes. See &lt;a href=&#34;#weighted-classification-metrics&#34;&gt;Weighted Classification Metrics&lt;/a&gt; in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; for a discussion on why these were removed from the body of the post.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Assuming diagonal (correctly predicted) elements represent a gain&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt; and off-diagonal elements a loss&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;, I will calculate the total value at the 0.50 decision threshold (by taking the difference of the aggregated gains and losses)&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( (conf_mat_0.5$table * outcome_weights) * matrix(c(1,-1,-1, 1), byrow = TRUE, ncol = 2) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -53.96&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At a prediction threshold for accepting loans of 0.50, our expected value would be negative.&lt;/p&gt;
&lt;div id=&#34;metrics-across-decision-thresholds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Metrics Across Decision Thresholds&lt;/h3&gt;
&lt;p&gt;We do not want to see performance at just the individual cut-point of 0.50 but across the range of possible decision thresholds&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wrote a function &lt;code&gt;conf_mat_threshold()&lt;/code&gt; that first creates hard predictions based on a threshold and then creates the associated confusion matrix at that threshold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; Confusion Matrix at Threshold
#&amp;#39;
#&amp;#39; @param df dataframe containing a column `.pred_good`.
#&amp;#39; @param threshold A value between 0 and 1.
#&amp;#39; @param wt A column that gets passed into `conf_mat_weighted()` if also wanting to give observation weights (default is NULL).
#&amp;#39;
#&amp;#39; @return a confusion matrix
conf_mat_threshold &amp;lt;- function(df = lending_test_pred, 
                               threshold = 0.5, 
                               ...){
  hard_pred &amp;lt;- df %&amp;gt;%
    mutate(.pred = probably::make_two_class_pred(.pred_good,
                                                 levels(Class),
                                                 threshold = threshold) %&amp;gt;%
             as.factor(c(&amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;))
           )
  
  conf_mat_weighted(hard_pred, Class, .pred, ...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will map the &lt;code&gt;conf_mat_threshold()&lt;/code&gt;&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt; function across all meaningful cut-points in the dataset (creating confusion matrices at each). Next I will use &lt;code&gt;weight_cells()&lt;/code&gt; to weight the resulting confusion matrices by the outcome weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get all unique predictions that differ by at least 0.001
thresholds_unique_df &amp;lt;- tibble(threshold = c(0, unique(lending_test_pred$.pred_good), 1)) %&amp;gt;% 
  arrange(threshold) %&amp;gt;%
  mutate(diff_prev = abs(lag(threshold) - threshold),
         diff_prev_small = ifelse((diff_prev) &amp;lt;= 0.001, TRUE,FALSE),
         diff_prev_small = ifelse(is.na(diff_prev_small), FALSE, diff_prev_small)) %&amp;gt;%
  filter(!diff_prev_small) %&amp;gt;% 
  select(threshold)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute confusion matrices and weighted confusion matrices
conf_mats_df &amp;lt;- thresholds_unique_df %&amp;gt;% 
  mutate(conf_mat = map(threshold, conf_mat_threshold, df = lending_test_pred)) %&amp;gt;% 
  mutate(conf_mat_weighted = map(conf_mat, weight_cells, weights = outcome_weights))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then use &lt;code&gt;total_value()&lt;/code&gt; to calculate the expected value at each &lt;code&gt;threshold&lt;/code&gt; and plot these across cut-points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_value &amp;lt;- function(weighted_conf_matrix){
  sum(weighted_conf_matrix * matrix(c(1, -1, -1, 1), byrow = TRUE, ncol = 2))
}

conf_mats_value &amp;lt;- conf_mats_df %&amp;gt;% 
  arrange(desc(threshold)) %&amp;gt;% 
  mutate(value = map_dbl(conf_mat_weighted, ~total_value(.x$table)))

conf_mats_value %&amp;gt;% 
  ggplot(aes(x = threshold, y = value))+
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/value-chart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This suggests the greatest value occurs when using a threshold between 0.93 and 0.96&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt;. Value suffers a steep drop-off at either end of this range (quickly turning negative). For comparison, an unweighted metric would have suggested predicting “Bad” loans for everything&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below is the confusion matrix and the weighted confusion matrix at the ideal cut-point of ~0.94 (the former corresponds with raw observation counts and the latter with the &lt;em&gt;value&lt;/em&gt; of those counts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cfm1 &amp;lt;- conf_mats_value %&amp;gt;% 
  arrange(desc(value)) %&amp;gt;% 
  pluck(&amp;quot;conf_mat&amp;quot;, 1) %&amp;gt;% 
  autoplot(type = &amp;quot;heatmap&amp;quot;)+
  coord_fixed()+
  labs(title = &amp;quot;Confusion Matrix&amp;quot;,
       subtitle = &amp;quot;Threshold: 0.94&amp;quot;)

cfm2 &amp;lt;- conf_mats_value %&amp;gt;% 
  arrange(desc(value)) %&amp;gt;% 
  pluck(&amp;quot;conf_mat_weighted&amp;quot;, 1) %&amp;gt;% 
  autoplot(type = &amp;quot;heatmap&amp;quot;)+
  coord_fixed()+
  labs(title = &amp;quot;Value Weighted Confusion Matrix&amp;quot;,
       subtitle = &amp;quot;Threshold: 0.94&amp;quot;)

library(patchwork)

cfm1 + cfm2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/confusion-matrix-unweighted-weighted-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting-by-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighting by Observations&lt;/h2&gt;
&lt;p&gt;In the prior section, we used weights to represent the value of classification outcomes. What if we want to account for the value of a specific loan (since higher loan amounts might amplify possible risks)? In this section I will apply the same steps as above but will first weight individual observations&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt; by the &lt;code&gt;funded_amnt&lt;/code&gt; column&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My function &lt;code&gt;conf_mat_weighted()&lt;/code&gt; (sourced from &lt;a href=&#34;https://gist.github.com/brshallo/37d524b82541c2f8540eab39f991830a&#34;&gt;gist&lt;/a&gt; previously) can handle a value for observation weights. Hence, we will follow the same steps as in &lt;a href=&#34;#weighting-by-classification-outcomes&#34;&gt;Weighting by Classification Outcomes&lt;/a&gt; except also supplying &lt;code&gt;wt = funded_amnt&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get confusion matrices and weighted confusion matrices
conf_mats_df_obs_weights &amp;lt;- thresholds_unique_df %&amp;gt;% 
  mutate(conf_mat = map(threshold, 
                        conf_mat_threshold, 
                        df = lending_test_pred,
                        # funded_amnt provides observation weights
                        wt = funded_amnt)) %&amp;gt;%
  mutate(conf_mat_weighted = map(conf_mat, weight_cells, weights = outcome_weights)) %&amp;gt;%
  mutate(across(contains(&amp;quot;conf&amp;quot;), list(metrics = ~map(.x, summary))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The range of cut-points that maximizes ‘value’ appears similar to that shown in the previous section (when not weighting observations by &lt;code&gt;funded_amnt&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mats_df_obs_weights %&amp;gt;% 
  arrange(desc(threshold)) %&amp;gt;% 
  mutate(value = map_dbl(conf_mat_weighted, ~total_value(.x$table))) %&amp;gt;% 
  discard(is.list) %&amp;gt;% 
  ggplot(aes(x = threshold, y = value))+
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Closing note&lt;/h1&gt;
&lt;p&gt;Once you’ve developed a well-calibrated model, weighting by classification outcomes and observation weights can be helpful for identifying an optimal decision threshold that will maximize expected value when making class predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;#weights-of-observations-during-and-prior-to-modeling&#34;&gt;Weights of Observations During and Prior to Modeling&lt;/a&gt; and (maybe) &lt;a href=&#34;#notes-on-cost-sensitive-classification&#34;&gt;Notes on Cost Sensitive Classification&lt;/a&gt; are the only sections people other than the author are likely to find interesting.&lt;/p&gt;
&lt;p&gt;Other sections are primarily bookmarks on thought-processes related to earlier versions of this post. I was unsure about the appropriateness of calculating weighted classification metrics as well as which weighting scheme I wanted to use. See question on &lt;a href=&#34;https://stats.stackexchange.com/questions/499841/weighting-confusion-matrix&#34;&gt;Cross Validated&lt;/a&gt; and sections &lt;a href=&#34;#questions-on-cost-sensitive-classification&#34;&gt;Questions on Cost Sensitive Classification&lt;/a&gt;, &lt;a href=&#34;#weighted-classification-metrics&#34;&gt;Weighted Classification Metrics&lt;/a&gt;, and &lt;a href=&#34;#arriving-at-weights&#34;&gt;Arriving at Weights&lt;/a&gt; for questions I wrestled with while writing this post.&lt;/p&gt;
&lt;div id=&#34;weights-of-observations-during-and-prior-to-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weights of Observations During and Prior to Modeling&lt;/h2&gt;
&lt;p&gt;The body of the blog post is focused exclusively on weighting in model evaluation &lt;em&gt;after model training&lt;/em&gt;. This section provides a brief overview of other types of weighting that can be used in modeling (as well as references for these topics).&lt;/p&gt;
&lt;p&gt;Robert Moser has a helpful blog post, &lt;a href=&#34;https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9&#34;&gt;Fraud detection with cost sensitive machine learning&lt;/a&gt;, where he differentiates cost dependent classification (weighting after model building) from cost sensitive training (the practice of baking in the costs of classification outcomes&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; into the objective function used during model training&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt;). Some modeling algorithms may inherently weight observations differently. For example, in the regression context, Weighted Least Squares will adjust observations weights to reduce the impact of highly influential observations.&lt;/p&gt;
&lt;p&gt;In other cases, weights are applied not in the model building step but immediately prior when setting the likelihood of an observation to be selected for training the model. For example, undersampling, oversampling or other sampling techniques are typically used in attempts to improve the performance of a model in predicting a minority class&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; but can also be used as a means of changing the base rate associated with each class&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;. Some algorithms use biased resampling techniques directly in their model building procedures&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;. Similar in effect to over and under sampling the data, models may also handle weights according to the outcome class of the observation&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes-on-cost-sensitive-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes on Cost Sensitive Classification&lt;/h2&gt;
&lt;p&gt;Depending on your problem you might also use different approaches for weighting confusion matrices after model building. To account for differences in the value of classification outcomes you might weight items in the confusion matrix more heavily depending on what their actual condition is (e.g. TRUE outcomes are weighted higher than FALSE outcomes). You might also use a cost based approach to evaluate performance, whereby the diagonal elements on the confusion matrix (the correctly predicted items) have a cost of zero and all other cells are weighted according to their associated costs (see &lt;a href=&#34;#questions-on-cost-sensitive-classification&#34;&gt;Questions on Cost Sensitive Classification&lt;/a&gt; for further discussion on these).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Some notes on implementations:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://mlr-org.com/&#34;&gt;mlr&lt;/a&gt; package provides a helpful vignette (&lt;a href=&#34;https://mlr.mlr-org.com/articles/tutorial/cost_sensitive_classif.html&#34;&gt;Cost-Sensitive Classification&lt;/a&gt;) that covers a variety of these and other approaches&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In python the &lt;a href=&#34;http://albahnsen.github.io/CostSensitiveClassification/index.html&#34;&gt;costcla&lt;/a&gt; module provides documentation on how to approach many of these types of problems in python.&lt;/li&gt;
&lt;li&gt;Unfortunately, the &lt;code&gt;tidymodels&lt;/code&gt; suite of packages does not yet have broad support for weights. There are some relevant open issues, e.g. &lt;a href=&#34;https://github.com/tidymodels/yardstick/issues/3&#34;&gt;#3&lt;/a&gt; in &lt;code&gt;yardstick&lt;/code&gt; describes approaches for cost sensitive metrics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-classification-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighted Classification Metrics&lt;/h2&gt;
&lt;p&gt;I posted a question to Cross Validated (CV) on &lt;a href=&#34;https://stats.stackexchange.com/questions/499841/weighting-common-performance-metrics-by-classification-outcomes?noredirect=1#comment925035_499841&#34;&gt;Weighting common performance metrics by classification outcomes?&lt;/a&gt;. I also reached out to other analytics professionals relating my questions about these metrics. I ultimately came to the conclusion that many performance metrics weighted by classification outcomes are inappropriate (other than expected value) or not useful so kept the focus of this post on using weighted classification outcomes strictly for maximizing expected value and identifying ideal cut-points on an already selected model.&lt;/p&gt;
&lt;p&gt;I also changed my weighting scheme though not such that any cells in the confusion matrix would be zeroed out&lt;a href=&#34;#fn30&#34; class=&#34;footnote-ref&#34; id=&#34;fnref30&#34;&gt;&lt;sup&gt;30&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Weighted performance metrics on a confusion matrix at threshold of 0.5:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(conf_mat_0.5_weighted) %&amp;gt;% 
  knitr::kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.estimator&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;.estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;accuracy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;kap&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sens&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;spec&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ppv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;npv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mcc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;j_index&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bal_accuracy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;detection_prevalence&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;precision&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;recall&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;f_meas&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;binary&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.632&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Weighted performance metrics across decision thresholds:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mats_df %&amp;gt;%
  mutate(across(contains(&amp;quot;conf&amp;quot;), list(metrics = ~map(.x, summary)))) %&amp;gt;% 
  unnest(conf_mat_weighted_metrics) %&amp;gt;%
  select(-contains(&amp;quot;conf&amp;quot;), -.estimator) %&amp;gt;% 
  filter(.metric != &amp;quot;sens&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = threshold, y = .estimate))+
  geom_line()+
  facet_wrap(~.metric, scales = &amp;quot;free_y&amp;quot;)+
  labs(title = &amp;quot;Weighted Performance Metrics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-08-weighting-classification-outcomes_files/figure-html/classification-metrics-charts-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interpretation of these metrics is unclear (as described on the CV issue mentioned above).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;questions-on-cost-sensitive-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Questions on Cost Sensitive Classification&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;I was unsure on which type of weighting scheme I wanted to use for this post. This section walks through some of the questions I asked myself related to this.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Resources on cost based approaches to model evaluation of classification problems seemed to take a variety of different approaches. This left me with a few questions about what is appropriate and what is inappropriate.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Should the diagonal elements be zero?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Most of the examples I saw on cost-sensitive classification had their correctly predicted items (the diagonal of the confusion matrix) set to zero (assuming no or small costs associated with correctly predicted outcomes)&lt;a href=&#34;#fn31&#34; class=&#34;footnote-ref&#34; id=&#34;fnref31&#34;&gt;&lt;sup&gt;31&lt;/sup&gt;&lt;/a&gt;. There are some exceptions to this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/sowmya_vivek&#34;&gt;Sowmya Vivek&lt;/a&gt; provides an example of this in her article &lt;a href=&#34;https://towardsdatascience.com/model-performance-cost-functions-for-classification-models-a7b1b00ba60&#34;&gt;Model performance &amp;amp; cost functions for classification models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Robert Moser’s post on &lt;a href=&#34;https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9&#34;&gt;Fraud detection with cost sensitive machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These gave me some confidence that there isn’t a problem with having non-zero items on the diagonals.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Should off-diagonal elements be negated?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I did notice that Moser only used positive weighting whereas Vivek’s post had the weights sign negated depending on if it was on the diagonal or off-diagonal. I prefer Moser’s approach as I figure the signs could be negated when evaluating ‘value’ (which is the approach that I took&lt;a href=&#34;#fn32&#34; class=&#34;footnote-ref&#34; id=&#34;fnref32&#34;&gt;&lt;sup&gt;32&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Do weighted classification metrics make sense?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When weighting a confusion matrix, does it also make sense to calculate other weighted metrics, e.g. weighted precision, recall, etc.? Does it make sense to look at these across decision thresholds&lt;a href=&#34;#fn33&#34; class=&#34;footnote-ref&#34; id=&#34;fnref33&#34;&gt;&lt;sup&gt;33&lt;/sup&gt;&lt;/a&gt;? The problem with the weighted versions of these metrics is that the possible number of observations could essentially change between decision cut-points because of observations slipping between higher or lower weighted classification outcomes. Does this present a mathematical challenge of some sort? Clearly if some cells are weighted to 0, some performance metrics may not make sense to calculate&lt;a href=&#34;#fn34&#34; class=&#34;footnote-ref&#34; id=&#34;fnref34&#34;&gt;&lt;sup&gt;34&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I ended-up posting this question on cross validated: &lt;a href=&#34;https://stats.stackexchange.com/questions/499841/weighting-common-performance-metrics-by-classification-outcomes?noredirect=1#comment925035_499841&#34;&gt;Weighting common performance metrics by classification outcomes?&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arriving-at-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Arriving at Weights&lt;/h2&gt;
&lt;p&gt;I initially used a different set of weights:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Weights = \left(\begin{array}{cc} 0.14 &amp;amp; 1.89\\1.86  &amp;amp; 0.11 \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I think this approach was an example of thinking too hard and that the approach here is incorrect or at least unnecessary…&lt;/em&gt; I also ended-up using a different set of weights (that were just made-up).&lt;/p&gt;
&lt;p&gt;Say there are three things that determine how value should be weighted across a confusion matrix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How value is spread across the &lt;em&gt;actual&lt;/em&gt; dimension&lt;/li&gt;
&lt;li&gt;How value is spread across the &lt;em&gt;predicted&lt;/em&gt; dimension&lt;/li&gt;
&lt;li&gt;How value is shared between the &lt;em&gt;actual&lt;/em&gt; and &lt;em&gt;predicted&lt;/em&gt; dimensions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Say for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;value associated with an actual outcome of FALSE is 20 times as much as an actual outcome of TRUE (e.g. in the former case you may lose the entire loan amount, whereas in the latter case you only gain the interest payments). Converted to be in terms of proportion allocated to each item, we’d get &lt;span class=&#34;math inline&#34;&gt;\(A_t = \frac{1}{21}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A_f = \frac{20}{21}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;value associated with a prediction of TRUE is two times as much as a prediction of FALSE (e.g. higher administrative costs in the former case that go with administering the loan). Or &lt;span class=&#34;math inline&#34;&gt;\(P_t = \frac{2}{3}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_f = \frac{1}{3}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Say the value associated with the &lt;em&gt;actual&lt;/em&gt; outcome is thirty times that of the value associated with the &lt;em&gt;prediction&lt;/em&gt;. &lt;span class=&#34;math inline&#34;&gt;\(V_a = \frac{30}{31}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_p = \frac{1}{31}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Pretend also that we want all weights to sum to be equal to the number of cells in our confusion matrix.)&lt;/p&gt;
&lt;p&gt;Assuming there is a multiplicative relationship between these outcomes. Calculating the value of the confusion matrix may become a matter of plugging in the associated values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Weights = 2\left(\begin{array}{cc} (A_tV_a+P_tV_p) &amp;amp; (A_fV_a+P_tV_p)\\(A_tV_a+P_fV_p)  &amp;amp; (A_fV_a+P_fV_p) \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This approach could be generalized for cases with more than two categories.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I am including &lt;em&gt;decision point threshold selection&lt;/em&gt; as falling into the bucket of approaches under the umbrella of ‘After model training during evaluation.’ You could argue it deserves a separate sub-step ‘Figuring out how to use your selected model,’ that could be distinguished from model evaluation intended for ‘Selecting your model.’ The approaches I describe could be used in either/both of these hypothetical sub-sections, I will focus in this post on threshold selection.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;While the AUC score can be thought of in terms of area under the curve when x-axis: 1 - specificity and y-axis: sensitivity at all possible decision thresholds, I actually prefer conceptualizing AUC as being more about whether your observations are properly ordered and therefore prefer this way of thinking about the metric:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Intuitive way to think of AUC:&lt;br&gt;&lt;br&gt;-Imagine taking a random point from both the distribution of TRUE events and of FALSE events&lt;br&gt;-Compare the predictions for the 2 points&lt;br&gt;&lt;br&gt;AUC is the probability your model gave the TRUE event a higher score than it gave the FALSE one.&lt;br&gt;&lt;br&gt;Viz &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; : &lt;a href=&#34;https://t.co/hmF3cjZo6I&#34;&gt;pic.twitter.com/hmF3cjZo6I&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1293979319308558336?ref_src=twsrc%5Etfw&#34;&gt;August 13, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I.e. if probability of event is greater than 0.50, predict TRUE, else FALSE.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Some metrics are often helpful for focusing on particular aspects of your predictions (e.g. precision, recall, F-score all focus more on the model’s capacity to identify TRUE events).&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Other approaches could be to investigate the ROC or precision-recall curves or to maximize the J-index.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;(A default on a loan may cost more than profits lost from interest payments).&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;A terrorist attack is more costly than the an unnecessary deployment of resources.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;I’ve &lt;a href=&#34;https://www.bryanshalloway.com/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/&#34;&gt;written previously&lt;/a&gt; on this topic (though somewhat tangentially).&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://probably.tidymodels.org/index.html&#34;&gt;probably&lt;/a&gt; is a new package in the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt; suite in R that is helpful for evaluation steps that occur after model building. &lt;a href=&#34;https://twitter.com/dvaughan32?lang=en&#34;&gt;Davis Vaughan&lt;/a&gt; describes its purpose:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Regarding placement in the modeling workflow, &lt;code&gt;probably&lt;/code&gt; best fits in as a post processing step after the model has been fit, but before the model performance has been calculated” -&lt;a href=&#34;https://probably.tidymodels.org/articles/where-to-use.html&#34;&gt;Where does probably fit in?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As &lt;a href=&#34;https://probably.tidymodels.org/index.html&#34;&gt;probably&lt;/a&gt; and &lt;a href=&#34;https://yardstick.tidymodels.org/&#34;&gt;yardstick&lt;/a&gt; continue to develop I imagine that functionality from those packages will replace much of the code I write in this post.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;From this point on, code is no longer being copied directly from the &lt;code&gt;probably&lt;/code&gt; vignette. Again, I would recommend exploring the &lt;code&gt;probably&lt;/code&gt; package which goes on to discuss performance metrics on classification problems more generally.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;The remainder of the post is about weighting classification outcomes or observations during model evaluation, and primarily uses methods that review performance across all decision thresholds (not just at 0.5).&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;For the purposes of this post I am unconcerned how realistic these weights are.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;I am choosing the element positions based on the defaults in the output from a “conf_mat” object that gets created by &lt;code&gt;yardstick::conf_mat()&lt;/code&gt;.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;For example in the case of True Positives, corresponding to interest payments.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;For example in the case of a False Negative this represents the loan being lost due to default of the recipient.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;I could convert this to the &lt;em&gt;average value&lt;/em&gt; by dividing by the number of observations.&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;The function &lt;code&gt;probably::threshold_perf()&lt;/code&gt; is designed for this type of task. However this function only supports a few types of metrics currently. &lt;a href=&#34;https://github.com/tidymodels/probably/issues/25&#34;&gt;#25&lt;/a&gt; suggests that in the future these may be more customizable. Neither &lt;code&gt;probably&lt;/code&gt; nor &lt;code&gt;yardstick&lt;/code&gt; can yet handle observation or classification outcome weights. Hence why I don’t just use functions from those packages directly. &lt;a href=&#34;https://github.com/tidymodels/yardstick/issues/30&#34;&gt;#30&lt;/a&gt; suggests that &lt;code&gt;yardstick&lt;/code&gt; will get support for weights in the future however this issue is referring to observation weights, not classification outcome weights. &lt;a href=&#34;https://github.com/tidymodels/yardstick/issues/3&#34;&gt;#3&lt;/a&gt; however suggests &lt;code&gt;yardstick&lt;/code&gt; will also get options for handling different weights in classification outcomes.&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;I made this function such that it can also take in observation weights which will be used in &lt;a href=&#34;#weighting-by-observations&#34;&gt;Weighting by Observations&lt;/a&gt;.&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;AUC and precision-recall curves may also be worth looking at.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;I.e. not having a lending business.&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;It should be noted though that if you are using observation weights there is a decent chance you will want to apply them &lt;em&gt;during&lt;/em&gt; the model building process (rather than after).&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;This is likely a case where the costs and gains would likely be case dependent (i.e. depending on how big of a loan the client is asking for). There is a tutorial on &lt;a href=&#34;https://mlr.mlr-org.com/articles/tutorial/cost_sensitive_classif.html#example-dependent-misclassification-costs&#34;&gt;Example-dependent misclassification costs&lt;/a&gt; from the &lt;code&gt;mlr&lt;/code&gt; package that provides descriptions of this more complicated case. One may also be interested in weighting the observations in the steps prior to evaluation but in those steps described in &lt;a href=&#34;#weights-of-observations-during-and-prior-to-modeling&#34;&gt;Weights of Observations During and Prior to Modeling&lt;/a&gt;.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;It is important to note that when using these procedures, your model no longer returns predicted probabilities but measures of whether or not you &lt;em&gt;should&lt;/em&gt; predict a particular outcome given your weights and costs.&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;He includes a simple example of doing this with keras in python.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;Or set of observations that are difficult to classify for other reason&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;The implications of this are sometimes not recognized by beginners as I write about in &lt;a href=&#34;https://www.bryanshalloway.com/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/&#34;&gt;Undersampling Will Change the Base Rates of Your Model’s Predictions&lt;/a&gt;.&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;For example gradient boosting techniques bias successive resamples such that observations where the model performs poorly are more likely to be selected in subsequent rounds of model training.&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;This is described in &lt;a href=&#34;https://mlr.mlr-org.com/articles/tutorial/cost_sensitive_classif.html#i--weighting&#34;&gt;Rebalancing, i. Weighting&lt;/a&gt; in the &lt;a href=&#34;https://mlr.mlr-org.com/&#34;&gt;mlr&lt;/a&gt; vignette.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;As well as documentation for how to approach these scenarios using &lt;code&gt;mlr&lt;/code&gt;&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn30&#34;&gt;&lt;p&gt;Some classification metrics cannot be calculated if you weight the cells to zero – although I do not believe them to be appropriate I wanted to preserve the examples below as mental bookmarks.&lt;a href=&#34;#fnref30&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn31&#34;&gt;&lt;p&gt;My concern was that there may be mathematical or contextual reasons why this should generally be the case. For example that not doing so in some ways ‘double counts’ things somehow.&lt;a href=&#34;#fnref31&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn32&#34;&gt;&lt;p&gt;Allowing them as weights on the existing confusion matrix means that other metrics (other than cost) may also be calculated… many of these are in ratio form and either can’t be calculated or don’t make sense if some of the cells are negative (or zero).&lt;a href=&#34;#fnref32&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn33&#34;&gt;&lt;p&gt;e.g looking at the AUC or the precision recall curve.&lt;a href=&#34;#fnref33&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn34&#34;&gt;&lt;p&gt;E.g. accuracy is pointless to calculate if weighting the diagonals to zero. Sensitivity pointless when weighting TP to 0… etc.&lt;a href=&#34;#fnref34&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Influencing Distributions with Tiered Incentives</title>
      <link>/2020/11/02/influencing-distributions/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/11/02/influencing-distributions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-example&#34;&gt;Simple Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#applying-incentives&#34;&gt;Applying Incentives&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#takeaways-of-resulting-distribution&#34;&gt;Takeaways of Resulting Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#think-carefully-about-assumptions&#34;&gt;Think Carefully About Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-set-assumptions&#34;&gt;How to Set Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-assumptions&#34;&gt;Simple Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trade-offs&#34;&gt;Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this post I will use incentives for sales representatives in pricing to provide examples of factors to consider when attempting to influence an existing distribution.&lt;/p&gt;
&lt;p&gt;For instance, if you have a lever that pushes prices from low to high, using the lever to influence the prices adjacent to the right of the largest parts of the distribution will (likely, though contingent on a variety of factors) make the biggest impact on raising the average price attained. If the starting distribution is normal, this means incentives applied near the lower prices (the tail of the distribution) may have the smallest impact.&lt;/p&gt;
&lt;p&gt;All figures in this post are created using the R programming language (see &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-11-02-influencing-distributions.Rmd&#34;&gt;Rmarkdown document&lt;/a&gt; on github for code).&lt;/p&gt;
&lt;div id=&#34;simple-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple Example&lt;/h1&gt;
&lt;p&gt;Imagine you have a product that can be sold anywhere from $100 to $150. Sales reps want to sell for as high of a price as possible and customers want to purchase for as low of a price as possible. In this tension your product ends-up selling, on average, for $125 and follows a truncated normal distribution with standard deviation of $10&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-02-influencing-distributions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;applying-incentives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying Incentives&lt;/h2&gt;
&lt;p&gt;Executive leadership wants to apply additional incentives on sales reps to keep prices high&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. They task you with setting-up a tiered compensation scheme whereby deals at the top-end of the distribution get a higher compensation rate compared to deals at the bottom end of the distribution.&lt;/p&gt;
&lt;p&gt;Applying such an additional incentive on sales teams has the potential advantage of pushing some proportion of deals to a higher price&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. There are also &lt;a href=&#34;#trade-offs&#34;&gt;Trade-offs&lt;/a&gt; associated with such an initiative (indicated in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;), these will be ignored for the purposes of this exercise.&lt;/p&gt;
&lt;p&gt;Say you decide to set cut-points to split the distribution into quartiles such that sales reps get larger bonuses if their deals fall into higher quartiles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-02-influencing-distributions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Applying incentives is likely to lead to a different distribution for future deals.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Consider what the relevant factors and assumptions are in influencing the existing distribution. &lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Take a moment to hypothesize what the new distribution will look like after incentives are applied&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After applying incentives the resulting distribution is likely to depend on:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The starting distribution&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; of deals.&lt;/li&gt;
&lt;li&gt;What the incentives are and &lt;em&gt;how&lt;/em&gt; they influence the initial distribution.&lt;/li&gt;
&lt;li&gt;How this influence degrades the farther away the starting position of a deal is from the next tier up in incentives.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You could paramaterize this problem and model the expected distribution. Making some &lt;a href=&#34;#simple-assumptions&#34;&gt;Simple Assumptions&lt;/a&gt; (described in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;), the chart below shows a (potential) resulting distribution after applying the incentives.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-02-influencing-distributions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Gray bars in the chart below indicate where (on the original distribution) movement to a higher tier will occur.
&lt;img src=&#34;/post/2020-11-02-influencing-distributions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;takeaways-of-resulting-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Takeaways of Resulting Distribution&lt;/h3&gt;
&lt;p&gt;The greatest proportion of deals were moved from orange to yellow and from yellow to blue. Pink to orange had the least amount of movement (due to the first quartile being spread across a wider range).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;incentive&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;density_converted&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pink to orange&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;orange to yellow&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.099&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;yellow to blue&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;stayed blue&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Incentives Make the Biggest Difference When Nearer to the Largest Parts of the Distribution Susceptible to Change&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because these incentives slide deals from lower prices to higher prices, those cut-points that are &lt;em&gt;just above&lt;/em&gt; the most dense parts of the distribution have the biggest impacts on the post-incentivized distribution. For a normal distribution, such as this one, that means incentives just to the right of the first quartile have the smallest impact. (Importantly, this assumes susceptibility to rightward mobility is evenly distributed across the starting distribution.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How Many Thresholds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For many reasonable assumptions&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, having more thresholds will lead to greater movement upwards in the distribution. Similarly, a continuous application of incentives (i.e. sales reps get higher compensation for every point they move up on the distribution) can be optimal under certain assumptions as well&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quartiles change&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After applying the incentives, the cut-points for segmenting the distribution into quartiles on future deals will be different. Given your assumptions, you could try forecasting where the new quartiles will exist (after applying the incentives) and adjust the bonus thresholds proactively.&lt;/p&gt;
&lt;p&gt;Thresholds for incentives could also be adjusted dynamically. For example based on a rolling average of the quartiles of recent deals. In this approach, you apply initial incentives and then allow them to change dynamically depending on the resulting distribution of deals – setting guard rails where appropriate. An advantage to this dynamic approach is that the compensation rates gets set based on behavior&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; – which is helpful in cases where you may not trust your ability to set appropriate thresholds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;think-carefully-about-assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Think Carefully About Assumptions&lt;/h3&gt;
&lt;p&gt;Simulating the expected outcome based on assumptions such as the ones described in this post are helpful in thoughtfully elucidating the problem for yourself or for others. Assumptions do not need to be &lt;em&gt;perfect&lt;/em&gt; to be useful for thinking through the problem but they should lean towards the &lt;em&gt;actual&lt;/em&gt; patterns in your example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do Incentives Aggregate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this case, we are assuming incentives aggregate in a linear way. This means that five 1 ppt incentives have the same amount of influence as one 5 ppt incentive. It could be that the former is more influential (people prefer many small bonuses) or the latter is more influential (people prefer one large bonuses)&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It could also be that there is a ‘correct’ size of incentive and that too small an incentive makes no difference but a large incentive has diminishing returns. If this is the case a logistic function or other ‘S’ shaped function may be more reasonable for modeling the influence of incentives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does Influence Degrade With Distance From Incentives?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this case, we are assuming the influence of an incentive exponential decays (the influence decreases by 25% for every point we move from the cut-point). Hence being only a few points away from a cut-point has a big impact, but the degradation is less with each point we move away.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How is Slack Distributed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I assumed slack (i.e. the possibility of deals being influenced by incentives) was equally distributed. (It could be that slack is distributed disproportionally towards the lower ends of the distribution for example.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-set-assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to Set Assumptions&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with what makes sense (e.g. normal distributions are often good starting places)&lt;/li&gt;
&lt;li&gt;Review historical data&lt;/li&gt;
&lt;li&gt;Set-up formal tests (e.g. create hypotheses and see how behavior adjusts as you change incentives on random subsets of your sales representatives)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;simple-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple Assumptions&lt;/h2&gt;
&lt;p&gt;For this example, we will say the incentives you established are higher compensation rates depending on which quartile the deal falls in. If the deal falls in the lowest quartile they get no increase, in the 2nd quartile they get a 5 percentage point (ppt) increase in pay, the 3rd a 10 ppt increase, the 4th a 15 ppt increase&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For now I’ll pick some overly simple but sensible values for each question:&lt;/em&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;As indicated, we are assuming the ‘natural’ distribution of prices is roughly normal&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will assume that for every 1 ppt change in incentive that 15% of the deals immediately to the right of the cut-off will be moved up to the cut-off value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will assume that this influence degrades by 25% for every dollar you move from the cut-point&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. I am ignoring the possibility of deals jumping more than on level (e.g. deals moving from the 1st quartile to the 3rd quartile)&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trade-offs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A sales rep may have been able to sell more product at a lower price.
&lt;ul&gt;
&lt;li&gt;The additional incentive causes some deals (those selling for lower prices) to be passed on because the incentive to close on the deal for reps has been lowered (this may be intentional in that the impact on price erosion of these deals is worth the decrease in sales…).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;You may have to pay your sales reps more&lt;/li&gt;
&lt;li&gt;Applying such incentives may create additional bureaucratic hurdles in closing deals that increase the friction of closing deals, causing some percentage of deals to be lost
&lt;ul&gt;
&lt;li&gt;It could be that deals don’t have slack in them and are already optimal…&lt;/li&gt;
&lt;li&gt;Any change in pricing behavior has the risk of upsetting customers or having downstream affects.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Ideally&lt;/em&gt; the organization is able to take into account risks and advantages in pricing and set-up incentives that are focused on overall profitability and firm growth (not &lt;em&gt;just&lt;/em&gt; in terms of a single factor).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Limits for Y axis appear too large for this chart but are set from 0 to 0.15 so as to be consistent with similar figures later in post.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;E.g. to prevent brand erosion, improve margins, etc.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;This assumes that there is some slack in the existing deals and that representatives are in a position to impact this and will do so if provided higher incentives.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;or potentially a ‘natural’ distribution that would exist in the absence of incentives&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Key to this assumption is how incentives degrade as you move farther from a cut-point.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;These do not consider potential psychological impacts or difficulty of implementation.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Similar to in a market.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Research into psychological biases suggests the former may be true.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;You could also construct this such that lower quartiles have negative incentives and higher quartiles have positive incentives.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;The functions governing these behaviors are almost certainly more sophisticated.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Factoring this possibility in would likely lead to incentives at the higher quartiles making a slightly larger impact.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should You Use an Assignment as Part of Your Hiring Process for a Data Scientist?</title>
      <link>/2020/10/27/should-you-use-an-assignment-or-exercise-as-part-of-your-hiring-process-for-a-data-scientist/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/27/should-you-use-an-assignment-or-exercise-as-part-of-your-hiring-process-for-a-data-scientist/</guid>
      <description>


&lt;p&gt;A version of this question was asked on my alumni Slack channel. There were some excellent points brought up by those answering the question in the negative, including that…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the practice is exploitative (or at least inconsiderate) of the interviewee’s time.&lt;/li&gt;
&lt;li&gt;it is not useful for the interviewing team (as the questions / scenarios are often so contrived as to be useless towards evaluating candidates’ future job performance)&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;doing so will turn-off candidates or many of them will simply not complete the activity. Hence the damage done to your applicant pool will be greater than the value you gain from additional information on your existing applicants&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think each of these points is in some cases true. However I answered the question in the positive. I pasted my answer below (making minor edits for clarity) &lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m in favor of using an activity as part of the hiring process for data scientists (at least in part). Though I believe they should be used more for evaluating the hard skills (compared to others who preferred using them for evaluating soft skills) and more as a pass / fail exercise.&lt;/p&gt;
&lt;p&gt;I think they are best given as an intermediate step in the application process (e.g. after an initial interview or phone screen). They can serve as a useful filter of if the applicant has some baseline technical competencies / skills you view as prerequisites for the role.&lt;/p&gt;
&lt;p&gt;I agree with others regarding the importance of being straight forward regarding what candidates will be expected to do in the activity ahead of time. We send an email beforehand that spells out &lt;em&gt;explicitly&lt;/em&gt; what they will be asked to do several days before the activity (e.g. “You will be asked to use a statistical test to measure the relationship between two variables,” “You will be asked to join data between tables,” etc)&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In contrast to those who were opposed to the idea of giving a limited time to complete the activity, I actually think setting rigid time constraints is helpful for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It prevents the “who has more free time” problem from influencing results.&lt;/li&gt;
&lt;li&gt;It also prevents candidates from spending a bunch of time on the task unnecessarily.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We evaluate the task more from the lens of being primarily a pass / fail activity (so just being fast and over achieving on the the activity doesn’t get a ton of extra credit). We let the candidates pick their date / time to receive the assignment and then expect them to complete (and submit their work) within the time frame.&lt;/p&gt;
&lt;p&gt;We’ve shortened the number of questions we ask. Making it as short as possible is considerate to the applicant&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. As an example, we may give three hours with the hope that a skilled (highly focused) candidate could complete it in 1 - 1.5 hours. We do not pay candidates for their time completing the exercise. We do provide feedback on the activity (usually either as part of the interview process or immediately afterwards – including to applicants that ‘fail’ the activity).&lt;/p&gt;
&lt;p&gt;We have (often) used presentations as part of the interview process as well (which are typically extensions upon the technical activity). I’m more ambivalent on the usefulness of these in terms of how clearly they differentiate candidates. Preparing for a presentation also means asking the candidates to invest substantially more time.&lt;/p&gt;
&lt;p&gt;Hiring is a highly noisy activity&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; with lots of risks for substitution errors and other biases. You’re probably not going to know who the “ideal candidate” is (even if you convince yourself that you do). What you (hopefully) can do is set some broadly useful (and ethical) filters that give you a good pool to select from&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;. For this I think a (relatively) short, well constructed technical activity can be helpful. However, for any individual position you also have to hope you get lucky…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;Another approach brought up was the idea of doing an assignment onsite. I’m also in favor of these. They make controlling for additional factors easier. With onsite evaluations (of any kind) the balance is between on the one hand not wanting to rush the applicant (as you don’t want to measure &lt;em&gt;just&lt;/em&gt; speed or penalize them for not remembering everything on the fly) and on the other hand wanting to make for a fair evaluation space (i.e. everyone gets the same time and types of questions). I think there are good ways of doing this. I still think doing a technical assignment ahead of time can make sense (if they don’t pass this, no point in forcing both parties to spend a full day onsite unnecessarily).&lt;/p&gt;
&lt;p&gt;The idea I would push back against the most is that a technical assessment (of at least some form) is not necessary and the notion that “if you’re smart, you can learn the skills.” This is true in the specific sense, but it also begs the question, “Why didn’t you learn at least some of this already?” Unless your organization has really strong onboarding &lt;em&gt;designed&lt;/em&gt; to bring people up to speed from near zero, you probably want people coming in with some baseline or to have demonstrated their interest by learning some things already. I’ve expressed this sentiment previously:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
…(even in high-level languages like &lt;a href=&#34;https://twitter.com/hashtag/python?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#python&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;). Data science, etc require skill and sweat to be effective. This idea that you should just try to hire the “smartest” person possible for any role and hope they turn into a unicorn is a destructive myth that needs to end. …
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1197234570011299843?ref_src=twsrc%5Etfw&#34;&gt;November 20, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;twitter-survey&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Twitter Survey&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
(+ &lt;a href=&#34;https://twitter.com/hashtag/python?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#python&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/julia?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#julia&lt;/a&gt; and others)
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1321166916304670720?ref_src=twsrc%5Etfw&#34;&gt;October 27, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This applies to almost every part of an application process; it’s about where you can gather small pieces of useful information at minimal time / expense.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;However, candidates who remain (and invest their time completing the exercise) may be more likely to accept an offer (falling to some extent for the sunk cost fallacy). Though this reasoning is dubious ethically and should be avoided in decision making.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Note that while I have participated in the hiring of a few data science roles I am by no means &lt;em&gt;seasoned&lt;/em&gt; in hiring data scientists.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;RTI has some good examples of data science activities on their github: &lt;a href=&#34;https://github.com/rtidatascience/data-scientist-exercise01&#34;&gt;RTI exercise 1&lt;/a&gt;, &lt;a href=&#34;https://github.com/rtidatascience/data-scientist-exercise02&#34;&gt;RTI exercise 2&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;and, depending on the questions, may save the team time in evaluation&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;particularly in a ‘hot’ field like data science where a lot of people are transitioning into it&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;and that provides some latitude for constructing a diverse team that fits well together&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression in Pricing Analysis, Essential Things to Know</title>
      <link>/2020/08/17/pricing-insights-from-historical-data-part-1/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/17/pricing-insights-from-historical-data-part-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-influences-price&#34;&gt;What influences price?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-regression-model&#34;&gt;Simple linear regression model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference-and-challenges&#34;&gt;Inference and challenges&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#violation-of-model-assumptions&#34;&gt;Violation of model assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-tug-of-war-between-colinear-inputs&#34;&gt;The tug-of-war between colinear inputs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-model-fit-considerations&#34;&gt;Improving model fit, considerations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing-notes-and-tips&#34;&gt;Closing notes and tips&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pricing-challenges&#34;&gt;Pricing challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dataset-considerations&#34;&gt;Dataset considerations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretability-of-machine-learning-methods&#34;&gt;Interpretability of machine learning methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization-and-colinear-variables&#34;&gt;Regularization and colinear variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficients-of-a-regularized-model&#34;&gt;Coefficients of a regularized model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Pricing is hard.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media2.giphy.com/media/SG0KKFtwUpqJW/giphy.gif&#34; alt=&#34;Price is Right Contestant… struggling&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Price is Right Contestant… struggling&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is particularly true with large complicated products, common in Business to Business sales (B2B). B2B sellers may lack important information (e.g. accurate estimates of customer budget or ‘street’ prices for the various configurations of their products – the &lt;a href=&#34;#pricing-challenges&#34;&gt;Pricing challenges&lt;/a&gt; section discusses other internal and external limitations in setting prices). However organizations typically &lt;em&gt;do have&lt;/em&gt; historical data on internal sales transactions as well as leadership with a strong desire for &lt;em&gt;insights&lt;/em&gt; into pricing behavior. For now I’ll put aside the question of how to use econometric approaches to set ideal prices. Instead, I’ll walk through some statistical methods that rely only on historical sales information and that can be used for analyzing differences, trends, and abnormalities in your organizations pricing.&lt;/p&gt;
&lt;p&gt;With internal data&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; you can still support answers to many important questions and provide a starting place for more sophisticated pricing strategies or analyses. I will be writing a series of posts on pricing (see &lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt; section for likely topics). In this post, I will focus on the basic ideas and considerations important when using regression models to understand prices.&lt;/p&gt;
&lt;p&gt;I will use data from the Ames, Iowa housing market. See the &lt;a href=&#34;#dataset-considerations&#34;&gt;Dataset considerations&lt;/a&gt; section for why I use the &lt;a href=&#34;https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf&#34;&gt;ames&lt;/a&gt; dataset as an analogue for B2B selling / pricing scenarios (as well as problems with this choice). My examples were built using the R programming language, you can find the source code at my &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-08-11-pricing-insights-from-historical-data-part-1.Rmd&#34;&gt;github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;what-influences-price&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What influences price?&lt;/h1&gt;
&lt;p&gt;Products have features. These features&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; can be used to train a model to estimate price. For a linear model, the outputted coefficients associated with these features can act as proxies for the expected &lt;em&gt;dollar per unit&lt;/em&gt; change associated with the component&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Ceteris_paribus&#34;&gt;ceteris paribus&lt;/a&gt;). In pricing contexts, the idea that regression coefficients relate to the value (i.e. ‘implicit price’) of the constituent components of the product is sometimes called hedonic modeling&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. An assumption in hedonic modeling is that our model includes all variables that matter to price&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. This assumption is important in that it suggests:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression modeling of price is not well suited to contexts where you cannot explain a reasonably high proportion of the variance in the price of your product.&lt;/li&gt;
&lt;li&gt;You should be particularly thoughtful regarding the variables you include in your model and avoid including variables that represent overlapping/duplicated information about your product.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a more full discussion on hedonic modeling&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; see the &lt;a href=&#34;https://www.oecd-ilibrary.org/docserver/9789264197183-7-en.pdf?expires=1597241573&amp;amp;id=id&amp;amp;accname=guest&amp;amp;checksum=0FA9E2EB249B3EB5DBA108E3AC44CCA3&#34;&gt;Handbook on Residential Property Prices Indices&lt;/a&gt;. In this post I will build very simple models that obviously don’t represent all relevant factors or meet some of the strong assumptions in hedonic modeling. Instead, my focus is on illustrating some basic considerations in regression that are particular important in pricing contexts.&lt;/p&gt;
&lt;div id=&#34;simple-linear-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear regression model&lt;/h2&gt;
&lt;p&gt;Let’s build a model for home price that uses &lt;em&gt;just&lt;/em&gt; house square footage, represented by &lt;code&gt;Gr_Liv_Area&lt;/code&gt;&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;, as a feature for predicting home price.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{Sale\_Price} = 13290 + 112(\operatorname{Gr\_Liv\_Area}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The coefficient on sale price of &lt;em&gt;112&lt;/em&gt; is a measure of expected dollars per unit change in square foot. If you build the model without an intercept&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;, the coefficient more directly equates to dollars per square foot&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. However it’s &lt;em&gt;typically&lt;/em&gt; more appropriate to &lt;a href=&#34;https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model&#34;&gt;leave the intercept in the model&lt;/a&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inference-and-challenges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference and challenges&lt;/h2&gt;
&lt;p&gt;In evaluating the impact of a component on the price, we don’t want &lt;em&gt;just&lt;/em&gt; an estimate of the magnitude of the impact. Instead we want a measure of the likely range this estimate falls within. The traditional way to compute this is by using the standard error associated with our estimate.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13289.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3269.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can do &lt;em&gt;{coefficient estimate} +/- 2&lt;/em&gt;&lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt;&lt;em&gt;{standard error of estimate}&lt;/em&gt; to get a 95% confidence interval for where we believe the ‘true’ coefficient estimate for &lt;code&gt;Gr_Liv_Area&lt;/code&gt; falls. In this case, this means that across our observations, the mean price change per square foot (while only taking into account this variables) is roughly between 108 and 116&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;violation-of-model-assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Violation of model assumptions&lt;/h3&gt;
&lt;p&gt;Linear regression has a number of &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression#Assumptions&#34;&gt;model assumptions&lt;/a&gt;. Following these is less important when using the model for predictions compared to for inference&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;. However if you are interpreting the coefficients as representations of the value associated with components of a product (as in our case), model assumptions &lt;em&gt;matter&lt;/em&gt;&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;. I will leave it up to you and Google to read more on model assumptions&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tug-of-war-between-colinear-inputs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The tug-of-war between colinear inputs&lt;/h3&gt;
&lt;p&gt;Let’s add to our regression model another variable, number of bathrooms represented by the &lt;code&gt;bathrooms&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{Sale\_Price} = 5491 + 94(\operatorname{Gr\_Liv\_Area}) + 19555(\operatorname{bathrooms}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5491.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3356.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bathrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19555.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2284.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient on square footage has decreased – this is because number of bathrooms and square feet of home are correlated (they have a &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&#34;&gt;correlation&lt;/a&gt; of 0.71). Some of the impact on home price that previously existed entirely in the coefficient for &lt;code&gt;Gr_Liv_Area&lt;/code&gt; is now shared with the related &lt;code&gt;bathrooms&lt;/code&gt; variable. Also, the standard error on &lt;code&gt;Gr_Liv_Area&lt;/code&gt; has increased – representing greater uncertainty as to the mean impact of the variable within the model (compared to the prior simple linear regression example).&lt;/p&gt;
&lt;p&gt;Let’s consider a model with another variable added: &lt;code&gt;TotRms_AbvGrd&lt;/code&gt;, the total number of rooms (above ground and excluding bathrooms) in the home. This variable is also correlated with &lt;code&gt;Gr_Liv_Area&lt;/code&gt; and number of &lt;code&gt;bathrooms&lt;/code&gt; (correlation of ~0.8 and ~0.6 respectively).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\operatorname{Sale\_Price} &amp;amp;= 35600 + 122(\operatorname{Gr\_Liv\_Area})\ + \\
&amp;amp;\quad 20411(\operatorname{bathrooms}) - 11389(\operatorname{TotRms\_AbvGrd})\ + \\
&amp;amp;\quad \epsilon
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35600.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4384.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;121.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bathrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20410.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2245.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TotRms_AbvGrd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-11389.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1093.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice the coefficient on &lt;code&gt;TotRms_AbvGrd&lt;/code&gt; is negative at &lt;em&gt;-11792.2&lt;/em&gt;. This doesn’t mean houses with more bedrooms are associated with negative home prices. Though it suggests a house with the same square footage and number of bathrooms will be less expensive if it has more rooms&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theoretical challenge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pretend we put in another variable: &lt;code&gt;half_bathrooms&lt;/code&gt; that represented the number of half bathrooms in the home. Our previous variable &lt;code&gt;bathrooms&lt;/code&gt; already included both full and half bathrooms. This presents a theoretical problem for the model: bathrooms would be represented in two different variables that have a &lt;em&gt;necessary&lt;/em&gt; overlap&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt; with one another. Our understanding of the value of a bathroom as its coefficient value would become less clear&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Beyond this &lt;em&gt;theoretical challenge&lt;/em&gt;, duplicated or highly associated inputs also create &lt;em&gt;numeric challenges&lt;/em&gt;. The remainder of this post will be focused on numeric challenges and considerations in fitting regression models. These lessons can be applied broadly across inferential contexts but are particularly important in pricing analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numeric challenge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linear regression models feature this ‘tug-of-war’ between the magnitude of coefficients whereby correlated variables share general influences in the model. At times this causes similar variables to seem to have opposing impacts&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;. When evaluating coefficients for pricing analysis exercises&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt; this competition between coefficients has potential drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As you increase the number of variables in the model, colinearity can make for models with a high degree of instability / variance in the parameter estimates – meaning that the coefficients in your model (and your resulting predictions) could change dramatically even from small changes in the training data&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;, which undermines confidence in estimates.&lt;/li&gt;
&lt;li&gt;You may want to limit methods that result in models with unintuitive variable relationships (e.g. where related factors have coefficients that appear to act in opposing directions).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-model-fit-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improving model fit, considerations&lt;/h2&gt;
&lt;p&gt;I do not discuss the topic of &lt;em&gt;variable selection&lt;/em&gt;, but highly recommend the associated chapter in the online textbook &lt;a href=&#34;http://www.feat.engineering/selection.html&#34;&gt;Feature Engineering and Selection&lt;/a&gt; by Max Kuhn and Kjell Johnson.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data transformations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before modeling, transformations to the underlying data are often applied for one of several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To help satisfy model assumptions or to minimize the impact of outliers and influential points on estimates.&lt;/li&gt;
&lt;li&gt;To improve the fit of the model.&lt;/li&gt;
&lt;li&gt;To help with model interpretation&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To facilitate preprocessing requirements important to the fitting procedure&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Important in pricing contexts, transformations to the data alter the meaning of the coefficients&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt;. Data transformations may improve model fit, but may complicate coefficient interpretability. In some cases this may be helpful in other cases it may not – it all depends on the aims of the model and the types of interpretations the analyst is hoping to make&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt;. As part of an internal presentation given at NetApp on pricing, I describe some common variable transformations and how these affect the resulting interpretation of the coefficients:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/dqrkIIziBLE?start=448&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;&lt;strong&gt;More sophisticated Machine Learning Methods:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When using more sophisticated machine learning techniques the term &lt;em&gt;data transformation&lt;/em&gt; is sometimes supplanted by the term &lt;em&gt;feature engineering&lt;/em&gt; (though the latter typically suggests more numerous or more complicated changes to input data). Some machine learning techniques&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; can also identify hard to find relationships or obviate the need for complex data transformations that would be required to produce good model fits with a linear model&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;. This may save an analyst time or allow them to produce models with a better fit but may come at a cost to ease of model interpretability. For a brief discussion, see the section &lt;a href=&#34;#interpretability-of-machine-learning-methods&#34;&gt;Interpretability of machine learning methods&lt;/a&gt;. For this post, I will stick to linear models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitting procedures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatives to the standard optimization technique for linear regression, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares#:~:text=In%20statistics%2C%20ordinary%20least%20squares,in%20a%20linear%20regression%20model.&amp;amp;text=Under%20these%20conditions%2C%20the%20method,the%20errors%20have%20finite%20variances&#34;&gt;Ordinary Least Squares&lt;/a&gt; (OLS), may be more robust to model assumptions and influential points or tend to produce more stable estimates&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;. A few options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Regularization&lt;/em&gt;: puts constraints on the linear model that discourage high levels of variance in your coefficient estimates. See the section &lt;a href=&#34;#regularization-and-colinear-variables&#34;&gt;Regularization and colinear variables&lt;/a&gt; for a more full discussion on how L1 &amp;amp; L2 penalties affect estimates for colinear inputs differently&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bayesian approaches&lt;/em&gt;: can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34;&gt;priors&lt;/a&gt; and rigorous estimation procedures to limit &lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34;&gt;overfitting&lt;/a&gt; and subdue extreme estimates.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Robust regression&lt;/em&gt;: typically refers to using weighted least squares (or similar methods) which allows for giving different amounts of weight to observations (typically to reduce the weight of extreme and influential points).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these fitting procedures has different advantages and disadvantages and will modulate coefficient estimates differently&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-notes-and-tips&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Closing notes and tips&lt;/h1&gt;
&lt;p&gt;You can use regression models to evaluate the impact of different factors on price. However it is important to consider how coefficient estimates will respond to your particular input data (e.g. multicolinearity of your inputs or violations of your model assumptions) and to use techniques that will produce an appropriate model fit for your needs. In pricing contexts in particular you should consider the types of inferences you will be asked to make and build your model in a way that fits your business requirements.&lt;/p&gt;
&lt;p&gt;Some tips for building models for inference in pricing contexts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your model doesn’t explain a high proportion of the data, be careful what you say to stakeholders about the respective value of components&lt;a href=&#34;#fn30&#34; class=&#34;footnote-ref&#34; id=&#34;fnref30&#34;&gt;&lt;sup&gt;30&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Getting a good model fit should be a driving force. However, in a similar way to how you may prefer fewer variables or a more simple modeling technique, you may also prefer fewer and less complicated variable transformations&lt;a href=&#34;#fn31&#34; class=&#34;footnote-ref&#34; id=&#34;fnref31&#34;&gt;&lt;sup&gt;31&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;When evaluating the influence of the components of your product, review the variability in your coefficient estimates and not just the estimates themselves.&lt;/li&gt;
&lt;li&gt;Consider building linear models using multiple model fit techniques&lt;a href=&#34;#fn32&#34; class=&#34;footnote-ref&#34; id=&#34;fnref32&#34;&gt;&lt;sup&gt;32&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn33&#34; class=&#34;footnote-ref&#34; id=&#34;fnref33&#34;&gt;&lt;sup&gt;33&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Even if you plan on using a linear model, using a generic more complex machine learning model can be helpful as a sanity check. If model performance is not substantially different between your models, you are fine, if it is, there may be an important relationship you are missing and need to identify.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for &lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt; on related topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;pricing-challenges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pricing challenges&lt;/h2&gt;
&lt;p&gt;Final price paid by a customer may vary substantially within a given product. This variability is often due in part to a high degree of complexity inherent in the product and different configurations between customers&lt;a href=&#34;#fn34&#34; class=&#34;footnote-ref&#34; id=&#34;fnref34&#34;&gt;&lt;sup&gt;34&lt;/sup&gt;&lt;/a&gt;. Fluctuations in product demand and macroeconomic factors are other important influences, as are factors associated with the buyer’s / seller’s negotiation skill and ability to use market information to leverage a higher or lower discount.&lt;/p&gt;
&lt;p&gt;The final price paid may also be influenced by a myriad of competing internal interests. Sales representatives may want leniency in price guidelines so they can hit their quota. Leadership may be concerned about potential brand erosion that often comes with lowering prices. Equity holders may be focused on immediate profitability or may be willing to sacrifice margin in order to expand market share. Effectively setting price guidelines requires the application of various economic, mathematical, and sociological principles&lt;a href=&#34;#fn35&#34; class=&#34;footnote-ref&#34; id=&#34;fnref35&#34;&gt;&lt;sup&gt;35&lt;/sup&gt;&lt;/a&gt; which may not be feasible to set-up&lt;a href=&#34;#fn36&#34; class=&#34;footnote-ref&#34; id=&#34;fnref36&#34;&gt;&lt;sup&gt;36&lt;/sup&gt;&lt;/a&gt;. Implementation of which requires reliable data, which could be lacking due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Market information may be inaccurate or unavailable&lt;a href=&#34;#fn37&#34; class=&#34;footnote-ref&#34; id=&#34;fnref37&#34;&gt;&lt;sup&gt;37&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Total&lt;/em&gt; costs of production may not be accessible (from your position in the organization).&lt;/li&gt;
&lt;li&gt;Current organizational goals may not be well defined.&lt;/li&gt;
&lt;li&gt;Information on successful deals may be more reliable than information on missed deals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These (or a host of other gaps in information) may make it difficult to define an objective function for identifying optimal price guidelines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-pricing-posts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Future pricing posts&lt;/h2&gt;
&lt;p&gt;In a series of posts I will tackle a variety of questions stakeholders may ask regarding organizational pricing. Some likely topics include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How do differences in product components associate with differences in price? What is the magnitude of the influence of these factors?&lt;/li&gt;
&lt;li&gt;How have these factors changed over time?&lt;/li&gt;
&lt;li&gt;Which customers fall outside the ‘normal’ behavior in regard to the price they are receiving?&lt;/li&gt;
&lt;li&gt;How can complexities in pricing strategy be captured by a statistically rigorous modeling framework (E.g. when volume dictates price)?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataset considerations&lt;/h2&gt;
&lt;p&gt;The relevant qualities of a dataset I was looking for were:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Multiple years of data&lt;/li&gt;
&lt;li&gt;Many features, with a few key variables associated with a large proportion of the variance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;ames&lt;/code&gt; housing dataset meets these qualifications and i was already familiar with it. Evaluating home prices can serve as a practical analogue for our problem; both home sales and business to business sales often represent large purchases with many features influencing price. You can pretend that individual rows represent B2B transactions for a large corporation selling a complicated product line (rather than individual home sales).&lt;/p&gt;
&lt;p&gt;There are also many important &lt;em&gt;differences&lt;/em&gt; between home sales and B2B sales that make this a weaker analogue. To name a few:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in B2B contexts, repeat sales are typically more important than initial sales. In the housing market, repeat sales don’t exist.&lt;/li&gt;
&lt;li&gt;information on home prices and prior home sales is accessible to both the buyer and seller – meaning there are no options for targeted pricing.&lt;/li&gt;
&lt;li&gt;in B2B contexts, an influential buyer may be able to leverage the possibility of a partnership of some kind in order to secure a better deal on a large purchase&lt;a href=&#34;#fn38&#34; class=&#34;footnote-ref&#34; id=&#34;fnref38&#34;&gt;&lt;sup&gt;38&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Volume selling schemes and other pricing strategies may have less of an impact on house prices compared to in B2B settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the notes in this first post, these don’t matter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretability-of-machine-learning-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretability of machine learning methods&lt;/h2&gt;
&lt;p&gt;In some pricing scenarios tree-based methods may be particularly helpful in modeling price – particularly in contexts where the price of a product can be well defined by if-then statements. This may be useful in cases where there is volume pricing – e.g. the pricing approach is different depending on the amount you are purchasing. Perhaps better though would be cubist models which start as decision trees but then terminate into individual linear models (allowing for different linear models based off pre-defined if-then statements).&lt;/p&gt;
&lt;p&gt;(Ignoring figuring out the &lt;em&gt;ideal&lt;/em&gt; type of model or feature engineering regiment&lt;a href=&#34;#fn39&#34; class=&#34;footnote-ref&#34; id=&#34;fnref39&#34;&gt;&lt;sup&gt;39&lt;/sup&gt;&lt;/a&gt; for your problem) the typical juxtaposition between linear models and more sophisticated machine learning techniques is in how easy they are to interpret. Sophisticated machine learning methods (sometimes described as ‘black-boxes’&lt;a href=&#34;#fn40&#34; class=&#34;footnote-ref&#34; id=&#34;fnref40&#34;&gt;&lt;sup&gt;40&lt;/sup&gt;&lt;/a&gt;) &lt;em&gt;can&lt;/em&gt; be made to be interpretable. Interpretation typically involves some approach that evaluates how the predictions change in relation to some change in the underlying data. This &lt;em&gt;prediction focused&lt;/em&gt; way of interpreting a model has the advantage of being more standard across model types. The argument goes that regardless of the structure of the model, you always get predictions, hence you should use these predictions to drive your interpretations of the model. This enables you to compare models (across things other than just raw performance) regardless of the type of model you use.&lt;/p&gt;
&lt;p&gt;The advantage linear models have is that the &lt;em&gt;model form itself&lt;/em&gt; is highly interpretable. Unlike other models the parameters of linear models are directly aggregatable. With a linear model you can more easily say how much value a component of a product adds to the price. With other types of models this translation is usually more difficult.&lt;/p&gt;
&lt;p&gt;Linear models can be understood by a wider audience and also may be viewed as more logical or fair&lt;a href=&#34;#fn41&#34; class=&#34;footnote-ref&#34; id=&#34;fnref41&#34;&gt;&lt;sup&gt;41&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn42&#34; class=&#34;footnote-ref&#34; id=&#34;fnref42&#34;&gt;&lt;sup&gt;42&lt;/sup&gt;&lt;/a&gt;. However, if you build a linear model with highly complicated transformations, interactions, or non-linear terms, notions of this ‘interpretability advantage’ start to deteriorate&lt;a href=&#34;#fn43&#34; class=&#34;footnote-ref&#34; id=&#34;fnref43&#34;&gt;&lt;sup&gt;43&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In summary, the breakdown of linear regression vs complicated machine learning models may be similar in pricing contexts as it is in other problem spaces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you only care about accuracy of your predictions (i.e. pricing estimates) or want to save time on complex feature engineering more sophisticated machine learning techniques may be valuable.&lt;/li&gt;
&lt;li&gt;If you care about interpretability or have audit requirements regarding prices, linear models have a certain advantages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;regularization-and-colinear-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regularization and colinear variables&lt;/h2&gt;
&lt;p&gt;Regularization typically comes in two flavors; either an L1 penalty (lasso regression) or L2 penalty (ridge regression), or some combination of these (elastic net) is applied to the linear model. These penalties provides a cost for larger coefficient which acts to decrease the variance in our estimates&lt;a href=&#34;#fn44&#34; class=&#34;footnote-ref&#34; id=&#34;fnref44&#34;&gt;&lt;sup&gt;44&lt;/sup&gt;&lt;/a&gt;. In conditions of colinear inputs, these two penalties act differently on coefficient estimates of colinear features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lasso regression tends to choose a ‘best’ variable (among a subset of colinear variables) whose coefficient ‘survives’, while the other associated variables’ coefficients are pushed towards zero&lt;/li&gt;
&lt;li&gt;For ridge regression, coefficients of similar variables gravitate to a similar value&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficients-of-a-regularized-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficients of a regularized model&lt;/h2&gt;
&lt;p&gt;Variable inputs are usually standardized before applying regularization. Hence because inputs are all (essentially) put on the same scale, the coefficient estimates can be directly compared with one another as measures of their relative influence on the target (home price). This ease of comparison may be convenient. However if our goal is interpreting the coefficient estimates in terms of dollar change per &lt;em&gt;unit&lt;/em&gt; increase, we will need to back-transform the coefficients.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Internal sales data alone is limited in that its focused on only a component of sales, rather than considering the full picture – this puts the analyst in a familiar position of one with incomplete information, and a constrained scope of influence.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Dataset should be structured such that each feature is a column and each row an observation, e.g. a sale.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Sort of, and under certain contexts…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hedonic_regression&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Hedonic_regression&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Missing important components or misattributing influence of price can cause bias in the model (omitted variable bias).&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;and how it can also be used for things like creating price indices&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Does not including basement.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;I.e. make it zero so that the expected value of a house of 0 square foot is $0&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;In this case, the coefficient for the model becomes 119.7 if the intercept is set to zero.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;Hedonic modeling also has a variety of approaches associated with evaluating changes in the intercept term between models that (again) can be read in the the &lt;a href=&#34;https://www.oecd-ilibrary.org/docserver/9789264197183-7-en.pdf?expires=1597241573&amp;amp;id=id&amp;amp;accname=guest&amp;amp;checksum=0FA9E2EB249B3EB5DBA108E3AC44CCA3&#34;&gt;Handbook on Residential Property Prices Indices&lt;/a&gt;.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Note there are more modern approaches to estimating this range using Bayesian or simulation based methods.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;At least to the extent that satisfying them doesn’t improve your predictions, or suggest a different model type may be more appropriate.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Although some would argue that you don’t need to worry too much about any of your assumptions except that your observations are independent of one another.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;Model assumptions of linear regression by Ordinary Least Squares is already covered extensively in essentially every tutorial and Introduction to Statistics textbook on regression.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;Perhaps representing a preference for larger rooms or open space among buyers or a confounding effect with some other variable. For the purposes of this post i simply want to point out how coefficient values can vary under conditions of colinearity.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;(though not perfect colinearity)&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;Hence the importance of being particularly thoughtful of the variables you input into the model and avoiding variables that roughly duplicate one another.&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;A common rule of thumb for when variables are ‘too correlated’ is 0.90 – at least in regression contexts and cases where you are focused on inference. In other contexts (e.g. those that appear in &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; prediction competitions) this threshold can be much higher. However, as discussed, lower levels of correlation can still contribute to instability in your coefficient estimates&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Where you care about the individual parameter estimates and want them to be meaningful.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;This is what “variance” means in the bias-variance trade-off common in model development. This may also be referred to as instability in the model or parameter estimates.&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;An example of this may be standardizing the underlying data so that the coefficient estimates may be more directly compared to one another (as the underlying data is all on the same scale).&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Standardizing the data is also important for many fitting methods, e.g. regularization.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;E.g. a log transform on an input alters the interpretation of the coefficient to be something closer to dollars per percentage point change of input. Log on target means percentage change in price per unit change in input. If you take the log of both your inputs and your target, the coefficient represents percent change in x related to percent change in y, also known as an ‘elasticity’ model in econometrics.&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;There may be a preference to speak in the simplest terms: change in price as a function of unit change in component – which may put pressure on the analyst to limit data transformations. It is the analysts job then to strike the correct balance between producing a model that fits the data and one that can be understood by stakeholders.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;Neural networks in particular.&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;Many non-linear methods still have sophisticated preprocessing requirements. Though these are sometimes more generic – meaning less work to customize between problems to reach at least some minimum level of fit between problems (again, in some contexts).&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;Some of what I read on hedonic modeling seemed to discourage the use of methods other than Ordinary Least Squares (e.g. Weighted Least Squares) but I’ve found other methods to be helpful.&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Regularization with an L1 penalty provides the added bonus of also doing variable selection.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;For robust methods and regularization, there are less established methods for producing confidence intervals. You may need to use simulation methods (which are more computationally intensive).&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn30&#34;&gt;&lt;p&gt;Generally it is a good idea to describe the mean error or some other measure, so that they can get a sense of how close the model you are describing is fitting the data, or whether the effects you are talking about are general, but not particularly useful for predictions.&lt;a href=&#34;#fnref30&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn31&#34;&gt;&lt;p&gt;And a preference for transformations that retain an intuitive interpretability for the model.&lt;a href=&#34;#fnref31&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn32&#34;&gt;&lt;p&gt;Then review the coefficient estimates across them.&lt;a href=&#34;#fnref32&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn33&#34;&gt;&lt;p&gt;I tend to rely heavily on regularization techniques.&lt;a href=&#34;#fnref33&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn34&#34;&gt;&lt;p&gt;A variety of factors though push organizations to simplify their products and this process – for the purposes of this post though, I’ll assume a complicated product portfolio.&lt;a href=&#34;#fnref34&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn35&#34;&gt;&lt;p&gt;For a more full discussion on these concepts see UVA coursera specialization on &lt;a href=&#34;https://www.coursera.org/learn/uva-darden-bcg-pricing-strategy-cost-economics?utm_source=gg&amp;amp;utm_medium=sem&amp;amp;utm_content=01-CourseraCatalog-DSA-US&amp;amp;campaignid=9918777773&amp;amp;adgroupid=102058276958&amp;amp;device=c&amp;amp;keyword=&amp;amp;matchtype=b&amp;amp;network=g&amp;amp;devicemodel=&amp;amp;adpostion=&amp;amp;creativeid=434544785640&amp;amp;hide_mobile_promo=&amp;amp;gclid=CjwKCAjwsan5BRAOEiwALzomXyDwos6rlUmAwFrv9BjJFUPnyvzPRedArpRD2iRkocMemgtsZrfihxoCjfUQAvD_BwE&#34;&gt;Cost and Economics in Pricing Strategy&lt;/a&gt;.&lt;a href=&#34;#fnref35&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn36&#34;&gt;&lt;p&gt;Organizations may lack the money or the will.&lt;a href=&#34;#fnref36&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn37&#34;&gt;&lt;p&gt;Maybe your company doesn’t want to pay the expensive prices that data vendors set for this information (this may especially be a problem if you are a small organization with a small budget).&lt;a href=&#34;#fnref37&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn38&#34;&gt;&lt;p&gt;While a home seller may be more sympathetic to some buyers over others (E.g. a newly wedded couple looking to start a family over a real-estate mogul looking for investment properties), such preferences likely impact price less than the analogue in the B2B contexts where sellers seek to strike details with popular brands as means of establishing product relevance and enabling further marketing and potentially collaboration opportunities. It is important to note though that the ‘Clayton Act’ and ‘Robinson Patman Act’ make price discrimination in B2B contexts illegal (except in certain circumstances).&lt;a href=&#34;#fnref38&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn39&#34;&gt;&lt;p&gt;PCA or factor analysis seems like a potentially useful approach in pricing contexts in cases where the variables you have do not clearly represent discrete components of the product – hopefully PCA would help to identify these implicit components.&lt;a href=&#34;#fnref39&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn40&#34;&gt;&lt;p&gt;Due to the lack of transparency into how they produce predictions.&lt;a href=&#34;#fnref40&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn41&#34;&gt;&lt;p&gt;Or at least in places where a price seems unfair, it may be easier to quickly identify where the issue lies.&lt;a href=&#34;#fnref41&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn42&#34;&gt;&lt;p&gt;There is a book &lt;em&gt;Weapons of Math Destruction&lt;/em&gt; by Cathy O’Neil that points to a lack of interpretability as one of the chief concerns with modern learning algorithms.&lt;a href=&#34;#fnref42&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn43&#34;&gt;&lt;p&gt;May as well use a Machine Learning method at this point.&lt;a href=&#34;#fnref43&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn44&#34;&gt;&lt;p&gt;In both cases non-informative regressors will tend towards zero (in the case of ridge regression, they will never &lt;em&gt;quite&lt;/em&gt; reach zero). These approaches typically require tuning to identify the ideal weight (i.e. pressure) assigned to the penalty.&lt;a href=&#34;#fnref44&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>