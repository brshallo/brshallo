<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats on Bryan Shalloway&#39;s Blog</title>
    <link>/tags/rstats/</link>
    <description>Recent content in rstats on Bryan Shalloway&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/rstats/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Remember, Resampling Techniques Change the Base Rates of Your Predictions</title>
      <link>/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/11/23/remember-resampling-techniques-change-the-base-rates-of-your-predictions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generate-data&#34;&gt;Generate Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-of-feature-and-target&#34;&gt;Association of ‘feature’ and ‘target’&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resample&#34;&gt;Resample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-models&#34;&gt;Build Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#re-scale-predictions-to-predicted-probabilities&#34;&gt;Re-scale Predictions to Predicted Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lift-plots&#34;&gt;Lift plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!-- This chunk allows more than three levels for the Table of Contents --&gt;
&lt;!-- &lt;script&gt; --&gt;
&lt;!--     $(document).ready(function() { --&gt;
&lt;!--       $items = $(&#39;div#TOC li&#39;); --&gt;
&lt;!--       $items.each(function(idx) { --&gt;
&lt;!--         num_ul = $(this).parentsUntil(&#39;#TOC&#39;).length; --&gt;
&lt;!--         $(this).css({&#39;text-indent&#39;: num_ul * 10, &#39;padding-left&#39;: 0}); --&gt;
&lt;!--       }); --&gt;
&lt;!--     }); --&gt;
&lt;!-- &lt;/script&gt; --&gt;
&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; In classification problems, over and under sampling techniques shift the distribution of predicted probabilities towards the minority class. If your problem requires accurate probabilities you will need to adjust your predictions in some way during post-processing (or at another step) to account for this&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;People new to predictive modeling may rush into using sampling procedures without understanding what these procedures are doing. They then sometimes get confused when their predictions appear way off (from those that would be expected according to the base rates in their data). I decided to write this vignette to briefly walk through (a little more explicitly) an example of the implications of under or over sampling procedures on the base rates of predictions&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My examples will appear obvious to individuals with experience in predictive modeling with imbalanced classes. The code and examples are pulled largely from a few emails I sent in early to mid 2018&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; to individuals new to data science.&lt;/p&gt;
&lt;p&gt;Note that this post is not about &lt;em&gt;why&lt;/em&gt; you might want to use resampling procedures&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, it is meant &lt;em&gt;only&lt;/em&gt; to demonstrate that such procedures change the base rates of your predictions (unless adjusted).&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The proportion of TRUE:FALSE cases of the target in classification problems largely determines the base rate of the predictions produced by the model Therefore if you use sampling techniques that change this proportion (e.g. you use sampling techniques to go from 5-95 to 50-50 TRUE-FALSE rates) there is a good chance you will want to rescale / calibrate&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; your predictions before using them in the wild (if you care about things other than simply ranking your observations&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(modelr)
library(ggplot2)
library(gridExtra)
library(purrr)

theme_set(theme_bw())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;generate-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generate Data&lt;/h1&gt;
&lt;p&gt;Generate classification data with substantial class imbalance&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert log odds to probability
convert_lodds &amp;lt;- function(log_odds) exp(log_odds) / (1 + exp(log_odds))

set.seed(123)

minority_data &amp;lt;- tibble(rand_lodds = rnorm(1000, log(.03 / (1 - .03)), sd = 1),
       rand_probs = convert_lodds(rand_lodds)) %&amp;gt;% 
  mutate(target = map(.x = rand_probs, ~rbernoulli(100, p = .x))) %&amp;gt;% 
  unnest() %&amp;gt;% 
  mutate(id = row_number())

# Change the name of the same of the variables to make the dataset more
# intuitive to follow.
example &amp;lt;- minority_data %&amp;gt;% 
  select(id, target, feature = rand_lodds)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;In this dataset we have a class imbalance where our &lt;code&gt;target&lt;/code&gt; is composed of ~5% positive (&lt;code&gt;TRUE&lt;/code&gt;) cases and ~95% negative (&lt;code&gt;FALSE&lt;/code&gt;) cases.&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example %&amp;gt;% 
  count(target) %&amp;gt;% 
  mutate(proportion = round(n / sum(n), 3)) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;target&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;proportion&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.954&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Make 80-20 train - test split&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
train &amp;lt;- example %&amp;gt;% 
  sample_frac(0.80)

test &amp;lt;- example %&amp;gt;% 
  anti_join(train, by = &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;association-of-feature-and-target&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Association of ‘feature’ and ‘target’&lt;/h1&gt;
&lt;p&gt;We have one important input to our model named &lt;code&gt;feature&lt;/code&gt;&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;% 
  ggplot(aes(feature, fill = target))+
  geom_histogram()+
  labs(title = &amp;quot;Distribution of values of &amp;#39;feature&amp;#39;&amp;quot;,
       subtitle = &amp;quot;Greater values of &amp;#39;feature&amp;#39; associate with higher likelihood &amp;#39;target&amp;#39; = TRUE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;resample&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resample&lt;/h2&gt;
&lt;p&gt;Make a new sample &lt;code&gt;train_downsamp&lt;/code&gt; that keeps all positive cases in the training set and an equal number of randomly sampled negative cases so that the split is no longer 5-95 but becomes 50-50.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;minority_class_size &amp;lt;- sum(train$target)

set.seed(1234)

train_downsamp &amp;lt;- train %&amp;gt;% 
  group_by(target) %&amp;gt;% 
  sample_n(minority_class_size) %&amp;gt;% 
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See below for what the distribution of &lt;code&gt;feature&lt;/code&gt; looks like in the down-sampled dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_downsamp %&amp;gt;% 
  ggplot(aes(feature, fill = target))+
  geom_histogram()+
  labs(title = &amp;quot;Distribution of values of &amp;#39;feature&amp;#39; (down-sampled)&amp;quot;,
       subtitle = &amp;quot;Greater values of &amp;#39;feature&amp;#39; associate with higher likelihood &amp;#39;target&amp;#39; = TRUE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build Models&lt;/h2&gt;
&lt;p&gt;Train a logistic regression model to predict positive cases for &lt;code&gt;target&lt;/code&gt; based on &lt;code&gt;feature&lt;/code&gt; using the training dataset without any changes in the sample (i.e. with the roughly 5-95 class imbalance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_5_95 &amp;lt;- glm(target ~ feature, family = binomial(&amp;quot;logit&amp;quot;), data = train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Train a model with the down-sampled (i.e. 50-50) dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_50_50 &amp;lt;- glm(target ~ feature, family = binomial(&amp;quot;logit&amp;quot;), data = train_downsamp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the predictions from each of these models&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; onto our test set (and convert log-odd predictions to probabilities).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds &amp;lt;- test %&amp;gt;% 
  gather_predictions(mod_5_95, mod_50_50) %&amp;gt;% 
  mutate(pred_prob = convert_lodds(pred))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize distributions of predicted probability of a positive and negative case for each model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_histogram()+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The predicted probabilities for the model built with the down-sampled 50-50 dataset are much higher than those built with the original 5-95 dataset. The predictions from the down-sampled model are not accurate predicted probabilities in their current form and would need to be rescaled to reflect the &lt;em&gt;actual&lt;/em&gt; underlying distribution of positive cases associated with the given inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;re-scale-predictions-to-predicted-probabilities&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Re-scale Predictions to Predicted Probabilities&lt;/h1&gt;
&lt;p&gt;Isotonic Regression&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; or Platt scaling&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; could be used. Such methods are used to calibrate outputted predictions and ensure they align with &lt;em&gt;actual&lt;/em&gt; probabilities. Recalibration techniques are typically used when you have models that may not output well-calibrated probabilities&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;. However these methods can also be used to rescale your outputs (as in this case). (In the case of linear models, there are also simpler approaches available&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_50_50_rescaled_calibrated &amp;lt;- train %&amp;gt;% 
  add_predictions(mod_50_50) %&amp;gt;% 
  glm(target ~ pred, family = binomial(&amp;quot;logit&amp;quot;), data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted &amp;lt;- test %&amp;gt;% 
  spread_predictions(mod_5_95, mod_50_50) %&amp;gt;% 
  rename(pred = mod_50_50) %&amp;gt;% 
  spread_predictions(mod_50_50_rescaled_calibrated) %&amp;gt;% 
  select(-pred) %&amp;gt;% 
  gather(mod_5_95, mod_50_50_rescaled_calibrated, key = &amp;quot;model&amp;quot;, value = &amp;quot;pred&amp;quot;) %&amp;gt;% 
  mutate(pred_prob = convert_lodds(pred)) 

test_with_preds_adjusted %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_histogram()+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the predictions have been calibrated according to their underlying base rate, you can see the distributions of the predictions between the models are essentially the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;Rebuilding plots but using density distributions by class (rather than histograms based on counts).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_density(alpha = 0.3)+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted %&amp;gt;% 
  ggplot(aes(x = pred_prob, fill = target))+
  geom_density(alpha = 0.3)+
  facet_wrap(~model, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;lift-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lift plots&lt;/h2&gt;
&lt;p&gt;Predictions from model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_with_preds_adjusted %&amp;gt;% 
  mutate(target = factor(target, c(&amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;))) %&amp;gt;% 
  filter(model == &amp;quot;mod_5_95&amp;quot;) %&amp;gt;%
  yardstick::lift_curve(target, pred) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-23-remember-resampling-techniques-change-the-base-rates-of-your-predictions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I expect the audience for this post to be rather limited.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I wrote this example After having conversations related to this a few times (and participants not grasping points that would become clear with demonstration).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;before I started using &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;or any of a myriad of topics related to this.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;There are often pretty easy built-in ways to accommodate this.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;There are also other reasons you may not want to rescale your predictions but in many cases you will want to.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Could have been more precise here…&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;no need for validation for this example&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;The higher incidence of TRUE values in the target at higher scores demonstrates the features predictive value.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;One built with 5-95 split the other with a downsampled 50-50 split.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Decision tree based approach&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;Logistic regression based approach&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;E.g. when using Support Vector Machines&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;In this case we are starting with a linear model hence we could also have just changed the intercept value to get the same affect. Rescaling methods act on the &lt;em&gt;predictions&lt;/em&gt; rather than the model parameters. Hence these scaling methods have the advantage of being generalizable as they are agnostic to model type.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Feature Engineering with Sliding Windows and Lagged Inputs</title>
      <link>/2020/10/12/window-functions-for-resampling/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/12/window-functions-for-resampling/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;Load data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering-data-splits&#34;&gt;Feature Engineering &amp;amp; Data Splits&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-based-features-before-split-use-dplyr-or-similar&#34;&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-splits&#34;&gt;Data Splits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-features-after-split-use-recipes&#34;&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-specification-and-training&#34;&gt;Model Specification and Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-building-with-hyperparameter-tuning&#34;&gt;Model Building with Hyperparameter Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;The new &lt;code&gt;rsample::sliding_*()&lt;/code&gt; functions bring the windowing approaches used in &lt;a href=&#34;https://github.com/DavisVaughan/slider&#34;&gt;slider&lt;/a&gt; to the sampling procedures used in the &lt;a href=&#34;https://github.com/tidymodels&#34;&gt;tidymodels&lt;/a&gt; framework&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. These functions make evaluation of models with time-dependent variables easier&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For some problems you may want to take a traditional regression or classification based approach&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; while still accounting for the date/time-sensitive components of your data. In this post I will use the &lt;code&gt;tidymodels&lt;/code&gt; suite of packages to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;build lag based and non-lag based features&lt;/li&gt;
&lt;li&gt;set-up appropriate time series cross-validation windows&lt;/li&gt;
&lt;li&gt;evaluate performance of linear regression and random forest models on a regression problem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For my example I will use data from Wake County food inspections. I will try to predict the &lt;code&gt;SCORE&lt;/code&gt; for upcoming restaurant food inspections.&lt;/p&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load data&lt;/h1&gt;
&lt;p&gt;You can use Wake County’s open API (does not require a login/account) and the &lt;a href=&#34;https://github.com/r-lib/httr&#34;&gt;httr&lt;/a&gt; and &lt;a href=&#34;https://github.com/jeroen/jsonlite&#34;&gt;jsonlite&lt;/a&gt; packages to load in the data. You can also download the data directly from the Wake County &lt;a href=&#34;https://data.wakegov.com/datasets/1b08c4eb32f44a198277c418b71b3a48_2&#34;&gt;website&lt;/a&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(httr)
library(jsonlite)
library(tidymodels)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Get food inspections data:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_insp &amp;lt;- GET(&amp;quot;https://opendata.arcgis.com/datasets/ebe3ae7f76954fad81411612d7c4fb17_1.geojson&amp;quot;)

inspections &amp;lt;- content(r_insp, &amp;quot;text&amp;quot;) %&amp;gt;% 
  fromJSON() %&amp;gt;% 
  .$features %&amp;gt;%
  .$properties %&amp;gt;% 
  as_tibble()

inspections_clean &amp;lt;- inspections %&amp;gt;% 
  mutate(date = ymd_hms(DATE_) %&amp;gt;% as.Date()) %&amp;gt;% 
  select(-c(DATE_, DESCRIPTION, OBJECTID))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Get food locations data:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_rest &amp;lt;- GET(&amp;quot;https://opendata.arcgis.com/datasets/124c2187da8c41c59bde04fa67eb2872_0.geojson&amp;quot;) #json

restauraunts &amp;lt;- content(r_rest, &amp;quot;text&amp;quot;) %&amp;gt;% 
  fromJSON() %&amp;gt;% 
  .$features %&amp;gt;%
  .$properties %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  select(-OBJECTID)

restauraunts &amp;lt;- restauraunts %&amp;gt;% 
  mutate(RESTAURANTOPENDATE = ymd_hms(RESTAURANTOPENDATE) %&amp;gt;% as.Date())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Further prep:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Join the &lt;code&gt;inspections&lt;/code&gt; and &lt;code&gt;restaurants&lt;/code&gt; datasets&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Filter out extreme outliers in &lt;code&gt;SCORE&lt;/code&gt; (likely data entry errors)&lt;/li&gt;
&lt;li&gt;Filter to only locations of &lt;code&gt;TYPE&lt;/code&gt; restaurant&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Filter out potential duplicate entries&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It’s important to consider which fields should be excluded for ethical reasons. For our problem, we will say that any restaurant name or location information must be excluded&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspections_restaurants &amp;lt;- inspections_clean %&amp;gt;% 
  left_join(restauraunts, by = c(&amp;quot;HSISID&amp;quot;, &amp;quot;PERMITID&amp;quot;)) %&amp;gt;% 
  filter(SCORE &amp;gt; 50, FACILITYTYPE == &amp;quot;Restaurant&amp;quot;) %&amp;gt;% 
  distinct(HSISID, date, .keep_all = TRUE) %&amp;gt;% 
  select(-c(FACILITYTYPE, PERMITID)) %&amp;gt;% 
  select(-c(NAME, contains(&amp;quot;ADDRESS&amp;quot;), CITY, STATE, POSTALCODE, PHONENUMBER, X, Y, GEOCODESTATUS))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspections_restaurants %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 24,294
## Columns: 6
## $ HSISID             &amp;lt;chr&amp;gt; &amp;quot;04092017542&amp;quot;, &amp;quot;04092017542&amp;quot;, &amp;quot;04092017542&amp;quot;, &amp;quot;04...
## $ SCORE              &amp;lt;dbl&amp;gt; 94.5, 92.0, 95.0, 93.5, 93.0, 93.5, 92.5, 94.0, ...
## $ TYPE               &amp;lt;chr&amp;gt; &amp;quot;Inspection&amp;quot;, &amp;quot;Inspection&amp;quot;, &amp;quot;Inspection&amp;quot;, &amp;quot;Inspe...
## $ INSPECTOR          &amp;lt;chr&amp;gt; &amp;quot;Anne-Kathrin Bartoli&amp;quot;, &amp;quot;Laura McNeill&amp;quot;, &amp;quot;Laura ...
## $ date               &amp;lt;date&amp;gt; 2017-04-07, 2017-11-08, 2018-03-23, 2018-09-07,...
## $ RESTAURANTOPENDATE &amp;lt;date&amp;gt; 2017-03-01, 2017-03-01, 2017-03-01, 2017-03-01,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-data-splits&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feature Engineering &amp;amp; Data Splits&lt;/h1&gt;
&lt;p&gt;Discussion on issue &lt;a href=&#34;https://github.com/tidymodels/rsample/pull/168&#34;&gt;#168&lt;/a&gt; suggests that some features (those depending on prior observations) should be created before the data is split&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. The first and last sub-sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-based-features-before-split-use-dplyr-or-similar&#34;&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-features-after-split-use-recipes&#34;&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;provide examples of the types of features that should be created before and after splitting your data respectively. Lag based features can, in some ways, be thought of as ‘raw inputs’ as they should be created prior to building a &lt;code&gt;recipe&lt;/code&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;lag-based-features-before-split-use-dplyr-or-similar&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lag Based Features (Before Split, use &lt;code&gt;dplyr&lt;/code&gt; or similar)&lt;/h2&gt;
&lt;p&gt;Lag based features should generally be computed prior to splitting your data into “training” / “testing” (or “analysis” / “assessment”&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;) sets. This is because calculation of these features may depend on observations in prior splits&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;. Let’s build a few features where this is the case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prior &lt;code&gt;SCORE&lt;/code&gt; for &lt;code&gt;HSISID&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Average of prior 3 years of &lt;code&gt;SCORE&lt;/code&gt; for &lt;code&gt;HSISISD&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overall recent (year) prior average &lt;code&gt;SCORE&lt;/code&gt; (across &lt;code&gt;HSISISD&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Days since &lt;code&gt;RESTAURANTOPENDATE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Days since last inspection &lt;code&gt;date&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_time_feats &amp;lt;- inspections_restaurants %&amp;gt;% 
  arrange(date) %&amp;gt;% 
  mutate(SCORE_yr_overall = slider::slide_index_dbl(SCORE, 
                                                    .i = date, 
                                                    .f = mean, 
                                                    na.rm = TRUE, 
                                                    .before = lubridate::days(365), 
                                                    .after = -lubridate::days(1))
         ) %&amp;gt;% 
  group_by(HSISID) %&amp;gt;% 
  mutate(SCORE_lag = lag(SCORE),
         SCORE_recent = slider::slide_index_dbl(SCORE, 
                                                date, 
                                                mean, 
                                                na.rm = TRUE, 
                                                .before = lubridate::days(365*3), 
                                                .after = -lubridate::days(1), 
                                                .complete = FALSE),
         days_since_open = (date - RESTAURANTOPENDATE) / ddays(1),
         days_since_last = (date - lag(date)) / ddays(1)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  arrange(date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The use of &lt;code&gt;.after = -lubridate::days(1)&lt;/code&gt; prevents data leakage by ensuring that this feature does not include information from the current day in its calculation&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-splits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Splits&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Additional Filtering:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will presume that the model is only intended for restaurants that have previous inspections on record&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt; and will use only the most recent seven years of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_time_feats &amp;lt;- data_time_feats %&amp;gt;% 
  filter(date &amp;gt;= (max(date) - years(7)), !is.na(SCORE_lag))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Initial Split:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After creating our lag based features, we can split our data into training and testing splits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial_split &amp;lt;- rsample::initial_time_split(data_time_feats, prop = .8)
train &amp;lt;- rsample::training(initial_split)
test &amp;lt;- rsample::testing(initial_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Resampling (Time Series Cross-Validation):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this problem we should evaluate our models using time series cross-validation&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;. This entails creating multiple ordered subsets of the training data where each set has a different assignment of observations into “analysis” or “assessment” data&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ideally the resampling scheme used for model evaluation mirrors how the model will be built and evaluated in production. For example, if the production model will be updated once every three months it makes sense that the “assessment” sets be this length. We can use &lt;code&gt;rsample::sliding_period()&lt;/code&gt; to set things up.&lt;/p&gt;
&lt;p&gt;For each set, we will use three years of “analysis” data for training a model and then three months of “assessment” data for evaluation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resamples &amp;lt;- rsample::sliding_period(train, 
                                     index = date, 
                                     period = &amp;quot;month&amp;quot;, 
                                     lookback = 36, 
                                     assess_stop = 3, 
                                     step = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will load in some helper functions I created for reviewing the dates of our resampling windows&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::source_gist(&amp;quot;https://gist.github.com/brshallo/7d180bde932628a151a4d935ffa586a5&amp;quot;)

resamples  %&amp;gt;% 
  extract_dates_rset() %&amp;gt;% 
  print() %&amp;gt;% 
  plot_dates_rset() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 6
##    splits         id     analysis_min analysis_max assessment_min assessment_max
##    &amp;lt;list&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;date&amp;gt;       &amp;lt;date&amp;gt;       &amp;lt;date&amp;gt;         &amp;lt;date&amp;gt;        
##  1 &amp;lt;split [6.2K/~ Slice~ 2013-10-09   2016-10-31   2016-11-01     2017-01-31    
##  2 &amp;lt;split [6.3K/~ Slice~ 2014-01-02   2017-01-31   2017-02-01     2017-04-28    
##  3 &amp;lt;split [6.6K/~ Slice~ 2014-04-01   2017-04-28   2017-05-01     2017-07-31    
##  4 &amp;lt;split [7.1K/~ Slice~ 2014-07-01   2017-07-31   2017-08-01     2017-10-31    
##  5 &amp;lt;split [7.5K/~ Slice~ 2014-10-01   2017-10-31   2017-11-01     2018-01-31    
##  6 &amp;lt;split [7.9K/~ Slice~ 2015-01-02   2018-01-31   2018-02-01     2018-04-30    
##  7 &amp;lt;split [8.3K/~ Slice~ 2015-04-01   2018-04-30   2018-05-01     2018-07-31    
##  8 &amp;lt;split [8.6K/~ Slice~ 2015-07-01   2018-07-31   2018-08-01     2018-10-31    
##  9 &amp;lt;split [9K/1K~ Slice~ 2015-10-01   2018-10-31   2018-11-01     2019-01-31    
## 10 &amp;lt;split [9.5K/~ Slice~ 2016-01-04   2019-01-31   2019-02-01     2019-04-30    
## 11 &amp;lt;split [9.9K/~ Slice~ 2016-04-01   2019-04-30   2019-05-01     2019-07-31    
## 12 &amp;lt;split [10.4K~ Slice~ 2016-07-01   2019-07-31   2019-08-01     2019-10-31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-12-window-functions-for-resampling_files/figure-html/check-resampling-splits-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For purposes of overall &lt;a href=&#34;#model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;, performance across each period will be weighted equally (regardless of number of observations in a period)&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-features-after-split-use-recipes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Features (After Split, use &lt;code&gt;recipes&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;Where possible, features should be created using the &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; package&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;. &lt;code&gt;recipes&lt;/code&gt; makes pre-processing convenient and helps prevent data leakage.&lt;/p&gt;
&lt;p&gt;It is OK to modify or transform a previously created lag based feature in a &lt;code&gt;recipes&lt;/code&gt; step. Assuming that you created the lag based input as well as your resampling windows in an appropriate manner, you should be safe from data leakage issues when modifying the variables during later feature engineering steps&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Some features / transformations I’ll make with &lt;code&gt;recipes&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collapse rare values for &lt;code&gt;INSPECTOR&lt;/code&gt; and &lt;code&gt;TYPE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;log transform &lt;code&gt;days_since_open&lt;/code&gt; and &lt;code&gt;days_since_last&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;add calendar based features&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec_general &amp;lt;- recipes::recipe(SCORE ~ ., data = train) %&amp;gt;% 
  step_rm(RESTAURANTOPENDATE) %&amp;gt;% 
  update_role(HSISID, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_other(INSPECTOR, TYPE, threshold = 50) %&amp;gt;% 
  step_string2factor(one_of(&amp;quot;TYPE&amp;quot;, &amp;quot;INSPECTOR&amp;quot;)) %&amp;gt;%
  step_novel(one_of(&amp;quot;TYPE&amp;quot;, &amp;quot;INSPECTOR&amp;quot;)) %&amp;gt;%
  step_log(days_since_open, days_since_last) %&amp;gt;% 
  step_date(date, features = c(&amp;quot;dow&amp;quot;, &amp;quot;month&amp;quot;)) %&amp;gt;% 
  update_role(date, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_zv(all_predictors()) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s peak at the features we will be passing into the model building step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prep(rec_general, data = train) %&amp;gt;% 
  juice() %&amp;gt;% 
  glimpse() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 17,048
## Columns: 12
## $ HSISID           &amp;lt;fct&amp;gt; 04092016152, 04092014520, 04092014483, 04092012102...
## $ TYPE             &amp;lt;fct&amp;gt; Inspection, Inspection, Inspection, Inspection, In...
## $ INSPECTOR        &amp;lt;fct&amp;gt; David Adcock, Naterra McQueen, Andrea Anover, othe...
## $ date             &amp;lt;date&amp;gt; 2013-10-09, 2013-10-09, 2013-10-09, 2013-10-09, 2...
## $ SCORE_yr_overall &amp;lt;dbl&amp;gt; 96.22766, 96.22766, 96.22766, 96.22766, 96.22766, ...
## $ SCORE_lag        &amp;lt;dbl&amp;gt; 96.0, 95.5, 97.0, 94.5, 97.5, 99.0, 96.0, 96.0, 10...
## $ SCORE_recent     &amp;lt;dbl&amp;gt; 96.75000, 95.75000, 97.50000, 95.25000, 96.75000, ...
## $ days_since_open  &amp;lt;dbl&amp;gt; 6.410175, 7.926964, 7.959276, 8.682029, 8.970432, ...
## $ days_since_last  &amp;lt;dbl&amp;gt; 4.709530, 4.941642, 4.934474, 4.875197, 5.117994, ...
## $ SCORE            &amp;lt;dbl&amp;gt; 98.5, 96.0, 96.0, 93.0, 95.0, 93.5, 95.0, 92.0, 98...
## $ date_dow         &amp;lt;fct&amp;gt; Wed, Wed, Wed, Wed, Wed, Wed, Wed, Wed, Wed, Thu, ...
## $ date_month       &amp;lt;fct&amp;gt; Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, Oct, ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-specification-and-training&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Specification and Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Simple linear regression model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_mod &amp;lt;- parsnip::linear_reg() %&amp;gt;% 
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)

lm_workflow_rs &amp;lt;- workflows::workflow() %&amp;gt;% 
  add_model(lm_mod) %&amp;gt;% 
  add_recipe(rec_general) %&amp;gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;ranger&lt;/code&gt; Random Forest model (using defaults):&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rand_mod &amp;lt;- parsnip::rand_forest() %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)
  
set.seed(1234)
rf_workflow_rs &amp;lt;- workflow() %&amp;gt;% 
  add_model(rand_mod) %&amp;gt;% 
  add_recipe(rec_general) %&amp;gt;% 
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;parsnip::null_model&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The NULL model will be helpful as a baseline Root Mean Square Error (RMSE) comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null_mod &amp;lt;- parsnip::null_model(mode = &amp;quot;regression&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;parsnip&amp;quot;)

null_workflow_rs &amp;lt;- workflow() %&amp;gt;% 
  add_model(null_mod) %&amp;gt;% 
  add_formula(SCORE ~ NULL) %&amp;gt;%
  fit_resamples(resamples,
                control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See code in &lt;a href=&#34;#model-building-with-hyperparameter-tuning&#34;&gt;Model Building with Hyperparameter Tuning&lt;/a&gt; for more sophisticated examples that include hyperparameter tuning for &lt;code&gt;glmnet&lt;/code&gt;&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; and &lt;code&gt;ranger&lt;/code&gt; models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Evaluation&lt;/h1&gt;
&lt;p&gt;The next several code chunks extract the &lt;em&gt;average&lt;/em&gt; performance across “assessment” sets&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt; or extract the performance across each of the individual “assessment” sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_types &amp;lt;- list(&amp;quot;lm&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;null&amp;quot;)

avg_perf &amp;lt;- map(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
                collect_metrics) %&amp;gt;% 
  map2(mod_types, ~mutate(.x, source = .y)) %&amp;gt;% 
  bind_rows() &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_splits_metrics &amp;lt;- function(rs_obj, name){
  
  rs_obj %&amp;gt;% 
    select(id, .metrics) %&amp;gt;% 
    unnest(.metrics) %&amp;gt;% 
    mutate(source = name)
}

splits_perf &amp;lt;- map2(list(lm_workflow_rs, rf_workflow_rs, null_workflow_rs), 
     mod_types, 
     extract_splits_metrics) %&amp;gt;% 
  bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall performance as well as the performance across splits suggests that both models were better than the baseline (the mean within the analysis set)&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; and that the linear model outperformed the random forest model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits_perf %&amp;gt;% 
  mutate(id = forcats::fct_rev(id)) %&amp;gt;% 
  ggplot(aes(x = .estimate, y = id, colour = source))+
  geom_vline(aes(xintercept = mean, colour = fct_relevel(source, c(&amp;quot;lm&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;null&amp;quot;))), 
           alpha = 0.4,
           data = avg_perf)+
  geom_point()+
  facet_wrap(~.metric, scales = &amp;quot;free_x&amp;quot;)+
  xlim(c(0, NA))+
  theme_bw()+
  labs(caption = &amp;quot;Vertical lines are average performance as captured by `tune::collect_metrics()`&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-12-window-functions-for-resampling_files/figure-html/plot-performance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could use a paired sample t-test to formally compare the random forest and linear models’ out-of-sample RMSE performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(
  filter(splits_perf, source == &amp;quot;lm&amp;quot;, .metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% pull(.estimate),
  filter(splits_perf, source == &amp;quot;rf&amp;quot;, .metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% pull(.estimate),
  paired = TRUE
) %&amp;gt;% 
  broom::tidy() %&amp;gt;% 
  mutate(across(where(is.numeric), round, 4)) %&amp;gt;% 
  knitr::kable() &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.high&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;alternative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.0839&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-3.7277&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0033&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1334&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0343&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Paired t-test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;two.sided&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This suggests the better performance by the linear model &lt;em&gt;is&lt;/em&gt; statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other potential steps:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is lots more we could do from here&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;. However the purpose of this post was to provide a short &lt;code&gt;tidymodels&lt;/code&gt; example that incorporates window functions from &lt;code&gt;rsample&lt;/code&gt; and &lt;code&gt;slider&lt;/code&gt; on a regression problem. For more resources on modeling and the &lt;code&gt;tidymodels&lt;/code&gt; framework, see &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels.org&lt;/a&gt; or &lt;a href=&#34;https://www.tmwr.org/&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;model-building-with-hyperparameter-tuning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Building with Hyperparameter Tuning&lt;/h2&gt;
&lt;p&gt;Below is code for tuning a &lt;code&gt;glmnet&lt;/code&gt; linear regression model (use &lt;code&gt;tune&lt;/code&gt; to optimize the L1/L2 penalty)&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec_glmnet &amp;lt;- rec_general %&amp;gt;% 
  step_dummy(all_predictors(), -all_numeric()) %&amp;gt;%
  step_normalize(all_predictors(), -all_nominal()) %&amp;gt;% 
  step_zv(all_predictors())

glmnet_mod &amp;lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&amp;gt;% 
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)

glmnet_workflow &amp;lt;- workflow::workflow() %&amp;gt;% 
  add_model(glmnet_mod) %&amp;gt;% 
  add_recipe(rec_glmnet)

glmnet_grid &amp;lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 
    0.2, 0.4, 0.6, 0.8, 1))

glmnet_tune &amp;lt;- tune::tune_grid(glmnet_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = glmnet_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And code to tune a &lt;code&gt;ranger&lt;/code&gt; Random Forest model, tuning the &lt;code&gt;mtry&lt;/code&gt; and &lt;code&gt;min_n&lt;/code&gt; parameters&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rand_mod &amp;lt;- parsnip::rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)
  
rf_workflow &amp;lt;- workflow() %&amp;gt;% 
  add_model(rand_mod) %&amp;gt;% 
  add_recipe(rec_general)

cores &amp;lt;- parallel::detectCores()

set.seed(1234)
rf_tune &amp;lt;- tune_grid(rf_workflow, 
                         resamples = resamples, 
                         control = control_grid(save_pred = TRUE), 
                         grid = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Link on doing regressions in slider: &lt;a href=&#34;https://twitter.com/dvaughan32/status/1247270052782637056?s=20&#34; class=&#34;uri&#34;&gt;https://twitter.com/dvaughan32/status/1247270052782637056?s=20&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rstudio lightning talk on &lt;code&gt;slider&lt;/code&gt;: &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/&#34; class=&#34;uri&#34;&gt;https://rstudio.com/resources/rstudioconf-2020/sliding-windows-and-calendars-davis-vaughan/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeltime&lt;/code&gt; package that applies &lt;code&gt;tidymodels&lt;/code&gt; suite to time series and forecasting problems: &lt;a href=&#34;https://business-science.github.io/modeltime/&#34; class=&#34;uri&#34;&gt;https://business-science.github.io/modeltime/&lt;/a&gt; (business-science course has more fully developed training materials on this topic as well)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;These were announced with version &lt;a href=&#34;https://github.com/tidymodels/rsample/blob/master/NEWS.md&#34;&gt;0.0.8&lt;/a&gt;. The help pages for &lt;code&gt;rsample&lt;/code&gt; (as well as the &lt;code&gt;slider&lt;/code&gt; package) are helpful resources for understanding the three types of sliding you can use, briefly these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sliding_window()&lt;/code&gt;: only takes into account order / position of dates&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sliding_index()&lt;/code&gt;: slide according to an index&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sliding_period()&lt;/code&gt;: slide according to an index and set k split points based on period (and other function arguments)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;rsample::sliding_index()&lt;/code&gt; and &lt;code&gt;rsample::sliding_period()&lt;/code&gt; are maybe the most useful additions as they allow you to do resampling based on a date/time index. For &lt;code&gt;sliding_index()&lt;/code&gt;, you usually want to make use of the &lt;code&gt;step&lt;/code&gt; argument (otherwise it defaults to having a split for every observation).&lt;/p&gt;
&lt;p&gt;I found &lt;code&gt;rsample::sliding_period()&lt;/code&gt; easier to get acquantied with than &lt;code&gt;rsample::sliding_index()&lt;/code&gt;. However within the &lt;code&gt;slider&lt;/code&gt; package I found &lt;code&gt;slider::sliding_index()&lt;/code&gt; easier to use than &lt;code&gt;slider::sliding_period()&lt;/code&gt;. Perhaps this makes sense as when setting sampling windows you are usually trying to return an object with far fewer rows, that is, collapsed to k number of rows (unless you are doing Leave-One-Out cross-validation). On the other hand, the &lt;code&gt;slider&lt;/code&gt; package is often used in a &lt;code&gt;mutate()&lt;/code&gt; step where you often want to output the same number of observations as are inputted. Perhaps then it is unsurprising the different scenarios when the &lt;code&gt;index&lt;/code&gt; vs &lt;code&gt;period&lt;/code&gt; approach feels more intuitive.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Previously users would have needed to use &lt;code&gt;rsample::rolling_origin()&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;As opposed to a more specialized time-series modeling approach.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This dataset is updated on an ongoing basis as Food Inspections are conducted. This makes it a poor choice as an example dataset (because results will vary if running in the future when more data has been collected). I used it because I am familiar with the dataset, it made for a good example, and because I wanted a publicly documented example of pulling in data using an API (even a simple one).&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;There is also “violations” dataset available, which may have additional useful features, but which I will ignore for this example.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;For this example I’m pretending that we only care about predicting &lt;code&gt;SCORE&lt;/code&gt; for restaurants… as opposed to food trucks or other entities that may receive inspections.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Or at least cases where historical data is claiming there were multiple inspections on the same day.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;In some cases you may need to be more careful than this and exclude information that are proxies for inappropriate fields as well. For example, pretend that the &lt;code&gt;INSPECTOR&lt;/code&gt;s are assigned based on region. In this case, &lt;code&gt;INSPECTOR&lt;/code&gt; would be a proxy for geographic information and perhaps warranting exclusion as well (in certain cases).&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Into training / testing sets or analysis / assessment sets.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;As discussed by Davis Vaughn at the end of this &lt;a href=&#34;https://gist.github.com/DavisVaughan/433dbdceb439c9be30ddcc78d836450d&#34;&gt;gist&lt;/a&gt;.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of &lt;a href=&#34;http://www.feat.engineering/resampling.html&#34;&gt;Feature Engineering and Selection…&lt;/a&gt; for further explanation.]&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;It is important that these features be created in a way that does not cause data leakage.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Which would not be available at the time of prediction.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;I’m a fan of the ability to use negative values in the &lt;code&gt;.after&lt;/code&gt; argument:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
This is a fairly obscure feature in {slider}, but I love it. Don’t want the current day in your rolling window? Set a negative &lt;code&gt;.after&lt;/code&gt; value to shift the end of the window backwards. For example:&lt;br&gt;&lt;br&gt;On day 5&lt;br&gt;.before = days(3)&lt;br&gt;.after = -days(1)&lt;br&gt;&lt;br&gt;Includes days:&lt;br&gt;[2, 4] &lt;a href=&#34;https://t.co/rG0IGuTj1c&#34;&gt;https://t.co/rG0IGuTj1c&lt;/a&gt;
&lt;/p&gt;
— Davis Vaughan (&lt;span class=&#34;citation&#34;&gt;@dvaughan32&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/dvaughan32/status/1233116713010573312?ref_src=twsrc%5Etfw&#34;&gt;February 27, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;If I did not make this assumption, I would need to impute the time based features at this point.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;Two helpful resources for understanding time series cross-validation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;From &lt;a href=&#34;https://eng.uber.com/forecasting-introduction/&#34;&gt;uber engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From &lt;a href=&#34;https://otexts.com/fpp3/tscv.html&#34;&gt;Forecasting Principles and Practices&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;An “Analysis” / “Assessment” split is similar to a “training” / “testing” split but within the training dataset (and typically multiple of these are created on the same training dataset). See section 3.4 of &lt;a href=&#34;http://www.feat.engineering/resampling.html&#34;&gt;Feature Engineering and Selection…&lt;/a&gt; for further explanation.]&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;I’ve tweeted previously about helper functions for reviewing your resampling scheme:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
❤️new &lt;code&gt;rsample::sliding_*()&lt;/code&gt; funs by &lt;a href=&#34;https://twitter.com/dvaughan32?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@dvaughan32&lt;/span&gt;&lt;/a&gt;. It can take a minute to check that all arguments are set correctly. Here are helper funs I&#39;ve used to check that my resampling windows are constructed as intended: &lt;a href=&#34;https://t.co/HhSjuRzAsB&#34;&gt;https://t.co/HhSjuRzAsB&lt;/a&gt; may make into an &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/shiny?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#shiny&lt;/a&gt; dashboard. &lt;a href=&#34;https://t.co/sNloHfkh4a&#34;&gt;pic.twitter.com/sNloHfkh4a&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1314720234373287937?ref_src=twsrc%5Etfw&#34;&gt;October 10, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Note that using &lt;code&gt;rsample::sliding_period()&lt;/code&gt; is likely to produce different numbers of observations between splits.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;It could also make sense to weight performance metrics by number of observations. One way to do this, would be to use a control function to extract the predictions, and then evaluate the performance across the predictions. In my examples below I do keep the predictions, but end-up not doing anything with them. Alternatively you could weight the performance metric by number of observations. The justification for weighting periods of different number of observations equally is that noise may vary consistently across time windows – weighting by observations may allow an individual time period too much influence (simply because it happened to be that there were a greater proportion of inspections at that period).&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;For each split, this will then build the features for the assessment set based on each analysis set.&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Although I just do a simple &lt;code&gt;step_log()&lt;/code&gt; transform below, more sophisticated steps on lag based inputs would also be kosher, e.g. &lt;code&gt;step_pca()&lt;/code&gt;. However there is a good argument that many of these should be done prior to a &lt;code&gt;recipes&lt;/code&gt; step. For example, say you have missing values for some of the lag based inputs – in that case it may make sense to use a lag based method for imputation, which may work better than say a mean imputation using the training set. So, like many things, just be thoughtful and constantly ask youself what will be the ideal method while &lt;em&gt;being careful&lt;/em&gt; that, to the question of “will this data be available prior to the prediction?” that you can answer in the affirmitive.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;Our number of observations is relatively high compared to the number of features, so there is a good chance we will have relatively low penalties. While working interactively, I did not see any substantive difference in performance.&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;Remember that this is not weighted by observations, so each assessment set impacts the overall performance equally, regardless of small differences in number of observations.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;There is no baseline performance for Rsquared because the metric itself is based off amount of variance that is explained compared to the baseline (i.e. the mean).&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;You would likely iterate on the model building process (e.g. perform exploratory data analysis, review outliers in initial models, etc.) and eventually get to a final set of models to evaluate on the test set.&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;I added a few other links to the &lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt; section in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Our number of observations is relatively high compared to the number of features, so there is a good chance we will have relatively low penalties.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;This was taking a &lt;em&gt;long&lt;/em&gt; time and is part of why I decided to move the tuned examples to the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;.&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression in Pricing Analysis, Essential Things to Know</title>
      <link>/2020/08/17/pricing-insights-from-historical-data-part-1/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/17/pricing-insights-from-historical-data-part-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-influences-price&#34;&gt;What influences price?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-regression-model&#34;&gt;Simple linear regression model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference-and-challenges&#34;&gt;Inference and challenges&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#violation-of-model-assumptions&#34;&gt;Violation of model assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-tug-of-war-between-colinear-inputs&#34;&gt;The tug-of-war between colinear inputs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-model-fit-considerations&#34;&gt;Improving model fit, considerations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing-notes-and-tips&#34;&gt;Closing notes and tips&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pricing-challenges&#34;&gt;Pricing challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dataset-considerations&#34;&gt;Dataset considerations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretability-of-machine-learning-methods&#34;&gt;Interpretability of machine learning methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization-and-colinear-variables&#34;&gt;Regularization and colinear variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficients-of-a-regularized-model&#34;&gt;Coefficients of a regularized model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Pricing is hard.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media2.giphy.com/media/SG0KKFtwUpqJW/giphy.gif&#34; alt=&#34;Price is Right Contestant… struggling&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Price is Right Contestant… struggling&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is particularly true with large complicated products, common in Business to Business sales (B2B). B2B sellers may lack important information (e.g. accurate estimates of customer budget or ‘street’ prices for the various configurations of their products – the &lt;a href=&#34;#pricing-challenges&#34;&gt;Pricing challenges&lt;/a&gt; section discusses other internal and external limitations in setting prices). However organizations typically &lt;em&gt;do have&lt;/em&gt; historical data on internal sales transactions as well as leadership with a strong desire for &lt;em&gt;insights&lt;/em&gt; into pricing behavior. For now I’ll put aside the question of how to use econometric approaches to set ideal prices. Instead, I’ll walk through some statistical methods that rely only on historical sales information and that can be used for analyzing differences, trends, and abnormalities in your organizations pricing.&lt;/p&gt;
&lt;p&gt;With internal data&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; you can still support answers to many important questions and provide a starting place for more sophisticated pricing strategies or analyses. I will be writing a series of posts on pricing (see &lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt; section for likely topics). In this post, I will focus on the basic ideas and considerations important when using regression models to understand prices.&lt;/p&gt;
&lt;p&gt;I will use data from the Ames, Iowa housing market. See the &lt;a href=&#34;#dataset-considerations&#34;&gt;Dataset considerations&lt;/a&gt; section for why I use the &lt;a href=&#34;https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf&#34;&gt;ames&lt;/a&gt; dataset as an analogue for B2B selling / pricing scenarios (as well as problems with this choice). My examples were built using the R programming language, you can find the source code at my &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-08-11-pricing-insights-from-historical-data-part-1.Rmd&#34;&gt;github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;what-influences-price&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What influences price?&lt;/h1&gt;
&lt;p&gt;Products have features. These features&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; can be used to train a model to estimate price. For a linear model, the outputted coefficients associated with these features can act as proxies for the expected &lt;em&gt;dollar per unit&lt;/em&gt; change associated with the component&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Ceteris_paribus&#34;&gt;ceteris paribus&lt;/a&gt;). In pricing contexts, the idea that regression coefficients relate to the value (i.e. ‘implicit price’) of the constituent components of the product is sometimes called hedonic modeling&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. An assumption in hedonic modeling is that our model includes all variables that matter to price&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. This assumption is important in that it suggests:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression modeling of price is not well suited to contexts where you cannot explain a reasonably high proportion of the variance in the price of your product.&lt;/li&gt;
&lt;li&gt;You should be particularly thoughtful regarding the variables you include in your model and avoid including variables that represent overlapping/duplicated information about your product.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a more full discussion on hedonic modeling&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; see the &lt;a href=&#34;https://www.oecd-ilibrary.org/docserver/9789264197183-7-en.pdf?expires=1597241573&amp;amp;id=id&amp;amp;accname=guest&amp;amp;checksum=0FA9E2EB249B3EB5DBA108E3AC44CCA3&#34;&gt;Handbook on Residential Property Prices Indices&lt;/a&gt;. In this post I will build very simple models that obviously don’t represent all relevant factors or meet some of the strong assumptions in hedonic modeling. Instead, my focus is on illustrating some basic considerations in regression that are particular important in pricing contexts.&lt;/p&gt;
&lt;div id=&#34;simple-linear-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear regression model&lt;/h2&gt;
&lt;p&gt;Let’s build a model for home price that uses &lt;em&gt;just&lt;/em&gt; house square footage, represented by &lt;code&gt;Gr_Liv_Area&lt;/code&gt;&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;, as a feature for predicting home price.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{Sale\_Price} = 13290 + 112(\operatorname{Gr\_Liv\_Area}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The coefficient on sale price of &lt;em&gt;112&lt;/em&gt; is a measure of expected dollars per unit change in square foot. If you build the model without an intercept&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;, the coefficient more directly equates to dollars per square foot&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. However it’s &lt;em&gt;typically&lt;/em&gt; more appropriate to &lt;a href=&#34;https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model&#34;&gt;leave the intercept in the model&lt;/a&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inference-and-challenges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference and challenges&lt;/h2&gt;
&lt;p&gt;In evaluating the impact of a component on the price, we don’t want &lt;em&gt;just&lt;/em&gt; an estimate of the magnitude of the impact. Instead we want a measure of the likely range this estimate falls within. The traditional way to compute this is by using the standard error associated with our estimate.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13289.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3269.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can do &lt;em&gt;{coefficient estimate} +/- 2&lt;/em&gt;&lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt;&lt;em&gt;{standard error of estimate}&lt;/em&gt; to get a 95% confidence interval for where we believe the ‘true’ coefficient estimate for &lt;code&gt;Gr_Liv_Area&lt;/code&gt; falls. In this case, this means that across our observations, the mean price change per square foot (while only taking into account this variables) is roughly between 108 and 116&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;violation-of-model-assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Violation of model assumptions&lt;/h3&gt;
&lt;p&gt;Linear regression has a number of &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression#Assumptions&#34;&gt;model assumptions&lt;/a&gt;. Following these is less important when using the model for predictions compared to for inference&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;. However if you are interpreting the coefficients as representations of the value associated with components of a product (as in our case), model assumptions &lt;em&gt;matter&lt;/em&gt;&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;. I will leave it up to you and Google to read more on model assumptions&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tug-of-war-between-colinear-inputs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The tug-of-war between colinear inputs&lt;/h3&gt;
&lt;p&gt;Let’s add to our regression model another variable, number of bathrooms represented by the &lt;code&gt;bathrooms&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\operatorname{Sale\_Price} = 5491 + 94(\operatorname{Gr\_Liv\_Area}) + 19555(\operatorname{bathrooms}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5491.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3356.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bathrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19555.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2284.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient on square footage has decreased – this is because number of bathrooms and square feet of home are correlated (they have a &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&#34;&gt;correlation&lt;/a&gt; of 0.71). Some of the impact on home price that previously existed entirely in the coefficient for &lt;code&gt;Gr_Liv_Area&lt;/code&gt; is now shared with the related &lt;code&gt;bathrooms&lt;/code&gt; variable. Also, the standard error on &lt;code&gt;Gr_Liv_Area&lt;/code&gt; has increased – representing greater uncertainty as to the mean impact of the variable within the model (compared to the prior simple linear regression example).&lt;/p&gt;
&lt;p&gt;Let’s consider a model with another variable added: &lt;code&gt;TotRms_AbvGrd&lt;/code&gt;, the total number of rooms (above ground and excluding bathrooms) in the home. This variable is also correlated with &lt;code&gt;Gr_Liv_Area&lt;/code&gt; and number of &lt;code&gt;bathrooms&lt;/code&gt; (correlation of ~0.8 and ~0.6 respectively).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\operatorname{Sale\_Price} &amp;amp;= 35600 + 122(\operatorname{Gr\_Liv\_Area})\ + \\
&amp;amp;\quad 20411(\operatorname{bathrooms}) - 11389(\operatorname{TotRms\_AbvGrd})\ + \\
&amp;amp;\quad \epsilon
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35600.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4384.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gr_Liv_Area&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;121.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bathrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20410.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2245.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TotRms_AbvGrd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-11389.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1093.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice the coefficient on &lt;code&gt;TotRms_AbvGrd&lt;/code&gt; is negative at &lt;em&gt;-11792.2&lt;/em&gt;. This doesn’t mean houses with more bedrooms are associated with negative home prices. Though it suggests a house with the same square footage and number of bathrooms will be less expensive if it has more rooms&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theoretical challenge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pretend we put in another variable: &lt;code&gt;half_bathrooms&lt;/code&gt; that represented the number of half bathrooms in the home. Our previous variable &lt;code&gt;bathrooms&lt;/code&gt; already included both full and half bathrooms. This presents a theoretical problem for the model: bathrooms would be represented in two different variables that have a &lt;em&gt;necessary&lt;/em&gt; overlap&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt; with one another. Our understanding of the value of a bathroom as its coefficient value would become less clear&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Beyond this &lt;em&gt;theoretical challenge&lt;/em&gt;, duplicated or highly associated inputs also create &lt;em&gt;numeric challenges&lt;/em&gt;. The remainder of this post will be focused on numeric challenges and considerations in fitting regression models. These lessons can be applied broadly across inferential contexts but are particularly important in pricing analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numeric challenge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linear regression models feature this ‘tug-of-war’ between the magnitude of coefficients whereby correlated variables share general influences in the model. At times this causes similar variables to seem to have opposing impacts&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;. When evaluating coefficients for pricing analysis exercises&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt; this competition between coefficients has potential drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As you increase the number of variables in the model, colinearity can make for models with a high degree of instability / variance in the parameter estimates – meaning that the coefficients in your model (and your resulting predictions) could change dramatically even from small changes in the training data&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;, which undermines confidence in estimates.&lt;/li&gt;
&lt;li&gt;You may want to limit methods that result in models with unintuitive variable relationships (e.g. where related factors have coefficients that appear to act in opposing directions).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-model-fit-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improving model fit, considerations&lt;/h2&gt;
&lt;p&gt;I do not discuss the topic of &lt;em&gt;variable selection&lt;/em&gt;, but highly recommend the associated chapter in the online textbook &lt;a href=&#34;http://www.feat.engineering/selection.html&#34;&gt;Feature Engineering and Selection&lt;/a&gt; by Max Kuhn and Kjell Johnson.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data transformations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before modeling, transformations to the underlying data are often applied for one of several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To help satisfy model assumptions or to minimize the impact of outliers and influential points on estimates.&lt;/li&gt;
&lt;li&gt;To improve the fit of the model.&lt;/li&gt;
&lt;li&gt;To help with model interpretation&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To facilitate preprocessing requirements important to the fitting procedure&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Important in pricing contexts, transformations to the data alter the meaning of the coefficients&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt;. Data transformations may improve model fit, but may complicate coefficient interpretability. In some cases this may be helpful in other cases it may not – it all depends on the aims of the model and the types of interpretations the analyst is hoping to make&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt;. As part of an internal presentation given at NetApp on pricing, I describe some common variable transformations and how these affect the resulting interpretation of the coefficients:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/dqrkIIziBLE?start=448&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;&lt;strong&gt;More sophisticated Machine Learning Methods:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When using more sophisticated machine learning techniques the term &lt;em&gt;data transformation&lt;/em&gt; is sometimes supplanted by the term &lt;em&gt;feature engineering&lt;/em&gt; (though the latter typically suggests more numerous or more complicated changes to input data). Some machine learning techniques&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; can also identify hard to find relationships or obviate the need for complex data transformations that would be required to produce good model fits with a linear model&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;. This may save an analyst time or allow them to produce models with a better fit but may come at a cost to ease of model interpretability. For a brief discussion, see the section &lt;a href=&#34;#interpretability-of-machine-learning-methods&#34;&gt;Interpretability of machine learning methods&lt;/a&gt;. For this post, I will stick to linear models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitting procedures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatives to the standard optimization technique for linear regression, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares#:~:text=In%20statistics%2C%20ordinary%20least%20squares,in%20a%20linear%20regression%20model.&amp;amp;text=Under%20these%20conditions%2C%20the%20method,the%20errors%20have%20finite%20variances&#34;&gt;Ordinary Least Squares&lt;/a&gt; (OLS), may be more robust to model assumptions and influential points or tend to produce more stable estimates&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;. A few options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Regularization&lt;/em&gt;: puts constraints on the linear model that discourage high levels of variance in your coefficient estimates. See the section &lt;a href=&#34;#regularization-and-colinear-variables&#34;&gt;Regularization and colinear variables&lt;/a&gt; for a more full discussion on how L1 &amp;amp; L2 penalties affect estimates for colinear inputs differently&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bayesian approaches&lt;/em&gt;: can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34;&gt;priors&lt;/a&gt; and rigorous estimation procedures to limit &lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34;&gt;overfitting&lt;/a&gt; and subdue extreme estimates.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Robust regression&lt;/em&gt;: typically refers to using weighted least squares (or similar methods) which allows for giving different amounts of weight to observations (typically to reduce the weight of extreme and influential points).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these fitting procedures has different advantages and disadvantages and will modulate coefficient estimates differently&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-notes-and-tips&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Closing notes and tips&lt;/h1&gt;
&lt;p&gt;You can use regression models to evaluate the impact of different factors on price. However it is important to consider how coefficient estimates will respond to your particular input data (e.g. multicolinearity of your inputs or violations of your model assumptions) and to use techniques that will produce an appropriate model fit for your needs. In pricing contexts in particular you should consider the types of inferences you will be asked to make and build your model in a way that fits your business requirements.&lt;/p&gt;
&lt;p&gt;Some tips for building models for inference in pricing contexts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your model doesn’t explain a high proportion of the data, be careful what you say to stakeholders about the respective value of components&lt;a href=&#34;#fn30&#34; class=&#34;footnote-ref&#34; id=&#34;fnref30&#34;&gt;&lt;sup&gt;30&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Getting a good model fit should be a driving force. However, in a similar way to how you may prefer fewer variables or a more simple modeling technique, you may also prefer fewer and less complicated variable transformations&lt;a href=&#34;#fn31&#34; class=&#34;footnote-ref&#34; id=&#34;fnref31&#34;&gt;&lt;sup&gt;31&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;When evaluating the influence of the components of your product, review the variability in your coefficient estimates and not just the estimates themselves.&lt;/li&gt;
&lt;li&gt;Consider building linear models using multiple model fit techniques&lt;a href=&#34;#fn32&#34; class=&#34;footnote-ref&#34; id=&#34;fnref32&#34;&gt;&lt;sup&gt;32&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn33&#34; class=&#34;footnote-ref&#34; id=&#34;fnref33&#34;&gt;&lt;sup&gt;33&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Even if you plan on using a linear model, using a generic more complex machine learning model can be helpful as a sanity check. If model performance is not substantially different between your models, you are fine, if it is, there may be an important relationship you are missing and need to identify.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for &lt;a href=&#34;#future-pricing-posts&#34;&gt;Future pricing posts&lt;/a&gt; on related topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;pricing-challenges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pricing challenges&lt;/h2&gt;
&lt;p&gt;Final price paid by a customer may vary substantially within a given product. This variability is often due in part to a high degree of complexity inherent in the product and different configurations between customers&lt;a href=&#34;#fn34&#34; class=&#34;footnote-ref&#34; id=&#34;fnref34&#34;&gt;&lt;sup&gt;34&lt;/sup&gt;&lt;/a&gt;. Fluctuations in product demand and macroeconomic factors are other important influences, as are factors associated with the buyer’s / seller’s negotiation skill and ability to use market information to leverage a higher or lower discount.&lt;/p&gt;
&lt;p&gt;The final price paid may also be influenced by a myriad of competing internal interests. Sales representatives may want leniency in price guidelines so they can hit their quota. Leadership may be concerned about potential brand erosion that often comes with lowering prices. Equity holders may be focused on immediate profitability or may be willing to sacrifice margin in order to expand market share. Effectively setting price guidelines requires the application of various economic, mathematical, and sociological principles&lt;a href=&#34;#fn35&#34; class=&#34;footnote-ref&#34; id=&#34;fnref35&#34;&gt;&lt;sup&gt;35&lt;/sup&gt;&lt;/a&gt; which may not be feasible to set-up&lt;a href=&#34;#fn36&#34; class=&#34;footnote-ref&#34; id=&#34;fnref36&#34;&gt;&lt;sup&gt;36&lt;/sup&gt;&lt;/a&gt;. Implementation of which requires reliable data, which could be lacking due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Market information may be inaccurate or unavailable&lt;a href=&#34;#fn37&#34; class=&#34;footnote-ref&#34; id=&#34;fnref37&#34;&gt;&lt;sup&gt;37&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Total&lt;/em&gt; costs of production may not be accessible (from your position in the organization).&lt;/li&gt;
&lt;li&gt;Current organizational goals may not be well defined.&lt;/li&gt;
&lt;li&gt;Information on successful deals may be more reliable than information on missed deals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These (or a host of other gaps in information) may make it difficult to define an objective function for identifying optimal price guidelines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-pricing-posts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Future pricing posts&lt;/h2&gt;
&lt;p&gt;In a series of posts I will tackle a variety of questions stakeholders may ask regarding organizational pricing. Some likely topics include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How do differences in product components associate with differences in price? What is the magnitude of the influence of these factors?&lt;/li&gt;
&lt;li&gt;How have these factors changed over time?&lt;/li&gt;
&lt;li&gt;Which customers fall outside the ‘normal’ behavior in regard to the price they are receiving?&lt;/li&gt;
&lt;li&gt;How can complexities in pricing strategy be captured by a statistically rigorous modeling framework (E.g. when volume dictates price)?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;dataset-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dataset considerations&lt;/h2&gt;
&lt;p&gt;The relevant qualities of a dataset I was looking for were:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Multiple years of data&lt;/li&gt;
&lt;li&gt;Many features, with a few key variables associated with a large proportion of the variance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;ames&lt;/code&gt; housing dataset meets these qualifications and i was already familiar with it. Evaluating home prices can serve as a practical analogue for our problem; both home sales and business to business sales often represent large purchases with many features influencing price. You can pretend that individual rows represent B2B transactions for a large corporation selling a complicated product line (rather than individual home sales).&lt;/p&gt;
&lt;p&gt;There are also many important &lt;em&gt;differences&lt;/em&gt; between home sales and B2B sales that make this a weaker analogue. To name a few:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in B2B contexts, repeat sales are typically more important than initial sales. In the housing market, repeat sales don’t exist.&lt;/li&gt;
&lt;li&gt;information on home prices and prior home sales is accessible to both the buyer and seller – meaning there are no options for targeted pricing.&lt;/li&gt;
&lt;li&gt;in B2B contexts, an influential buyer may be able to leverage the possibility of a partnership of some kind in order to secure a better deal on a large purchase&lt;a href=&#34;#fn38&#34; class=&#34;footnote-ref&#34; id=&#34;fnref38&#34;&gt;&lt;sup&gt;38&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Volume selling schemes and other pricing strategies may have less of an impact on house prices compared to in B2B settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the notes in this first post, these don’t matter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretability-of-machine-learning-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretability of machine learning methods&lt;/h2&gt;
&lt;p&gt;In some pricing scenarios tree-based methods may be particularly helpful in modeling price – particularly in contexts where the price of a product can be well defined by if-then statements. This may be useful in cases where there is volume pricing – e.g. the pricing approach is different depending on the amount you are purchasing. Perhaps better though would be cubist models which start as decision trees but then terminate into individual linear models (allowing for different linear models based off pre-defined if-then statements).&lt;/p&gt;
&lt;p&gt;(Ignoring figuring out the &lt;em&gt;ideal&lt;/em&gt; type of model or feature engineering regiment&lt;a href=&#34;#fn39&#34; class=&#34;footnote-ref&#34; id=&#34;fnref39&#34;&gt;&lt;sup&gt;39&lt;/sup&gt;&lt;/a&gt; for your problem) the typical juxtaposition between linear models and more sophisticated machine learning techniques is in how easy they are to interpret. Sophisticated machine learning methods (sometimes described as ‘black-boxes’&lt;a href=&#34;#fn40&#34; class=&#34;footnote-ref&#34; id=&#34;fnref40&#34;&gt;&lt;sup&gt;40&lt;/sup&gt;&lt;/a&gt;) &lt;em&gt;can&lt;/em&gt; be made to be interpretable. Interpretation typically involves some approach that evaluates how the predictions change in relation to some change in the underlying data. This &lt;em&gt;prediction focused&lt;/em&gt; way of interpreting a model has the advantage of being more standard across model types. The argument goes that regardless of the structure of the model, you always get predictions, hence you should use these predictions to drive your interpretations of the model. This enables you to compare models (across things other than just raw performance) regardless of the type of model you use.&lt;/p&gt;
&lt;p&gt;The advantage linear models have is that the &lt;em&gt;model form itself&lt;/em&gt; is highly interpretable. Unlike other models the parameters of linear models are directly aggregatable. With a linear model you can more easily say how much value a component of a product adds to the price. With other types of models this translation is usually more difficult.&lt;/p&gt;
&lt;p&gt;Linear models can be understood by a wider audience and also may be viewed as more logical or fair&lt;a href=&#34;#fn41&#34; class=&#34;footnote-ref&#34; id=&#34;fnref41&#34;&gt;&lt;sup&gt;41&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn42&#34; class=&#34;footnote-ref&#34; id=&#34;fnref42&#34;&gt;&lt;sup&gt;42&lt;/sup&gt;&lt;/a&gt;. However, if you build a linear model with highly complicated transformations, interactions, or non-linear terms, notions of this ‘interpretability advantage’ start to deteriorate&lt;a href=&#34;#fn43&#34; class=&#34;footnote-ref&#34; id=&#34;fnref43&#34;&gt;&lt;sup&gt;43&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In summary, the breakdown of linear regression vs complicated machine learning models may be similar in pricing contexts as it is in other problem spaces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you only care about accuracy of your predictions (i.e. pricing estimates) or want to save time on complex feature engineering more sophisticated machine learning techniques may be valuable.&lt;/li&gt;
&lt;li&gt;If you care about interpretability or have audit requirements regarding prices, linear models have a certain advantages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;regularization-and-colinear-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regularization and colinear variables&lt;/h2&gt;
&lt;p&gt;Regularization typically comes in two flavors; either an L1 penalty (lasso regression) or L2 penalty (ridge regression), or some combination of these (elastic net) is applied to the linear model. These penalties provides a cost for larger coefficient which acts to decrease the variance in our estimates&lt;a href=&#34;#fn44&#34; class=&#34;footnote-ref&#34; id=&#34;fnref44&#34;&gt;&lt;sup&gt;44&lt;/sup&gt;&lt;/a&gt;. In conditions of colinear inputs, these two penalties act differently on coefficient estimates of colinear features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lasso regression tends to choose a ‘best’ variable (among a subset of colinear variables) whose coefficient ‘survives’, while the other associated variables’ coefficients are pushed towards zero&lt;/li&gt;
&lt;li&gt;For ridge regression, coefficients of similar variables gravitate to a similar value&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficients-of-a-regularized-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficients of a regularized model&lt;/h2&gt;
&lt;p&gt;Variable inputs are usually standardized before applying regularization. Hence because inputs are all (essentially) put on the same scale, the coefficient estimates can be directly compared with one another as measures of their relative influence on the target (home price). This ease of comparison may be convenient. However if our goal is interpreting the coefficient estimates in terms of dollar change per &lt;em&gt;unit&lt;/em&gt; increase, we will need to back-transform the coefficients.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Internal sales data alone is limited in that its focused on only a component of sales, rather than considering the full picture – this puts the analyst in a familiar position of one with incomplete information, and a constrained scope of influence.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Dataset should be structured such that each feature is a column and each row an observation, e.g. a sale.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Sort of, and under certain contexts…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hedonic_regression&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Hedonic_regression&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Missing important components or misattributing influence of price can cause bias in the model (omitted variable bias).&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;and how it can also be used for things like creating price indices&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Does not including basement.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;I.e. make it zero so that the expected value of a house of 0 square foot is $0&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;In this case, the coefficient for the model becomes 119.7 if the intercept is set to zero.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;Hedonic modeling also has a variety of approaches associated with evaluating changes in the intercept term between models that (again) can be read in the the &lt;a href=&#34;https://www.oecd-ilibrary.org/docserver/9789264197183-7-en.pdf?expires=1597241573&amp;amp;id=id&amp;amp;accname=guest&amp;amp;checksum=0FA9E2EB249B3EB5DBA108E3AC44CCA3&#34;&gt;Handbook on Residential Property Prices Indices&lt;/a&gt;.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Note there are more modern approaches to estimating this range using Bayesian or simulation based methods.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;At least to the extent that satisfying them doesn’t improve your predictions, or suggest a different model type may be more appropriate.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Although some would argue that you don’t need to worry too much about any of your assumptions except that your observations are independent of one another.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;Model assumptions of linear regression by Ordinary Least Squares is already covered extensively in essentially every tutorial and Introduction to Statistics textbook on regression.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;Perhaps representing a preference for larger rooms or open space among buyers or a confounding effect with some other variable. For the purposes of this post i simply want to point out how coefficient values can vary under conditions of colinearity.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;(though not perfect colinearity)&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;Hence the importance of being particularly thoughtful of the variables you input into the model and avoiding variables that roughly duplicate one another.&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;A common rule of thumb for when variables are ‘too correlated’ is 0.90 – at least in regression contexts and cases where you are focused on inference. In other contexts (e.g. those that appear in &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; prediction competitions) this threshold can be much higher. However, as discussed, lower levels of correlation can still contribute to instability in your coefficient estimates&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Where you care about the individual parameter estimates and want them to be meaningful.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;This is what “variance” means in the bias-variance trade-off common in model development. This may also be referred to as instability in the model or parameter estimates.&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;An example of this may be standardizing the underlying data so that the coefficient estimates may be more directly compared to one another (as the underlying data is all on the same scale).&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Standardizing the data is also important for many fitting methods, e.g. regularization.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;E.g. a log transform on an input alters the interpretation of the coefficient to be something closer to dollars per percentage point change of input. Log on target means percentage change in price per unit change in input. If you take the log of both your inputs and your target, the coefficient represents percent change in x related to percent change in y, also known as an ‘elasticity’ model in econometrics.&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;There may be a preference to speak in the simplest terms: change in price as a function of unit change in component – which may put pressure on the analyst to limit data transformations. It is the analysts job then to strike the correct balance between producing a model that fits the data and one that can be understood by stakeholders.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;Neural networks in particular.&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;Many non-linear methods still have sophisticated preprocessing requirements. Though these are sometimes more generic – meaning less work to customize between problems to reach at least some minimum level of fit between problems (again, in some contexts).&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;Some of what I read on hedonic modeling seemed to discourage the use of methods other than Ordinary Least Squares (e.g. Weighted Least Squares) but I’ve found other methods to be helpful.&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Regularization with an L1 penalty provides the added bonus of also doing variable selection.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;For robust methods and regularization, there are less established methods for producing confidence intervals. You may need to use simulation methods (which are more computationally intensive).&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn30&#34;&gt;&lt;p&gt;Generally it is a good idea to describe the mean error or some other measure, so that they can get a sense of how close the model you are describing is fitting the data, or whether the effects you are talking about are general, but not particularly useful for predictions.&lt;a href=&#34;#fnref30&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn31&#34;&gt;&lt;p&gt;And a preference for transformations that retain an intuitive interpretability for the model.&lt;a href=&#34;#fnref31&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn32&#34;&gt;&lt;p&gt;Then review the coefficient estimates across them.&lt;a href=&#34;#fnref32&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn33&#34;&gt;&lt;p&gt;I tend to rely heavily on regularization techniques.&lt;a href=&#34;#fnref33&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn34&#34;&gt;&lt;p&gt;A variety of factors though push organizations to simplify their products and this process – for the purposes of this post though, I’ll assume a complicated product portfolio.&lt;a href=&#34;#fnref34&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn35&#34;&gt;&lt;p&gt;For a more full discussion on these concepts see UVA coursera specialization on &lt;a href=&#34;https://www.coursera.org/learn/uva-darden-bcg-pricing-strategy-cost-economics?utm_source=gg&amp;amp;utm_medium=sem&amp;amp;utm_content=01-CourseraCatalog-DSA-US&amp;amp;campaignid=9918777773&amp;amp;adgroupid=102058276958&amp;amp;device=c&amp;amp;keyword=&amp;amp;matchtype=b&amp;amp;network=g&amp;amp;devicemodel=&amp;amp;adpostion=&amp;amp;creativeid=434544785640&amp;amp;hide_mobile_promo=&amp;amp;gclid=CjwKCAjwsan5BRAOEiwALzomXyDwos6rlUmAwFrv9BjJFUPnyvzPRedArpRD2iRkocMemgtsZrfihxoCjfUQAvD_BwE&#34;&gt;Cost and Economics in Pricing Strategy&lt;/a&gt;.&lt;a href=&#34;#fnref35&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn36&#34;&gt;&lt;p&gt;Organizations may lack the money or the will.&lt;a href=&#34;#fnref36&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn37&#34;&gt;&lt;p&gt;Maybe your company doesn’t want to pay the expensive prices that data vendors set for this information (this may especially be a problem if you are a small organization with a small budget).&lt;a href=&#34;#fnref37&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn38&#34;&gt;&lt;p&gt;While a home seller may be more sympathetic to some buyers over others (E.g. a newly wedded couple looking to start a family over a real-estate mogul looking for investment properties), such preferences likely impact price less than the analogue in the B2B contexts where sellers seek to strike details with popular brands as means of establishing product relevance and enabling further marketing and potentially collaboration opportunities. It is important to note though that the ‘Clayton Act’ and ‘Robinson Patman Act’ make price discrimination in B2B contexts illegal (except in certain circumstances).&lt;a href=&#34;#fnref38&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn39&#34;&gt;&lt;p&gt;PCA or factor analysis seems like a potentially useful approach in pricing contexts in cases where the variables you have do not clearly represent discrete components of the product – hopefully PCA would help to identify these implicit components.&lt;a href=&#34;#fnref39&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn40&#34;&gt;&lt;p&gt;Due to the lack of transparency into how they produce predictions.&lt;a href=&#34;#fnref40&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn41&#34;&gt;&lt;p&gt;Or at least in places where a price seems unfair, it may be easier to quickly identify where the issue lies.&lt;a href=&#34;#fnref41&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn42&#34;&gt;&lt;p&gt;There is a book &lt;em&gt;Weapons of Math Destruction&lt;/em&gt; by Cathy O’Neil that points to a lack of interpretability as one of the chief concerns with modern learning algorithms.&lt;a href=&#34;#fnref42&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn43&#34;&gt;&lt;p&gt;May as well use a Machine Learning method at this point.&lt;a href=&#34;#fnref43&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn44&#34;&gt;&lt;p&gt;In both cases non-informative regressors will tend towards zero (in the case of ridge regression, they will never &lt;em&gt;quite&lt;/em&gt; reach zero). These approaches typically require tuning to identify the ideal weight (i.e. pressure) assigned to the penalty.&lt;a href=&#34;#fnref44&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Short Examples of Best Practices When Writing Functions That Call dplyr Verbs</title>
      <link>/2020/06/25/using-across-to-build-functions-with-dplyr-with-notes-on-legacy-approaches/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/25/using-across-to-build-functions-with-dplyr-with-notes-on-legacy-approaches/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#function-expecting-one-column&#34;&gt;Function expecting one column&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions-allowing-multiple-columns&#34;&gt;Functions allowing multiple columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#older-approaches&#34;&gt;Older approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tidyverse/dplyr&#34;&gt;dplyr&lt;/a&gt;, the foundational &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; package, makes a trade-off between being easy to code in interactively at the expense of being more difficult to create functions with. The source of the trade-off is in how &lt;code&gt;dplyr&lt;/code&gt; evaluates column names (specifically, allowing for unquoted column names as argument inputs). Tidy evaluation has been under major development the last couple of years in order to make &lt;a href=&#34;https://dplyr.tidyverse.org/articles/programming.html&#34;&gt;programming with dplyr&lt;/a&gt; easier.&lt;/p&gt;
&lt;p&gt;During this development, there have been a variety of proposed methods for programming with &lt;code&gt;dplyr&lt;/code&gt;. In this post, I will document the current ‘best-practices’ with &lt;code&gt;dplyr&lt;/code&gt; 1.0.0. In the &lt;a href=&#34;#older-approaches&#34;&gt;Older approaches&lt;/a&gt; section I provide analogous examples that someone (i.e. myself) might have used during this maturation period.&lt;/p&gt;
&lt;p&gt;For a more full discussion on this topic see &lt;code&gt;dplyr&lt;/code&gt;’s documentation at &lt;a href=&#34;https://dplyr.tidyverse.org/articles/programming.html&#34;&gt;programming with dplyr&lt;/a&gt; and the various links referenced there.&lt;/p&gt;
&lt;div id=&#34;function-expecting-one-column&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Function expecting one column&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretend we want to create a function that calculates the sum of a given variable in a dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  
  summarise(df, {{var}} := sum({{var}}))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, cty)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to edit the variable in place and avoid using the special assignment operator &lt;code&gt;:=&lt;/code&gt;, you could use the new (in &lt;code&gt;dplyr&lt;/code&gt; 1.0.0) &lt;code&gt;across()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise(df, across({{vars}}, sum))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions-allowing-multiple-columns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functions allowing multiple columns&lt;/h1&gt;
&lt;p&gt;Using the &lt;code&gt;across()&lt;/code&gt; approach also allows you to input more than one variable, e.g. a user could call the following to get summaries on both &lt;code&gt;cty&lt;/code&gt; and &lt;code&gt;hwy&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, c(cty, hwy))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to compute multiple column summaries with different functions and you wanted to glue the function name onto your outputted column names&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, you could instead pass a named list of functions into the &lt;code&gt;.fns&lt;/code&gt; argument of &lt;code&gt;across()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise(df, across({{vars}}, list(sum = sum, mean = mean)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might want to create a function that can take in multiple sets of columns, e.g. the function below allows you to &lt;code&gt;group_by()&lt;/code&gt; one set of variables and &lt;code&gt;summarise()&lt;/code&gt; another set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_vars &amp;lt;- function(df, group_vars, sum_vars){
  df %&amp;gt;% 
    group_by(across({{group_vars}})) %&amp;gt;% 
    summarise(across({{sum_vars}}, list(sum = sum, mean = mean)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How a user would run &lt;code&gt;sum_group_vars()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_vars(mpg,
               c(model, year), 
               c(hwy, cty))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re feeling fancy, you could also make the input to &lt;code&gt;.fns&lt;/code&gt; an argument to &lt;code&gt;sum_group_vars()&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;older-approaches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Older approaches&lt;/h1&gt;
&lt;p&gt;Generally, I find the new &lt;code&gt;across()&lt;/code&gt; approaches introduced in &lt;code&gt;dplyr&lt;/code&gt; 1.0.0 are easier and more consistent to use than the methods that preceded them. However the methods in this section still work and are supported. They are just no longer the ‘recommended’ or most ‘modern’ approach available for creating functions that pass column names into &lt;code&gt;dplyr&lt;/code&gt; verbs.&lt;/p&gt;
&lt;p&gt;Prior to the introduction of the &lt;em&gt;bracket-bracket&lt;/em&gt;, &lt;code&gt;{{}}&lt;/code&gt;, you would have used the &lt;em&gt;&lt;code&gt;enquo()&lt;/code&gt; + bang-bang&lt;/em&gt; approach&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. The function below is equivalent to the &lt;code&gt;sum_var()&lt;/code&gt; function shown at the start of this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  var_quo &amp;lt;- enquo(var)
  summarise(df, !!var_quo := sum(!!var_quo))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To modify variables in-place you would have used the &lt;code&gt;*_at()&lt;/code&gt;, &lt;code&gt;*_if()&lt;/code&gt; or &lt;code&gt;*_all()&lt;/code&gt; function variants (which are now superseded by &lt;code&gt;across()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars &amp;lt;- function(df, vars){
  
  summarise_at(df, {{vars}}, sum)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to using &lt;code&gt;across()&lt;/code&gt; this method allows multiple variables being input. However what is weird about this function is that it requires the user wrapping the variable names in &lt;code&gt;vars()&lt;/code&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Hence to use the previously created function, a user would run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_vars(mpg, vars(hwy, cty))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you could have the variable name inputs be character vectors by modifying the function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, vars){
  
  summarise_at(df, vars(one_of(vars)), sum)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which could be called by a user as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var(mpg, c(&amp;quot;hwy&amp;quot;, &amp;quot;cty&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These &lt;code&gt;*_at()&lt;/code&gt; variants also support inputting a list of functions, e.g. the below function would output both the sums and means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var &amp;lt;- function(df, var){
  
  summarise_at(df, vars(one_of(var)), list(sum = sum, mean = mean))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For multiple grouping variables and multiple variables to be summarised you could create:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groupsum &amp;lt;- function(df, group_vars, sum_vars){
  df %&amp;gt;% 
    group_by_at(vars(one_of(group_vars))) %&amp;gt;% 
    summarise_at(vars(one_of(sum_vars)), list(sum = sum, mean = mean))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which would be called by a user:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_var(mpg, 
        c(&amp;quot;model&amp;quot;, &amp;quot;year&amp;quot;), 
        c(&amp;quot;hwy&amp;quot;, &amp;quot;cty&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a variety of similar spins you might take on handling tidy evaluation when creating these or similar types of functions.&lt;/p&gt;
&lt;p&gt;One other older approach perhaps worth mentioning (presented &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2019/working-with-names-and-expressions-in-your-tidy-eval-code/&#34;&gt;here&lt;/a&gt;) is “passing the dots”. Here is an example for if we want to &lt;code&gt;group_by()&lt;/code&gt; multiple columns and then &lt;code&gt;summarise()&lt;/code&gt; on just one column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_group_var &amp;lt;- function(df, sum_var, ...){
  df %&amp;gt;% 
    group_by(...) %&amp;gt;% 
    summarise({{sum_var}} := sum({{sum_var}}))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The limitation with this approach is that only one set of your inputs can have more than one variable in it, i.e. wherever you pass the &lt;code&gt;...&lt;/code&gt; in your function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;Image shared on social media was created using &lt;code&gt;xaringan&lt;/code&gt; and &lt;code&gt;flair&lt;/code&gt;. See &lt;a href=&#34;https://github.com/brshallo/dplyr-1.0.0-example&#34;&gt;dplyr-1.0.0-example&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/brshallo/dplyr-1.0.0-example/blob/master/dplyr-example-cropped.png?raw=true&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; 1.0.0 also now has &lt;a href=&#34;https://www.tidyverse.org/blog/2020/02/glue-strings-and-tidy-eval/&#34;&gt;support for using the glue&lt;/a&gt; package syntax for modifying variable names.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Doing this doesn’t require any tidy evaluation knowledge&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;There is also the &lt;code&gt;rlang::enquos()&lt;/code&gt; and &lt;code&gt;!!!&lt;/code&gt; operator for when the input has length greater than one.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;A niche function specific to tidy evaluation (which users might not think of).&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Use Flipbooks to Explain Your Code and Thought Process</title>
      <link>/2020/06/24/use-flipbooks-to-explain-your-code-and-thought-process/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/24/use-flipbooks-to-explain-your-code-and-thought-process/</guid>
      <description>


&lt;div id=&#34;learning-rs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Learning R’s &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Using the pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) is one of my favorite things about coding in R and the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;. However when it was first shown to me, I couldn’t understand what the &lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=typed_query&#34;&gt;#rstats&lt;/a&gt; nut describing it was &lt;em&gt;so enthusiastic&lt;/em&gt; about. They tried to explain, “It means &lt;em&gt;and then&lt;/em&gt; do the next operation.” When that didn’t click for me, they continued (while becoming ever more excited) “It &lt;em&gt;passes the previous steps output into the first argument&lt;/em&gt; of the next function,” still… 😐😐😕.
Self-evident verbs in their code like &lt;code&gt;select()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;summarise()&lt;/code&gt; helped me nod along, partly following the operations. Though it wasn’t until I evaluated the code &lt;em&gt;line-by-line&lt;/em&gt; that I recognized the pipe’s elegance, power, beauty, simplicity 😄!&lt;/p&gt;
&lt;p&gt;Now, a few years and reads through &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt; later&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I will often share my work by keeping the code and output together and showing, line-by-line, what I am building towards. For example when…&lt;/p&gt;
&lt;p&gt;… giving a 2019 talk on &lt;em&gt;“Managing objects in analytics workflows, using lists as columns in dataframes”&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/gme4Fb9JVjk?start=258&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;… giving a 2017 talk on &lt;em&gt;“Getting started with ‘tidy’ data science in R”&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/eeCELJNWEuw?start=474&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;… promoting a recent blog post on &lt;em&gt;“Tidy pairwise operations”&lt;/em&gt; (though in this case I removed the code):&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
What is your &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; (or other &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; ) approach for doing arbitrary pairwise operations across variables? Mine is frequently something like:&lt;br&gt;&lt;br&gt;I. nest…&lt;br&gt;II. expand combos… &lt;br&gt;III. filter…&lt;br&gt;IV. map fun(s)…&lt;br&gt;…&lt;br&gt;&lt;br&gt;I wrote a post walking through this: &lt;a href=&#34;https://t.co/xRnRf5yh3m&#34;&gt;https://t.co/xRnRf5yh3m&lt;/a&gt; &lt;a href=&#34;https://t.co/Zvxey2gm3H&#34;&gt;pic.twitter.com/Zvxey2gm3H&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1271194908477591553?ref_src=twsrc%5Etfw&#34;&gt;June 11, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;p&gt;However each of these examples were built using PowerPoint (and a lot of copy and pasting of code + output). The series of images cannot be easily reproduced. In this post I’ll point to resources on how to create these sorts of code communication materials in ways that &lt;em&gt;are reproducible&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flipbooks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Flipbooks&lt;/h1&gt;
&lt;p&gt;When I started writing this post, I planned to call this type of output a “&lt;strong&gt;LEXPREX&lt;/strong&gt;” for “&lt;strong&gt;L&lt;/strong&gt;ine-by-line &lt;strong&gt;EX&lt;/strong&gt;ecution with &lt;strong&gt;PR&lt;/strong&gt;inted &lt;strong&gt;EX&lt;/strong&gt;amples” (and a name evocative of the inspiring &lt;a href=&#34;https://github.com/tidyverse/reprex&#34;&gt;reprex&lt;/a&gt; package by &lt;a href=&#34;https://twitter.com/JennyBryan%5D&#34;&gt;Jenny Bryan&lt;/a&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;). But, thankfully, an excellent solution containing thorough explanations (and a much better name) already existed, &lt;em&gt;flipbooks&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As described in the &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;flipbookr documentation&lt;/a&gt;, “flipbooks are tools that present side-by-side, aligned, incremental code-output.”&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/brshallo/flipbookr-gifs-examples/raw/master/example-r4ds.gif?raw=true&#34; alt=&#34;(Example inspired by ‘Many Models’ chapter of ‘R For Data Science’ by Grolemund &amp;amp; Wickham.)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;(Example inspired by ‘Many Models’ chapter of ‘R For Data Science’ by Grolemund &amp;amp; Wickham.)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At this point you should stop reading this blog and instead go learn about &lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr&#34;&gt;flipbookr&lt;/a&gt;. My post was largely written before I learned about this package. Hence, starting at &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2020/flipbooks-evangeline-reynolds/&#34;&gt;this presentation&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/EvaMaeRey&#34;&gt;Gina Reynolds&lt;/a&gt; or &lt;code&gt;flipbookr&lt;/code&gt;’s &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;about page&lt;/a&gt; will generally be a more productive use of your time. The remainder of this post discusses either tools adjacent to flipbooks or describes workflows that can also be found within &lt;code&gt;flipbookr&lt;/code&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-with-xaringan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example with xaringan&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt; package for making slideshows contains highlighting features (and is what &lt;code&gt;flipbookr&lt;/code&gt; is built-on). For highlighting &lt;em&gt;code&lt;/em&gt; you can use the trailing comment &lt;code&gt;#&amp;lt;&amp;lt;&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. For highlighting &lt;em&gt;output&lt;/em&gt; there is the &lt;code&gt;highlight.output&lt;/code&gt; code chunk option.&lt;/p&gt;
&lt;blockquote&gt;
&lt;iframe src=&#34;https://slides.yihui.org/xaringan/#31&#34; style=&#34;width: 560px; height: 315px;&#34;&gt;
&lt;/iframe&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/mitchoharawild&#34;&gt;Mitchell O’Hara-Wild&lt;/a&gt;’s 2019 presentation on &lt;em&gt;“Flexible futures for &lt;a href=&#34;https://github.com/tidyverts/fable&#34;&gt;fable&lt;/a&gt; functionality”&lt;/em&gt; contains a helpful example where he uses these features to walk-through &lt;a href=&#34;https://github.com/mitchelloharawild/fable-combinations-2019/blob/6a55628e1ad156c0040676b7881a799f7f75370a/user2019/index.Rmd&#34;&gt;his code&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/DhDOTxojQ3k?start=554&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;See &lt;a href=&#34;#more-sophisticated-highlighting&#34;&gt;More sophisticated highlighting&lt;/a&gt; if your use-case requires more than line-level highlighting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animating-a-flipbook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Animating a flipbook&lt;/h1&gt;
&lt;p&gt;I sometimes want to convert a flipbook into a gif, e.g. when sharing an example in a README or a snippet of a concept on social media. If you ignored my prior entreaty, this is a second reminder to stop and go read about &lt;code&gt;flipbookr&lt;/code&gt;. The &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;template file&lt;/a&gt; now shows how to create gifs using &lt;code&gt;flipbookr&lt;/code&gt; (html) –&amp;gt; &lt;code&gt;pagedown&lt;/code&gt; (pdf) –&amp;gt; &lt;code&gt;magick&lt;/code&gt; (gif). I also describe this workflow and provide examples &lt;a href=&#34;https://github.com/brshallo/flipbookr-gifs-examples&#34;&gt;here&lt;/a&gt;, e.g.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/brshallo/flipbookr-gifs-examples/raw/master/example-riddler-solution.gif&#34; alt=&#34;(Example from a prior blog post, “Riddler Solutions: Pedestrian Puzzles”)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;(Example from a prior blog post, “Riddler Solutions: Pedestrian Puzzles”)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Closing note&lt;/h1&gt;
&lt;p&gt;I recommend exploring the &lt;a href=&#34;https://education.rstudio.com/blog/&#34;&gt;Rstudio Education blog&lt;/a&gt;. The site contains helpful resources for improving your technical communication. It was here that I stumbled on the post &lt;a href=&#34;https://education.rstudio.com/blog/2020/05/flair/&#34;&gt;Decorate your R code with flair&lt;/a&gt;. Reading this inspired me to make a first attempt at building a reproducible animation of line-by-line execution of R code (something I’d been wanting to do for ages). The positive response &amp;amp; feedback to my initial tweet led me to learn about &lt;code&gt;flipbookr&lt;/code&gt; and motivated additional actions (described in &lt;a href=&#34;#engagement-contributions&#34;&gt;Engagement &amp;amp; contributions&lt;/a&gt;) including the review and completion of this blog post.&lt;/p&gt;
&lt;p&gt;Finally, please go enjoy the beautiful examples you can find at the &lt;code&gt;flipbookr&lt;/code&gt; &lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;about page&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://evamaerey.github.io/flipbooks/about&#34;&gt;&lt;img src=&#34;/post/2020-06-16-use-flipbooks-to-explain-your-code-and-thought-process_files/flipbookr-example.gif&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;more-sophisticated-highlighting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More sophisticated highlighting&lt;/h2&gt;
&lt;p&gt;For more sophisticated highlighting of &lt;em&gt;code&lt;/em&gt;, use the &lt;a href=&#34;https://github.com/kbodwin/flair&#34;&gt;flair package&lt;/a&gt;. I’m not sure what to recommend for highlighting changes in &lt;em&gt;output&lt;/em&gt; to the console… perhaps &lt;a href=&#34;https://github.com/brodieG/diffobj&#34;&gt;diffobj&lt;/a&gt; would be an option. You could also just explicitly format the output, e.g. using &lt;a href=&#34;https://github.com/rstudio/gt&#34;&gt;gt&lt;/a&gt; or &lt;a href=&#34;https://github.com/haozhu233/kableExtra&#34;&gt;kableExtra&lt;/a&gt; for tabular outputs, or using geoms, annotations, etc. in &lt;a href=&#34;https://github.com/tidyverse/ggplot2&#34;&gt;ggplot&lt;/a&gt;s. And, of course, you can always dive into the html.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;engagement-contributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Engagement &amp;amp; contributions&lt;/h2&gt;
&lt;p&gt;Blogging is time consuming. Reaching out to package maintainers or making contributions (even small ones) on open-source software projects can be intimidating. As a &lt;em&gt;tiny&lt;/em&gt; success story, I documented actions that stemmed (in some part) from engaging with the #rstats online communities while working on this blog post topic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While this post was in draft form, I tweeted out my initial approach (that used the &lt;a href=&#34;https://github.com/kbodwin/flair&#34;&gt;flair&lt;/a&gt; package).
&lt;ul&gt;
&lt;li&gt;The next step might have been trying to improve upon this. Thankfully, instead, &lt;a href=&#34;https://twitter.com/KellyBodwin&#34;&gt;Kelly Bodwin&lt;/a&gt; pointed me to &lt;code&gt;flipbookr&lt;/code&gt;!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
P.S. &lt;br&gt;&lt;br&gt;The &lt;code&gt;flair_lines()&lt;/code&gt; function lets you highlight whole line(s) if you want! &lt;br&gt;&lt;br&gt;{flipbookr} is a better option for making gifs/slides like this, but {flair} + {pagedown} + {magick} might help if you want specialty or layered highlighting.
&lt;/p&gt;
— Kelly Bodwin (&lt;span class=&#34;citation&#34;&gt;@KellyBodwin&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/KellyBodwin/status/1272741205365764097?ref_src=twsrc%5Etfw&#34;&gt;June 16, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Kelly also created an &lt;a href=&#34;https://github.com/kbodwin/flair/issues/15&#34;&gt;issue&lt;/a&gt; to further discuss possible integrations between &lt;code&gt;flair&lt;/code&gt; and &lt;code&gt;flipbookr&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I remade my initial example using &lt;code&gt;flipbookr&lt;/code&gt; (&lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr/issues/22&#34;&gt;see issue&lt;/a&gt;).
&lt;ul&gt;
&lt;li&gt;I first created an &lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr/issues/21&#34;&gt;issue&lt;/a&gt; showing how to print &lt;code&gt;xaringan&lt;/code&gt; slides incrementally using &lt;code&gt;pagedown::chrome_print()&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;Which helped to close a related &lt;a href=&#34;https://github.com/rstudio/pagedown/issues/110&#34;&gt;issue&lt;/a&gt; on &lt;code&gt;xaringan&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Gina Reynolds made a variety of updates to &lt;code&gt;flipbookr&lt;/code&gt;, one of which included adding the html –&amp;gt; pdf –&amp;gt; gif workflow to the template 😄.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Big thanks to &lt;a href=&#34;https://twitter.com/grrrck?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@grrrck&lt;/span&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/statsgen?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@statsgen&lt;/span&gt;&lt;/a&gt; for helps and &lt;a href=&#34;https://twitter.com/xieyihui?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@xieyihui&lt;/span&gt;&lt;/a&gt; because {xaringan}! And to &lt;a href=&#34;https://twitter.com/brshallo?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/KellyBodwin?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@KellyBodwin&lt;/span&gt;&lt;/a&gt; for new ideas about how to share flipbooks, html -&amp;gt; pdf -&amp;gt; gif. Guidance now included in template update on this - this gif created w/ that workflow!🙏🤩
&lt;/p&gt;
— Gina Reynolds (&lt;span class=&#34;citation&#34;&gt;@EvaMaeRey&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/EvaMaeRey/status/1274837474460626945?ref_src=twsrc%5Etfw&#34;&gt;June 21, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;See my notes and solutions &lt;a href=&#34;https://brshallo.github.io/r4ds_solutions/&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I also considered names such as &lt;code&gt;pexprex&lt;/code&gt;, &lt;code&gt;sexprex&lt;/code&gt;, &lt;code&gt;pripex&lt;/code&gt;, … I’ll let the reader guess at the acronyms.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Which I prefer over the alternatives of using the leading &lt;code&gt;*&lt;/code&gt; or wrapping the message in&lt;code&gt;{{}}&lt;/code&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Pairwise Operations</title>
      <link>/2020/06/03/tidy-2-way-column-combinations/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/03/tidy-2-way-column-combinations/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ii.-expand-combinations&#34;&gt;II. Expand combinations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iii.-filter-redundancies&#34;&gt;III. Filter redundancies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iv.-map-functions&#34;&gt;IV. Map function(s)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#v.-return-to-normal-dataframe&#34;&gt;V. Return to normal dataframe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vi.-bind-back-to-data&#34;&gt;VI. Bind back to data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functionalize&#34;&gt;Functionalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-creating-evaluating-features&#34;&gt;Example creating &amp;amp; evaluating features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-is-this-approach-inappropriate&#34;&gt;When is this approach inappropriate?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interactions-example-tidymodels&#34;&gt;Interactions example, tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expand-via-join&#34;&gt;Expand via join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nested-tibbles&#34;&gt;Nested tibbles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pivot-and-then-summarise&#34;&gt;Pivot and then summarise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gif-for-social-media&#34;&gt;Gif for social media&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tweets&#34;&gt;Tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;Session info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Say you want to map an operation or list of operations across all two-way&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; combinations of a set of variables/columns in a dataframe. For example, you may be doing feature engineering and want to create a set of interaction terms, ratios, etc&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. You may be interested in computing a summary statistic across all pairwise combinations of a given set of variables&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. In some cases there may be a pairwise implementation already available, e.g. R’s &lt;code&gt;cor()&lt;/code&gt; function for computing correlations&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. In other cases one may not exist or is not easy to use&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. In this post I’ll walk through an example&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; explaining code and steps for setting-up arbitrary pairwise operations across sets of variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I’ll break my approach down into several steps:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I. Nest and pivot&lt;br /&gt;
II. Expand combinations&lt;br /&gt;
III. Filter redundancies&lt;br /&gt;
IV. Map function(s)&lt;br /&gt;
V. Return to normal dataframe&lt;br /&gt;
VI. Bind back to data&lt;/p&gt;
&lt;p&gt;If your interest is only in computing summary statistics (as opposed to modifying an existing dataframe with new columns / features), then only steps I - IV are needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevant software and style:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I will primarily be using R’s &lt;code&gt;tidyverse&lt;/code&gt; packages. I make frequent use of lists as columns within dataframes – if you are new to these, see my previous &lt;a href=&#34;https://www.youtube.com/watch?v=gme4Fb9JVjk&#34;&gt;talk&lt;/a&gt; and the resources&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; I link to in the description.&lt;/p&gt;
&lt;p&gt;Throughout this post, wherever I write “dataframe” I really mean “tibble” (a dataframe with minor changes to default options and printing behavior). Also note that I am using &lt;code&gt;dplyr&lt;/code&gt; 0.8.3 rather than the newly released 1.0.0&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other resources and open issues (updated 2020-06-14):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In particular, the comments in issue &lt;a href=&#34;https://github.com/tidymodels/corrr/issues/44&#34;&gt;44&lt;/a&gt; for the &lt;code&gt;corrr&lt;/code&gt; package contain excellent solutions for doing pairwise operations (the subject of this post)&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. Issue &lt;a href=&#34;https://github.com/tidymodels/corrr/issues/94&#34;&gt;94&lt;/a&gt; also features discussion on this topic. Throughout this post I will reference other alternative code/approaches (especially in the footnotes and the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’ll use the ames housing dataset across examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames &amp;lt;- AmesHousing::make_ames()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifically, I’ll focus on ten numeric columns that, based on a random sample of 1000 rows, show the highest correlation with &lt;code&gt;Sale_Price&lt;/code&gt;&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(2020)
ames_cols &amp;lt;- ames %&amp;gt;% 
  select_if(is.numeric) %&amp;gt;% 
  sample_n(1000) %&amp;gt;% 
  corrr::correlate() %&amp;gt;% 
  corrr::focus(Sale_Price) %&amp;gt;% 
  arrange(-abs(Sale_Price)) %&amp;gt;% 
  head(10) %&amp;gt;% 
  pull(rowname)

ames_subset &amp;lt;- select(ames, ames_cols) %&amp;gt;% 
  # Could normalize data or do other prep 
  # but is not pertinent for examples
  mutate_all(as.double)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;i.-nest-and-pivot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;I. Nest and pivot&lt;/h2&gt;
&lt;p&gt;There are a variety of ways to make lists into columns within a dataframe. In the example below, I first use &lt;code&gt;summarise_all(.tbl = ames_subset, .funs = list)&lt;/code&gt; to create a one row dataframe where each column is a list containing a single element and each individual element corresponds with a numeric vector of length 2930.&lt;/p&gt;
&lt;p&gt;After nesting, I pivot&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; the columns leaving a dataframe with two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;var&lt;/code&gt; the variable names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vector&lt;/code&gt; a list where each element contains the associated vector&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists &amp;lt;- ames_subset %&amp;gt;% 
  summarise_all(list) %&amp;gt;% 
  pivot_longer(cols = everything(), 
               names_to = &amp;quot;var&amp;quot;, 
               values_to = &amp;quot;vector&amp;quot;) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    var            vector       
##    &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;       
##  1 Gr_Liv_Area    &amp;lt;dbl [2,930]&amp;gt;
##  2 Garage_Cars    &amp;lt;dbl [2,930]&amp;gt;
##  3 Garage_Area    &amp;lt;dbl [2,930]&amp;gt;
##  4 Total_Bsmt_SF  &amp;lt;dbl [2,930]&amp;gt;
##  5 First_Flr_SF   &amp;lt;dbl [2,930]&amp;gt;
##  6 Year_Built     &amp;lt;dbl [2,930]&amp;gt;
##  7 Full_Bath      &amp;lt;dbl [2,930]&amp;gt;
##  8 Year_Remod_Add &amp;lt;dbl [2,930]&amp;gt;
##  9 TotRms_AbvGrd  &amp;lt;dbl [2,930]&amp;gt;
## 10 Fireplaces     &amp;lt;dbl [2,930]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;a href=&#34;#pivot-and-then-summarise&#34;&gt;Pivot and then summarise&lt;/a&gt; for a nearly identical approach with just an altered order of steps. Also see &lt;a href=&#34;#nested-tibbles&#34;&gt;Nested tibbles&lt;/a&gt; for how you could create a list-column of dataframes&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; rather than vectors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if my variables are across rows not columns?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, pretend you want to see if &lt;code&gt;Sale_Price&lt;/code&gt; is different across &lt;code&gt;Mo_Sold&lt;/code&gt;. Perhaps you started by doing an F-test, found that to be significant, and now want to do pairwise t-tests across the samples of &lt;code&gt;Sale_Price&lt;/code&gt; for each &lt;code&gt;Mo_Sold&lt;/code&gt;. To set this up, you will want a &lt;code&gt;group_by()&lt;/code&gt; rather than a &lt;code&gt;pivot_longer()&lt;/code&gt; step. E.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames %&amp;gt;% 
  group_by(Mo_Sold) %&amp;gt;% 
  summarise(Sale_Price = list(Sale_Price)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 2
##    Mo_Sold Sale_Price 
##      &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;     
##  1       1 &amp;lt;int [123]&amp;gt;
##  2       2 &amp;lt;int [133]&amp;gt;
##  3       3 &amp;lt;int [232]&amp;gt;
##  4       4 &amp;lt;int [279]&amp;gt;
##  5       5 &amp;lt;int [395]&amp;gt;
##  6       6 &amp;lt;int [505]&amp;gt;
##  7       7 &amp;lt;int [449]&amp;gt;
##  8       8 &amp;lt;int [233]&amp;gt;
##  9       9 &amp;lt;int [161]&amp;gt;
## 10      10 &amp;lt;int [173]&amp;gt;
## 11      11 &amp;lt;int [143]&amp;gt;
## 12      12 &amp;lt;int [104]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At which point your data is in fundamentally the same form as was created in the previous code chunk (at least for if we only care about computing summary metrics that don’t require vectors of equal length&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;) so you can move onto &lt;a href=&#34;#ii.-expand-combinations&#34;&gt;II. Expand combinations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If the variables needed for your combinations of interest are across both rows and columns, you may want to use both &lt;code&gt;pivot_longer()&lt;/code&gt; and &lt;code&gt;group_by()&lt;/code&gt; steps and may need to make a few small modifications.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ii.-expand-combinations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;II. Expand combinations&lt;/h2&gt;
&lt;p&gt;I then use &lt;code&gt;tidyr::nesting()&lt;/code&gt; within &lt;code&gt;tidyr::expand()&lt;/code&gt; to make all 2-way combinations of our rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists_comb &amp;lt;- expand(df_lists,
                        nesting(var, vector),
                        nesting(var2 = var, vector2 = vector)) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 4
##    var        vector        var2           vector2      
##    &amp;lt;chr&amp;gt;      &amp;lt;list&amp;gt;        &amp;lt;chr&amp;gt;          &amp;lt;list&amp;gt;       
##  1 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Fireplaces     &amp;lt;dbl [2,930]&amp;gt;
##  2 Fireplaces &amp;lt;dbl [2,930]&amp;gt; First_Flr_SF   &amp;lt;dbl [2,930]&amp;gt;
##  3 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Full_Bath      &amp;lt;dbl [2,930]&amp;gt;
##  4 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Garage_Area    &amp;lt;dbl [2,930]&amp;gt;
##  5 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Garage_Cars    &amp;lt;dbl [2,930]&amp;gt;
##  6 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Gr_Liv_Area    &amp;lt;dbl [2,930]&amp;gt;
##  7 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Total_Bsmt_SF  &amp;lt;dbl [2,930]&amp;gt;
##  8 Fireplaces &amp;lt;dbl [2,930]&amp;gt; TotRms_AbvGrd  &amp;lt;dbl [2,930]&amp;gt;
##  9 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Year_Built     &amp;lt;dbl [2,930]&amp;gt;
## 10 Fireplaces &amp;lt;dbl [2,930]&amp;gt; Year_Remod_Add &amp;lt;dbl [2,930]&amp;gt;
## # ... with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;a href=&#34;#expand-via-join&#34;&gt;Expand via join&lt;/a&gt; for an alternative approach using the &lt;code&gt;dplyr::*_join()&lt;/code&gt; operations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You could make a strong case that this step should be after &lt;a href=&#34;#iii.-filter-redundancies&#34;&gt;III. Filter redundancies&lt;/a&gt;&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;. However putting it beforehand makes the required code easier to write and to read.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iii.-filter-redundancies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;III. Filter redundancies&lt;/h2&gt;
&lt;p&gt;Filter-out redundant columns, sort the rows, better organize the columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_lists_comb &amp;lt;- df_lists_comb %&amp;gt;% 
  filter(var != var2) %&amp;gt;% 
  arrange(var, var2) %&amp;gt;% 
  mutate(vars = paste0(var, &amp;quot;.&amp;quot;, var2)) %&amp;gt;% 
  select(contains(&amp;quot;var&amp;quot;), everything()) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 90 x 5
##    var          var2           vars                    vector       vector2     
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                   &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;      
##  1 Fireplaces   First_Flr_SF   Fireplaces.First_Flr_SF &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  2 Fireplaces   Full_Bath      Fireplaces.Full_Bath    &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  3 Fireplaces   Garage_Area    Fireplaces.Garage_Area  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  4 Fireplaces   Garage_Cars    Fireplaces.Garage_Cars  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  5 Fireplaces   Gr_Liv_Area    Fireplaces.Gr_Liv_Area  &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  6 Fireplaces   Total_Bsmt_SF  Fireplaces.Total_Bsmt_~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  7 Fireplaces   TotRms_AbvGrd  Fireplaces.TotRms_AbvG~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  8 Fireplaces   Year_Built     Fireplaces.Year_Built   &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
##  9 Fireplaces   Year_Remod_Add Fireplaces.Year_Remod_~ &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
## 10 First_Flr_SF Fireplaces     First_Flr_SF.Fireplaces &amp;lt;dbl [2,930~ &amp;lt;dbl [2,930~
## # ... with 80 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your operation of interest is associative&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;, apply a filter to remove additional redundant combinations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c_sort_collapse &amp;lt;- function(...){
  c(...) %&amp;gt;% 
    sort() %&amp;gt;% 
    str_c(collapse = &amp;quot;.&amp;quot;)
}

df_lists_comb_as &amp;lt;- df_lists_comb %&amp;gt;% 
  mutate(vars = map2_chr(.x = var, 
                         .y = var2, 
                         .f = c_sort_collapse)) %&amp;gt;%
  distinct(vars, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;iv.-map-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IV. Map function(s)&lt;/h2&gt;
&lt;p&gt;Each row of your dataframe now contains the relevant combinations of variables and is ready to have any arbitrary function(s) mapped across them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example with summary statistic&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, let’s say we want to compute the p-value of the correlation coefficient for each pair&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs_cor_pvalues &amp;lt;- df_lists_comb_as %&amp;gt;% 
  mutate(cor_pvalue = map2(vector, vector2, cor.test) %&amp;gt;% map_dbl(&amp;quot;p.value&amp;quot;),
         vars = fct_reorder(vars, -cor_pvalue)) %&amp;gt;% 
  arrange(cor_pvalue) %&amp;gt;% 
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45 x 6
##    var        var2         vars                vector     vector2     cor_pvalue
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;               &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 First_Flr~ Total_Bsmt_~ First_Flr_SF.Total~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  2 Full_Bath  Gr_Liv_Area  Full_Bath.Gr_Liv_A~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  3 Garage_Ar~ Garage_Cars  Garage_Area.Garage~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  4 Gr_Liv_Ar~ TotRms_AbvG~ Gr_Liv_Area.TotRms~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  0.       
##  5 Year_Built Year_Remod_~ Year_Built.Year_Re~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  7.85e-301
##  6 First_Flr~ Gr_Liv_Area  First_Flr_SF.Gr_Li~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  8.17e-244
##  7 Garage_Ca~ Year_Built   Garage_Cars.Year_B~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  1.57e-219
##  8 Full_Bath  TotRms_AbvG~ Full_Bath.TotRms_A~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  1.24e-210
##  9 First_Flr~ Garage_Area  First_Flr_SF.Garag~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  8.16e-178
## 10 Garage_Ca~ Gr_Liv_Area  Garage_Cars.Gr_Liv~ &amp;lt;dbl [2,9~ &amp;lt;dbl [2,93~  4.80e-175
## # ... with 35 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For fun, let’s plot the most significant associations onto a bar graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs_cor_pvalues %&amp;gt;% 
  head(15) %&amp;gt;% 
  mutate(cor_pvalue_nlog = -log(cor_pvalue)) %&amp;gt;% 
  ggplot(aes(x = vars, 
             y = cor_pvalue_nlog, 
             fill = is.infinite(cor_pvalue_nlog) %&amp;gt;% factor(c(T, F))))+
  geom_col()+
  coord_flip()+
  theme_bw()+
  labs(title = &amp;quot;We are confident that garage area and # of garage cars are correlated&amp;quot;,
       y = &amp;quot;Negative log of p-value of correlation coefficient&amp;quot;,
       x = &amp;quot;Variable combinations&amp;quot;,
       fill = &amp;quot;Too high to\nmeaningfully\ndifferentiate:&amp;quot;)+
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You could use this approach to calculate any pairwise summary statistic. For example, see &lt;a href=&#34;https://gist.github.com/brshallo/dc3c1f2f34519ca2a8a68024bc3a22e5&#34;&gt;gist&lt;/a&gt; where I calculate the K-S statistic across each combination of a group of distributions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you only care about computing summary statistics on your pairwise combinations, (and not adding new columns onto your original dataframe) you can stop here.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example with transformations&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Back to the feature engineering example, perhaps we want to create new features of the difference and quotient of each combination of our variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features_prep1 &amp;lt;- df_lists_comb %&amp;gt;% 
  mutate(difference = map2(vector, vector2, `-`),
         ratio = map2(vector, vector2, `/`))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;v.-return-to-normal-dataframe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;V. Return to normal dataframe&lt;/h2&gt;
&lt;p&gt;The next set of steps will put our data back into a more traditional form consistent with our starting dataframe/tibble.&lt;/p&gt;
&lt;p&gt;First let’s revert our data to a form similar to where it was at the end of &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt; where we had two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one with our variable names&lt;/li&gt;
&lt;li&gt;a second containing a list-column of vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features_prep2 &amp;lt;- new_features_prep1 %&amp;gt;% 
  pivot_longer(cols = c(difference, ratio)) %&amp;gt;% # 1
  mutate(name_vars = str_c(var, name, var2, sep = &amp;quot;.&amp;quot;)) %&amp;gt;% # 2
  select(name_vars, value) # 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of each line of code above is a number corresponding with the following explanations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;if we had done just one operation, this step would not be needed, but we did multiple operations, created multiple list-columns (&lt;code&gt;difference&lt;/code&gt; and &lt;code&gt;ratio&lt;/code&gt;) which we need to get into a single list-column&lt;/li&gt;
&lt;li&gt;create new variable name that combines constituent variable names with name of transformation&lt;/li&gt;
&lt;li&gt;remove old columns&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next we simply apply the inverse of those operations performed in &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_features &amp;lt;- new_features_prep2 %&amp;gt;% 
  pivot_wider(values_from = value,
              names_from = name_vars) %&amp;gt;%
  unnest(cols = everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new features will add a good number of columns onto our original dataset&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(new_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2930  180&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;vi.-bind-back-to-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;VI. Bind back to data&lt;/h2&gt;
&lt;p&gt;I then bind the new features back onto the original subsetted dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_data_features &amp;lt;- bind_cols(ames_subset, new_features)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At which point I could do further exploring, feature engineering, model building, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;functionalize&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functionalize&lt;/h1&gt;
&lt;p&gt;I put these steps into a few (unpolished) functions found at &lt;a href=&#34;https://gist.github.com/brshallo/f92a5820030e21cfed8f823a6e1d56e1&#34;&gt;this gist&lt;/a&gt;&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::source_gist(&amp;quot;https://gist.github.com/brshallo/f92a5820030e21cfed8f823a6e1d56e1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mutate_pairwise()&lt;/code&gt; takes in your dataframe, the set of numeric columns to create pairwise combinations from, and a list of functions&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt; to apply.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-creating-evaluating-features&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example creating &amp;amp; evaluating features&lt;/h1&gt;
&lt;p&gt;Let’s use the new &lt;code&gt;mutate_pairwise()&lt;/code&gt; function to create new columns for the differences and quotients between all pairwise combinations of our variables of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_data_features_example &amp;lt;- mutate_pairwise(
  df = mutate_if(ames, is.numeric, as.double),
  one_of(ames_cols),
  funs = list(&amp;quot;/&amp;quot;, &amp;quot;-&amp;quot;),
  funs_names = list(&amp;quot;ratio&amp;quot;, &amp;quot;difference&amp;quot;),
  associative = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps you want to calculate some measure of association between your features and a target of interest. To keep things simple, I’ll remove any columns that contain any NA’s or infinite values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;features_keep &amp;lt;- ames_data_features_example %&amp;gt;% 
  keep(is.numeric) %&amp;gt;% 
  keep(~sum(is.na(.) | is.infinite(.)) == 0) %&amp;gt;% 
  colnames()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe, for some reason, you want to see the statistical significance of the correlation of each feature with &lt;code&gt;Sale_Price&lt;/code&gt; when weighting by &lt;code&gt;Lot_Area&lt;/code&gt;. I’ll calculate these across variables (and a random sample of 1500 observations) then plot them on a histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
ames_data_features_example %&amp;gt;% 
  sample_n(1500) %&amp;gt;% 
  summarise_at(
    .vars = features_keep[!(features_keep %in% c(&amp;quot;Sale_Price&amp;quot;, &amp;quot;Lot_Area&amp;quot;))],
    .funs = ~weights::wtd.cor(., Sale_Price, weight = Lot_Area)[1]) %&amp;gt;% 
  gather() %&amp;gt;% # gather() is an older version of pivot_longer() w/ fewer parameters
  ggplot(aes(x = value))+
  geom_vline(xintercept = 0, colour = &amp;quot;lightgray&amp;quot;, size = 2)+
  geom_histogram()+
  scale_x_continuous(labels = scales::comma)+
  labs(title = &amp;quot;Distribution of correlations with Sale_Price&amp;quot;,
       subtitle = &amp;quot;Weighted by Lot Area&amp;quot;,
       x = &amp;quot;Weighted correlation coefficient&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If doing predictive modeling or inference you may want to fit any transformations and analysis into a &lt;code&gt;tidymodels&lt;/code&gt; pipeline or other framework. For some brief notes on this see &lt;a href=&#34;#interactions-example-tidymodels&#34;&gt;Interactions example, tidymodels&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-is-this-approach-inappropriate&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;When is this approach inappropriate?&lt;/h1&gt;
&lt;p&gt;Combinatorial growth is very fast&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;. As you increase either the number of variables in your pool or the size of each set, you will quickly bump into computational limitations.&lt;/p&gt;
&lt;p&gt;Tidyverse packages are optimized to be efficient. However operations with matrices or other specialized formats&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; are generally faster&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt; than with dataframes/tibbles. If you are running into computational challenges but prefer to stick with a tidyverse aesthetic (which uses dataframes as a cornerstone), you might:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use heuristics to reduce the number of variables or operations you need to perform (e.g. take a sample, use a preliminary filter, a step-wise like iteration, etc.)&lt;/li&gt;
&lt;li&gt;Look for packages that abstract the storage and computationally heavy operations away&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; and then return back an output in a convenient form&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improve the efficiency of your code (e.g. filter redundancies before rather than after expanding combinations)&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consider parralelizing&lt;/li&gt;
&lt;li&gt;Use matrices&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is sometimes an urge to do &lt;em&gt;everything&lt;/em&gt; in a tidy way, which is not necessary. For example, you &lt;em&gt;could&lt;/em&gt; use an approach like the one I walk through to calculate pairwise correlations between each of your variables. However, the &lt;code&gt;cor()&lt;/code&gt; function would do this much more efficiently if called on a matrix or traditional dataframe without list-columns (though you could also use the &lt;code&gt;corrr&lt;/code&gt; package within the &lt;code&gt;tidymodels&lt;/code&gt; suite which calls &lt;code&gt;cor()&lt;/code&gt; in the back-end&lt;a href=&#34;#fn29&#34; class=&#34;footnote-ref&#34; id=&#34;fnref29&#34;&gt;&lt;sup&gt;29&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;However, for many operations…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there may not be an efficient pairwise implementation available / accessible&lt;/li&gt;
&lt;li&gt;the slower computation may not matter or can be mitigated in some way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These situations&lt;a href=&#34;#fn30&#34; class=&#34;footnote-ref&#34; id=&#34;fnref30&#34;&gt;&lt;sup&gt;30&lt;/sup&gt;&lt;/a&gt; are where the approach I walked through is most appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;interactions-example-tidymodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions example, tidymodels&lt;/h2&gt;
&lt;p&gt;A good example for creating and evaluating interaction terms&lt;a href=&#34;#fn31&#34; class=&#34;footnote-ref&#34; id=&#34;fnref31&#34;&gt;&lt;sup&gt;31&lt;/sup&gt;&lt;/a&gt; is in &lt;a href=&#34;http://www.feat.engineering/complete-enumeration.html#complete-enumeration-simple-screening&#34;&gt;The Brute-Force Approach to Identifying Predictive Interactions, Simple Screening&lt;/a&gt; section of &lt;em&gt;Max Kuhn&lt;/em&gt; and &lt;em&gt;Kjell Johnson’s&lt;/em&gt; (free) online book “Feature Engineering and Selection: A Practical Approach for Predictive Models”.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/topepo/FES/blob/master/07_Detecting_Interaction_Effects/7_04_The_Brute-Force_Approach_to_Identifying_Predictive_Interactions/ames_pairwise.R&#34;&gt;source code&lt;/a&gt; shows another approach for combining variables. The author uses…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;combn()&lt;/code&gt; to create all combinations of variable names which are then…&lt;/li&gt;
&lt;li&gt;turned into formulas and passed into &lt;code&gt;recipes::step_interact()&lt;/code&gt;, specifying the new columns to be created&lt;a href=&#34;#fn32&#34; class=&#34;footnote-ref&#34; id=&#34;fnref32&#34;&gt;&lt;sup&gt;32&lt;/sup&gt;&lt;/a&gt;…&lt;/li&gt;
&lt;li&gt;for each interaction term…&lt;/li&gt;
&lt;li&gt;in each associated model being evaluated&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The example uses a mix of packages and styles and is not a purely tidy approach – &lt;code&gt;tidymodels&lt;/code&gt; has also gone through a lot of development since “Feature Engineering and Selection…” was published in 2019&lt;a href=&#34;#fn33&#34; class=&#34;footnote-ref&#34; id=&#34;fnref33&#34;&gt;&lt;sup&gt;33&lt;/sup&gt;&lt;/a&gt;. Section 11.2 on &lt;a href=&#34;http://www.feat.engineering/greedy-simple-filters.html&#34;&gt;Greedy Search Methods, Simple Filters&lt;/a&gt; is also highly relevant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expand-via-join&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expand via join&lt;/h2&gt;
&lt;p&gt;You can take advantage of join&lt;a href=&#34;#fn34&#34; class=&#34;footnote-ref&#34; id=&#34;fnref34&#34;&gt;&lt;sup&gt;34&lt;/sup&gt;&lt;/a&gt; behavior to create all possible row combinations. In this case, the output will be the same as shown when using &lt;code&gt;expand()&lt;/code&gt; (except row order will be different).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(mutate(df_lists, id = 1),
          mutate(df_lists, id = 1) %&amp;gt;% rename_at(vars(-one_of(&amp;quot;id&amp;quot;)), paste0, &amp;quot;2&amp;quot;)) %&amp;gt;%
  select(-id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nested-tibbles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nested tibbles&lt;/h2&gt;
&lt;p&gt;Creates list of tibbles rather than list of vectors – typically the first way lists as columns in dataframes is introduced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_subset %&amp;gt;% 
  pivot_longer(everything(), names_to = &amp;quot;var&amp;quot;, values_to = &amp;quot;list&amp;quot;) %&amp;gt;% 
  group_by(var) %&amp;gt;% 
  nest()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pivot-and-then-summarise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pivot and then summarise&lt;/h2&gt;
&lt;p&gt;(Almost) equivalent to the example in &lt;a href=&#34;#i.-nest-and-pivot&#34;&gt;I. Nest and pivot&lt;/a&gt;. Steps just run in a different order (row order will also be different).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_test %&amp;gt;% 
  pivot_longer(cols = everything(), 
             names_to = &amp;quot;var&amp;quot;, 
             values_to = &amp;quot;vector&amp;quot;) %&amp;gt;% 
  group_by(var) %&amp;gt;% 
  summarise_all(list)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gif-for-social-media&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gif for social media&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AmesHousing::make_ames() %&amp;gt;% 
  select(Year = Year_Sold, Price = Sale_Price) %&amp;gt;% 
  # I.
  group_by(Year) %&amp;gt;% 
  summarise(Price = list(Gr_Liv_Area)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  # II.
  expand(nesting(Year, Price),
         nesting(Year2 = Year, Price2 = Price)
  ) %&amp;gt;%
  # III.
  filter(Year != Year2) %&amp;gt;% 
  mutate(Years = map2_chr(.x = Year, 
                          .y = Year2, 
                          .f = c_sort_collapse)) %&amp;gt;%
  distinct(Years, .keep_all = TRUE) %&amp;gt;% 
  select(-Years) %&amp;gt;% 
  #IV.
  mutate(ks_test = map2(Price, 
                        Price2, 
                        stats::ks.test) %&amp;gt;% map_dbl(&amp;quot;p.value&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-31-tidy-2-way-column-combinations_files/pairwise-comparison-gif-edit.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Actual gif was created by embedding above code into a presentation and exporting it as a gif and then making a few minor edits.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tweets&lt;/h2&gt;
&lt;p&gt;A few tweets as documentation of thinking.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Original tweet + R bloggers tweet:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
What is your &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; (or other &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; ) approach for doing arbitrary pairwise operations across variables? Mine is frequently something like:&lt;br&gt;&lt;br&gt;I. nest…&lt;br&gt;II. expand combos… &lt;br&gt;III. filter…&lt;br&gt;IV. map fun(s)…&lt;br&gt;…&lt;br&gt;&lt;br&gt;I wrote a post walking through this: &lt;a href=&#34;https://t.co/xRnRf5yh3m&#34;&gt;https://t.co/xRnRf5yh3m&lt;/a&gt; &lt;a href=&#34;https://t.co/Zvxey2gm3H&#34;&gt;pic.twitter.com/Zvxey2gm3H&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1271194908477591553?ref_src=twsrc%5Etfw&#34;&gt;June 11, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Tidy Pairwise Operations {&lt;a href=&#34;https://t.co/mI5r2e5ttN&#34;&gt;https://t.co/mI5r2e5ttN&lt;/a&gt;} &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DataScience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DataScience&lt;/a&gt;
&lt;/p&gt;
— R-bloggers (&lt;span class=&#34;citation&#34;&gt;@Rbloggers&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Rbloggers/status/1307007573611155456?ref_src=twsrc%5Etfw&#34;&gt;September 18, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet with link to gist of other example applying this approach:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/W_R_Chase?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@W_R_Chase&lt;/span&gt;&lt;/a&gt; alludes to using &lt;code&gt;expand()&lt;/code&gt; for a solution but takes a different approach. I wrote a short gist that fleshes in what a &lt;code&gt;tidyr::expand()&lt;/code&gt; approach to this problem could look like: &lt;a href=&#34;https://t.co/agloPgJR1r&#34;&gt;https://t.co/agloPgJR1r&lt;/a&gt; (2/3)
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1272411480193974273?ref_src=twsrc%5Etfw&#34;&gt;June 15, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet about &lt;code&gt;widyr&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Will add &lt;code&gt;widyr&lt;/code&gt; to my set of tools for tidy pairwise operations: &lt;a href=&#34;https://t.co/NSxNC3nehK&#34;&gt;https://t.co/NSxNC3nehK&lt;/a&gt; !&lt;br&gt;&lt;br&gt;Seems to overlap some w/ tidymodels, eg &lt;code&gt;corrr&lt;/code&gt;📦 (&lt;a href=&#34;https://twitter.com/thisisdaryn?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@thisisdaryn&lt;/span&gt;&lt;/a&gt; ) or could imagine widely_kmeans() as a &lt;code&gt;recipe&lt;/code&gt;/&lt;code&gt;embed&lt;/code&gt; step…&lt;a href=&#34;https://twitter.com/drob?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@drob&lt;/span&gt;&lt;/a&gt; any tips on when/how you use these in combination? &lt;a href=&#34;https://t.co/dAsJtNW7Vo&#34;&gt;https://t.co/dAsJtNW7Vo&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1313966803488583686?ref_src=twsrc%5Etfw&#34;&gt;October 7, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Tweet with more efficient approach (for case when just combining multiple columns and returning output of equal number of rows):&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
How you can use &lt;code&gt;dplyr::mutate()&lt;/code&gt; to return a dataframe consisting of all combinations of arbitrary pairwise operations across a selection of columns: &lt;a href=&#34;https://t.co/RxwtbmWqap&#34;&gt;https://t.co/RxwtbmWqap&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; … &lt;a href=&#34;https://t.co/UpJw0pGPUd&#34;&gt;pic.twitter.com/UpJw0pGPUd&lt;/a&gt;
&lt;/p&gt;
— Bryan Shalloway (&lt;span class=&#34;citation&#34;&gt;@brshallo&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brshallo/status/1316851879658356736?ref_src=twsrc%5Etfw&#34;&gt;October 15, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.5.1 (2018-07-02)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] forcats_0.4.0   stringr_1.4.0   dplyr_1.0.1     purrr_0.3.4    
## [5] readr_1.3.1     tidyr_1.1.1     tibble_3.0.3    ggplot2_3.3.2  
## [9] tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] fs_1.3.2            usethis_1.4.0       lubridate_1.7.9    
##  [4] devtools_2.0.0      RColorBrewer_1.1-2  httr_1.4.0         
##  [7] rprojroot_1.3-2     tools_3.5.1         backports_1.1.8    
## [10] utf8_1.1.4          R6_2.4.1            rpart_4.1-13       
## [13] Hmisc_4.1-1         colorspace_1.4-1    nnet_7.3-12        
## [16] withr_2.2.0         gridExtra_2.3       tidyselect_1.1.0   
## [19] prettyunits_1.1.1   processx_3.4.2      curl_3.3           
## [22] compiler_3.5.1      cli_2.0.2           rvest_0.3.4        
## [25] htmlTable_1.12      mice_3.8.0          xml2_1.2.0         
## [28] desc_1.2.0          labeling_0.3        bookdown_0.11      
## [31] checkmate_2.0.0     scales_1.1.1        corrr_0.4.2.9000   
## [34] callr_3.4.3         digest_0.6.25       foreign_0.8-70     
## [37] rmarkdown_1.13      base64enc_0.1-3     pkgconfig_2.0.3    
## [40] htmltools_0.5.0     sessioninfo_1.1.1   htmlwidgets_1.3    
## [43] rlang_0.4.7         readxl_1.3.1        rstudioapi_0.11    
## [46] farver_2.0.3        generics_0.0.2      jsonlite_1.6.1     
## [49] gtools_3.8.2        acepack_1.4.1       magrittr_1.5       
## [52] Formula_1.2-3       Matrix_1.2-14       Rcpp_1.0.4.6       
## [55] munsell_0.5.0       fansi_0.4.1         lifecycle_0.2.0    
## [58] weights_1.0.1       stringi_1.4.6       yaml_2.2.1         
## [61] pkgbuild_1.1.0      grid_3.5.1          gdata_2.18.0       
## [64] crayon_1.3.4        lattice_0.20-35     haven_2.1.0        
## [67] splines_3.5.1       hms_0.5.2           knitr_1.29         
## [70] ps_1.3.2            pillar_1.4.6        pkgload_1.0.2      
## [73] glue_1.4.1          evaluate_0.14       blogdown_0.15      
## [76] latticeExtra_0.6-28 data.table_1.12.8   remotes_2.1.0      
## [79] modelr_0.1.4        vctrs_0.3.2         testthat_2.3.2     
## [82] cellranger_1.1.0    gtable_0.3.0        assertthat_0.2.1   
## [85] xfun_0.16           broom_0.7.0         AmesHousing_0.0.3  
## [88] survival_2.42-3     memoise_1.1.0       cluster_2.0.7-1    
## [91] ellipsis_0.3.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Will focus on two-way example in this post, but could use similar methods to make more generalizable solution across n-way examples. If I were to do this, the code below would change. E.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to use &lt;code&gt;pmap*()&lt;/code&gt; operations over &lt;code&gt;map2*()&lt;/code&gt; operations&lt;/li&gt;
&lt;li&gt;I’d need to make some functions that make it so I can remove all the places where I have &lt;code&gt;var&lt;/code&gt; and &lt;code&gt;var2&lt;/code&gt; type column names hard-coded&lt;/li&gt;
&lt;li&gt;Alternatively, I might shift approaches and make better use of &lt;code&gt;combn()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Though this “throw everything and the kitchen-sink” approach may not always be a good idea.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I’ve done this type of operation in a variety of ways. Sometimes without any really good reason as to why I used one approach or another. It isn’t completely clear (at least to me) the recommended way of doing these type of operations within the tidyverse – hence the diversity of my approaches in the past and deciding to document the typical steps in the approach I take… via writing this post.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Or the tidymodels implementation &lt;code&gt;corrr::correlate()&lt;/code&gt; in the &lt;a href=&#34;https://corrr.tidymodels.org/&#34;&gt;corrr&lt;/a&gt; package.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;or is not in a style you prefer&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;I’ll also reference related approaches / small tweaks (putting those materials in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;. This is by no means an exhaustive list (e.g. don’t have an example with a &lt;code&gt;for&lt;/code&gt; loop or with a &lt;code&gt;%do%&lt;/code&gt; operator). The source code of my post on &lt;a href=&#34;https://www.bryanshalloway.com/2020/02/13/fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs/&#34;&gt;Ambiguous Absolute Value&lt;/a&gt; signs shows a related but more complex / messy approach on a combinatorics problem.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;In particular, the chapters on “Iteration” and “Many Models” in &lt;a href=&#34;https://r4ds.had.co.nz/iteration.html&#34;&gt;R for Data Science&lt;/a&gt;. I would also recommend Rebecca Barter’s &lt;a href=&#34;http://www.rebeccabarter.com/blog/2019-08-19_purrr/&#34;&gt;Learn to purrr&lt;/a&gt; blog post.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;The new &lt;code&gt;dplyr&lt;/code&gt; 1.0.0. contains new functions that would have been potentially useful for several of these operations. I highly recommend checking these updates out in the various &lt;a href=&#34;https://www.tidyverse.org/tags/dplyr-1-0-0/&#34;&gt;recent posts&lt;/a&gt; by Hadley Wickham. Some of the major updates (potentially relevant to the types of operations I’ll be discussing in my post):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;new approach for across-column operations (replacing &lt;code&gt;_at()&lt;/code&gt;, &lt;code&gt;_if()&lt;/code&gt;, &lt;code&gt;_all()&lt;/code&gt; variants with &lt;code&gt;across()&lt;/code&gt; function)&lt;/li&gt;
&lt;li&gt;brought-back rowwise operations&lt;/li&gt;
&lt;li&gt;emphasize ability to output tibbles / multiple columns in core &lt;code&gt;dplyr&lt;/code&gt; verbs. This is something I had only taken advantage of occassionally in the past (&lt;a href=&#34;https://stackoverflow.com/a/54725732/9059865&#34;&gt;example&lt;/a&gt;), but will look to use more going forward.&lt;/li&gt;
&lt;/ul&gt;
&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;f I’d spotted this issue initially I’m not sure I would have written this post. However what this post offers is a more verbose treatment of the problem which may be useful for people newer to pairwise operations or the tidyverse.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;For technical reasons, I also converted all integer types to doubles – was getting integer overflow problems in later operations before changing. &lt;a href=&#34;https://stackoverflow.com/questions/8804779/what-is-integer-overflow-in-r-and-how-can-it-happen&#34;&gt;Thread&lt;/a&gt; on integer overflow in R. In this post I’m not taking a disciplined approach to feature engineering. For example it may make sense to normalize the variables so that variable combinations would be starting on a similar scale. This could be done using &lt;code&gt;recipes::step_normalize()&lt;/code&gt; or with code like &lt;code&gt;dplyr::mutate_all(df, ~(. - mean(.)) / sd(.))&lt;/code&gt; .&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Note that this part of the problem is one where I actually find using &lt;code&gt;tidyr::gather()&lt;/code&gt; easier – but I’ve been forcing myself to switch over to using the &lt;code&gt;pivot_()&lt;/code&gt; functions over &lt;code&gt;spread()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt;.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;The more common approach.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;If your variables are across rows you are likely concerned with getting summary metrics rather than creating new features – as if your data is across rows there is nothing guaranteeing you have the same number of observations or that they are lined-up appropriately. If you &lt;em&gt;are&lt;/em&gt; interested in creating new features, you should probably have first reshaped your data to ensure each column represented a variable.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;As switching these would be more computationally efficient – see &lt;a href=&#34;#when-is-this-approach-inappropriate&#34;&gt;When is this approach inappropriate?&lt;/a&gt; for notes related to this. Switching the order here would suggest using approaches with the&lt;code&gt;combn()&lt;/code&gt; function.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;I.e. has the same output regardless of the order of the variables. E.g. multiplication or addition but not subtraction or division.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;Function(s) that output vectors of length 1 (or less than length of input vectors).&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;Note that the pairwise implementation &lt;code&gt;psych::corr.test()&lt;/code&gt; could have been used on your original subsetted dataframe, see &lt;a href=&#34;https://stackoverflow.com/questions/13112238/a-matrix-version-of-cor-test&#34;&gt;stack overflow thread&lt;/a&gt;.&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;Function(s) that output vector of length equal to length of input vectors.&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Did not print this output because cluttered-up page with so many column names.&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;Steps I - III and V &amp;amp; VI are essentially direct copies of the code above. The approach I took with Step IV may take more effort to follow as it requires understanding a little &lt;code&gt;rlang&lt;/code&gt; and could likely have been done more simply.&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;Must have two vectors as input, but do not need to be infix functions.&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;Non-technical article discussing combinatorial explosion in context of company user growth targets: &lt;a href=&#34;https://medium.com/@TorBair/exponential-growth-isn-t-cool-combinatorial-growth-is-85a0b1fdb6a5&#34;&gt;Exponential Growth Isn’t Cool. Combinatorial Growth Is.&lt;/a&gt;.&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;E.g. &lt;a href=&#34;https://github.com/Rdatatable/data.table&#34;&gt;data.table&lt;/a&gt; dataframes&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;Hence, if you are doing operations across combinations of lots of variables it may not make sense to do the operations directly within dataframes.&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;Much (if not most) of the &lt;code&gt;tidyverse&lt;/code&gt; (and the R programming language generally) is about creating a smooth interface between the analyst/scientist and the back-end complexity of the operations they are performing. Projects like &lt;a href=&#34;https://spark.rstudio.com/&#34;&gt;sparklyr&lt;/a&gt;, &lt;a href=&#34;https://db.rstudio.com/dbi/&#34;&gt;DBI&lt;/a&gt;, &lt;a href=&#34;https://github.com/rstudio/reticulate&#34;&gt;reticulate&lt;/a&gt;, &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;, and &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; (to name a few) represent cases where this &lt;em&gt;interface&lt;/em&gt; role of R is most apparent.&lt;a href=&#34;#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;For tidyverse packages, this is often returned into or in the form of a dataframe.&lt;a href=&#34;#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;Could make better use of &lt;code&gt;combn()&lt;/code&gt; function to help.&lt;a href=&#34;#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;Depending on the complexity may just need to brush-up on your linear algebra.&lt;a href=&#34;#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn29&#34;&gt;&lt;p&gt;&lt;code&gt;corrr&lt;/code&gt; can also be used to run the operation on databases that may have larger data than you could fit on your computer.&lt;a href=&#34;#fnref29&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn30&#34;&gt;&lt;p&gt;Likely more common for many, if not most, analysts and data scientists.&lt;a href=&#34;#fnref30&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn31&#34;&gt;&lt;p&gt;I.e. multiplying two variables together&lt;a href=&#34;#fnref31&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn32&#34;&gt;&lt;p&gt;Created upon the recipe being &lt;em&gt;baked&lt;/em&gt; or &lt;em&gt;juiced&lt;/em&gt; – if you have not checked it out, &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; is AWESOME!&lt;a href=&#34;#fnref32&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn33&#34;&gt;&lt;p&gt;Maybe at a future date I’ll make a post writing out the example here using the newer approaches now available in &lt;code&gt;tidymodels&lt;/code&gt;. &lt;a href=&#34;https://gist.github.com/brshallo/674ff06608c1a55fefb8d5dc49896d65&#34;&gt;Gist&lt;/a&gt; of &lt;code&gt;combn_ttible()&lt;/code&gt;… starting place for if I ever get to that write-up.&lt;a href=&#34;#fnref33&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn34&#34;&gt;&lt;p&gt;Could also have used &lt;code&gt;right_join()&lt;/code&gt; or &lt;code&gt;full_join()&lt;/code&gt;.&lt;a href=&#34;#fnref34&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>animatrixr &amp; Visualizing Matrix Transformations pt. 2</title>
      <link>/2020/02/24/animatrixr-visualizing-matrix-transformations-pt-2/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/24/animatrixr-visualizing-matrix-transformations-pt-2/</guid>
      <description>


&lt;p&gt;This post is a continuation on my post from last week on &lt;a href=&#34;https://www.bryanshalloway.com/2020/02/20/visualizing-matrix-transformations-with-gganimate/&#34;&gt;Visualizing Matrix Transformations with gganimate&lt;/a&gt;. Both posts are largely inspired by &lt;a href=&#34;https://twitter.com/3blue1brown&#34;&gt;Grant Sanderson’s&lt;/a&gt; beautiful video series &lt;a href=&#34;https://www.youtube.com/watch?v=kYB8IZa5AuE&amp;amp;list=PL_w8oSr1JpVCZ5pKXHKz6PkjGCbPbSBYv&amp;amp;index=4&#34;&gt;The Essence of Linear Algebra&lt;/a&gt; and wanting to continue messing around with &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;Thomas Lin Peterson’s&lt;/a&gt; fantastic &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; package in R.&lt;/p&gt;
&lt;p&gt;As with the last post, I’ll describe trying to (very loosely) recreate a &lt;em&gt;small&lt;/em&gt; part of the visualizations showing the geometry of matrix multiplication and changing basis vectors (using &lt;code&gt;gganimate&lt;/code&gt; in R). (Once again, just in the 2x2 case.)&lt;/p&gt;
&lt;p&gt;If you are &lt;em&gt;really&lt;/em&gt; interested in building visualizations like the ones shown on 3Blue1Brown, you should check-out the associated &lt;a href=&#34;https://github.com/3b1b/manim&#34;&gt;manim&lt;/a&gt; project on github.&lt;/p&gt;
&lt;div id=&#34;topics-to-cover&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Topics to cover&lt;/h1&gt;
&lt;p&gt;I had two major sections in the Appendix of last week’s post:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Multiple matrix transformations”&lt;/li&gt;
&lt;li&gt;“Potential improvements” (where I mostly describe limitations around visualizing rotations)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post expands on these topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animatrixr-and-multiple-matrix-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;animatrixr&lt;/code&gt; and multiple matrix transformations&lt;/h1&gt;
&lt;p&gt;Sanderson discusses the value in sometimes decomposing a matrix transformation and thinking of its parts sequentially. I created a &lt;strong&gt;toy&lt;/strong&gt; package &lt;code&gt;animatrixr&lt;/code&gt; for building chained matrix transformations that can then be animated using &lt;code&gt;gganimate&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;animatrixr::add_transformation()&lt;/code&gt; lets you chain together matrix transformations with R’s pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, let’s consider three matrix transformations: horizontal sheer –&amp;gt; vertical sheer –&amp;gt; reflection across x-axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

if (!requireNamespace(&amp;quot;animatrixr&amp;quot;)) devtools::install_github(&amp;#39;brshallo/animatrixr&amp;#39;)
library(animatrixr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sheer_horizontal &amp;lt;- tribble(~ x, ~ y,
                      1, 0.5,
                      0, 1) %&amp;gt;%
  as.matrix()

sheer_vertical &amp;lt;- tribble(~ x, ~ y,
                      1, 0,
                      0.5, 1) %&amp;gt;%
  as.matrix()

reflect_x &amp;lt;- tribble(~ x, ~ y,
                      1, 0,
                      0, -1) %&amp;gt;%
  as.matrix() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s visualize the transformations being applied sequentially:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(sheer_horizontal) %&amp;gt;% 
  add_transformation(sheer_vertical) %&amp;gt;% 
  add_transformation(reflect_x, 
                     seq_fun = animatrixr::seq_matrix_l,
                     n_frames = 40) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/vsheer-hsheer-reflect-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;add_transformation()&lt;/code&gt; explicitly creates in-between frames for a given transformation. The &lt;code&gt;seq_fun&lt;/code&gt; argument allows you to define the interpolation method, for example whether the coordinates should (during the animation) follow a linear path (default) or the angle of a rotation.&lt;/p&gt;
&lt;p&gt;It would be nice to add-in functionality where the final transformation object could then be added to layers of a ggplot (though I’ve done nothing towards this except add an argument in &lt;code&gt;animatrixr::animate_matrix()&lt;/code&gt; for displaying the &lt;a href=&#34;https://github.com/lockedata/datasauRus&#34;&gt;datasauRus&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;(Warning: &lt;code&gt;animatrixr&lt;/code&gt; is severely limited, as discussed in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; and in package documentation. However you can find it at the “brshallo/animatrixr” repo on &lt;a href=&#34;https://github.com/brshallo/animatrixr&#34;&gt;my github page&lt;/a&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-rotations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing rotations&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;seq_fun&lt;/code&gt; argument within &lt;code&gt;add_transformation()&lt;/code&gt; specifies frames in-between the start and end states after a matrix transformation. By default it uses &lt;code&gt;animatrixr::seq_matrix_l&lt;/code&gt; which changes in-between coordinates linearly (as does &lt;code&gt;gganimate&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s look at a rotation where the in-between coordinates are interpolated linearly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rotate_90 &amp;lt;- tribble(~ x, ~ y,
                        cos(pi / 2), -sin(pi / 2),
                        sin(pi / 2), cos(pi / 2)) %&amp;gt;%
  as.matrix()

matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(rotate_90) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-linear-1.gif&#34; width=&#34;71%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Linear interpolation makes the rotation transformation appear scrunched during the animation (from how we intuitively think of a rotation) as the coordinate points take a straight line path to their positions after applying the transformation&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To make the in-between coordinates instead follow the angle of rotation we could change the &lt;code&gt;seq_fun&lt;/code&gt; from &lt;code&gt;animatrixr::seq_matrix_l&lt;/code&gt; to &lt;code&gt;animatrixr::seq_matrix_lp&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(rotate_90, seq_fun = animatrixr::seq_matrix_lp) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-polar-sheer-linear-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;During the rotation portion of the animation &lt;code&gt;gganimate&lt;/code&gt; is still tweening images linearly, however the frames &lt;code&gt;add_transformation()&lt;/code&gt; creates are now following along the angle of rotation of the transformation. Hence the animation ends-up approximating a curved path.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;seq_matrix_lp()&lt;/code&gt; needs improvement and was just set-up to work for toy examples – it really only looks ‘right’ if doing rotations off of &lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} 1 &amp;amp; 0\\0  &amp;amp; 1 \end{array}\right)\]&lt;/span&gt; See &lt;a href=&#34;#showing-rotations&#34;&gt;Showing rotations&lt;/a&gt; in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; for additional detail on how this is set-up and the various limitations with &lt;code&gt;animatrixr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Happy animatrixing!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# animatrixr::rotation_matrix() is helper function for creating matrix
# transformations of rotations
matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(animatrixr::rotation_matrix(pi / 2),
                     seq_fun = animatrixr::seq_matrix_lp) %&amp;gt;% 
  add_transformation(matrix(c(1, 0.5, 0, 1), nrow = 2)) %&amp;gt;% 
  add_transformation(matrix(c(1, 0, 0, -1), nrow = 2)) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/unnamed-chunk-1-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;using-animatrixr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;animatrixr&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;This is a toy package (very hastily written). I have not put effort into thinking about making it usable for others. Also, some parts just don’t really work or aren’t set-up quite right… (as noted in the README and elsewhere in the package). But feel free to check-it out / improve it / make something better! Let me know if you do!&lt;/p&gt;
&lt;p&gt;This has been a fun dabble into thinking (at least surface level) about animation. Though I don’t have any plans to add onto this (or write any more posts on this topic). If I do add anything, it will most likely just be cleaning-up the decomposition methods in the &lt;code&gt;seq_matrix*()&lt;/code&gt; functions. But no plans&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes-on-seq-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes on seq functions&lt;/h2&gt;
&lt;p&gt;Below are additional notes on the &lt;code&gt;animatrixr::seq_matrix*&lt;/code&gt; functions. They need some work, but here is a description of how they are currently set-up.&lt;/p&gt;
&lt;div id=&#34;showing-rotations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Showing rotations&lt;/h3&gt;
&lt;p&gt;To animate the rotation of a transformation, &lt;code&gt;add_transformation(m = matrix(c(0, 1, -1, 0), nrow = 2), seq_fun = seq_matrix_lp)&lt;/code&gt; explicitly creates in-between frames on the path the points would follow if they were instead following polar coordinates along the angle of rotation. In the next few sections I’ll discuss the process for doing this (again, this is not necessarily an ideal set-up).&lt;/p&gt;
&lt;p&gt;Given any 2x2 matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} a &amp;amp; b\\ c &amp;amp; d \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;you can use the equation &lt;code&gt;atan2(c, a)&lt;/code&gt; to extract the angle of rotation from the matrix&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and then create a sequence from the starting angle of rotation to the final angle of rotation.&lt;/p&gt;
&lt;p&gt;For example, if my start angle is &lt;span class=&#34;math inline&#34;&gt;\(0^\circ\)&lt;/span&gt;, and final angle of rotation is at &lt;span class=&#34;math inline&#34;&gt;\(38^\circ\)&lt;/span&gt; and I have 20 frames, then my sequence would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0^\circ, 2^\circ, ... 38^\circ\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A rotation matrix is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} cos(\theta) &amp;amp; -sin(\theta)\\ sin(\theta) &amp;amp; cos(\theta) \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence I can convert my sequence of angles into a sequence of matrices that define the rotations applied for each explicit in-between frame.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(\begin{array}{cc} cos(0^\circ) &amp;amp; -sin(0^\circ)\\ sin(0^\circ) &amp;amp; cos(0^\circ) \end{array}\right), 
\left(\begin{array}{cc} cos(2^\circ) &amp;amp; -sin(2^\circ)\\ sin(2^\circ) &amp;amp; cos(2^\circ) \end{array}\right)...
\left(\begin{array}{cc} cos(28^\circ) &amp;amp; -sin(28^\circ)\\ sin(28^\circ) &amp;amp; cos(28^\circ) \end{array}\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seq_matrix_lp-applied-on-non-standard-unit-basis-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;seq_matrix_lp&lt;/code&gt; applied on non-standard unit basis vectors&lt;/h3&gt;
&lt;p&gt;If you input a matrix transformation into &lt;code&gt;seq_matrix_lp&lt;/code&gt; that is not a pure rotation from the unit vectors it will decompose the matrix into a &lt;em&gt;rotation&lt;/em&gt; component and &lt;em&gt;other&lt;/em&gt; component&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, the &lt;em&gt;other&lt;/em&gt; component creates a sequence of matrices that have the in-between frames interpolated linearly. The sequence of &lt;em&gt;rotation&lt;/em&gt; and &lt;em&gt;other&lt;/em&gt; matrices are then recomposed to provide the final sequence.&lt;/p&gt;
&lt;p&gt;This approach means that non-pure rotations on the unit vectors, etc. will not really look like rotations. I would need to factor in other components (e.g. scale) to improve this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-rotation-first&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Show rotation first&lt;/h3&gt;
&lt;p&gt;Beyond &lt;code&gt;seq_matrip_l()&lt;/code&gt; and &lt;code&gt;seq_matrix_lp()&lt;/code&gt;, I made another seq_matrix* function: &lt;code&gt;seq_matrix_rotate_first&lt;/code&gt; which (like &lt;code&gt;seq_matrix_lp&lt;/code&gt;) also decomposes a matrix into rotation and other components. Rather than interpolating these separately and then recomposing them (as &lt;code&gt;seq_matrix_lp&lt;/code&gt; does) &lt;code&gt;seq_matrix_rotate_first&lt;/code&gt; works by interpolating them separately and then applying the decomposed sequences sequentially – so the entire rotation component of the transformation will be animated and then the ‘other’ component will be animated (this makes for twice as many frames when there is a ‘rotation’ and ‘other’ component in the transformation matrix).&lt;/p&gt;
&lt;p&gt;I.e. starting from our identity matrix and applying a single matrix transformation, it will automatically decompose this and animate the decomposed parts in two steps, &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; –&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and then from &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; –&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. Below is an example of the animation for the transformation matrix:
&lt;span class=&#34;math display&#34;&gt;\[ \left(\begin{array}{cc} 0 &amp;amp; -1\\1  &amp;amp; -0.5 \end{array}\right)\]&lt;/span&gt;
(which could be decomposed into a rotation and a sheer part).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transformation_matrix &amp;lt;- sheer_vertical %*% animatrixr::rotation_matrix(pi/4)

matrix(c(1,0,0,1), nrow = 2) %&amp;gt;% 
  add_transformation(transformation_matrix, seq_fun = seq_matrix_rotate_first) %&amp;gt;% 
  animate_matrix(datasaurus = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-animatrixr-visualizing-matrix-transformations-pt-2_files/figure-html/rotate-sheer-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;There are (especially) a lot of problems with this function currently and I don’t recommend using it e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;only works (at all correctly) if starting from standard unit vectors (hence cannot really be combined into a chain of matrix transformations)&lt;/li&gt;
&lt;li&gt;rotation component extracted will vary depending on what ‘other’ is within M
E.g. if M = {rotation}{vertical sheer} vs. M = {rotation}{horizontal sheer} – rotation component will look different&lt;/li&gt;
&lt;li&gt;I defaulted the amount of frames given to the rotation component to be the same as the amount of frames given to other component. If the size of the rotation is small relative to the other part of the transformation (or vice versa) the timing will feel slow/jumpy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Provides a cleaner approach for doing this compared to the clunky method I walked through in my post last week.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;All visualizations from last week used this linear interpolation method.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I discuss this at more length in my previous post – see the sub-section in the “Appendix”, “Problem of squeezing during rotation”.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;However I also hadn’t planned on writing a follow-up post… so who knows…&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://computergraphics.stackexchange.com/questions/3932/animating-a-smooth-linear-transformation&#34;&gt;post&lt;/a&gt; referencing this.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;To find the ‘other’ component of a matrix transformation… say &lt;em&gt;M&lt;/em&gt; represents the overall matrix transformation, in &lt;a href=&#34;#showing-rotations&#34;&gt;Showing rotations&lt;/a&gt; I described how to calculate &lt;em&gt;R&lt;/em&gt; (the rotation component), hence to calculate &lt;em&gt;A&lt;/em&gt;, ‘other’, I do:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[AR = M\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ARR^{-1} = MR^{-1}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[A = MR^{-1}\]&lt;/span&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Riddler Solutions: Palindrome Dates &amp; Ambiguous Absolute Value Bars</title>
      <link>/2020/02/13/fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/13/fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/str_view/str_view.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/str_view-binding/str_view.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#riddler-express&#34;&gt;Riddler Express&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#riddler-classic&#34;&gt;Riddler Classic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#on-duplicates&#34;&gt;On duplicates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-than-9-numbers&#34;&gt;More than 9 numbers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-more-rules&#34;&gt;Define more rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-gif&#34;&gt;Creating gif&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This post contains solutions to FiveThirtyEight’s two riddles released 2020-02-07, &lt;a href=&#34;#riddler-express&#34;&gt;Riddler Express&lt;/a&gt; and &lt;a href=&#34;#riddler-classic&#34;&gt;Riddler Classic&lt;/a&gt;. Code for figures and solutions can be found on &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs.Rmd&#34;&gt;my github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;riddler-express&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Express&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The riddle:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From James Anderson comes a palindromic puzzle of calendars:&lt;/p&gt;
&lt;p&gt;This past Sunday was Groundhog Day. Also, there was a football game. But to top it all off, the date, 02/02/2020, was palindromic, meaning it reads the same forwards and backwards (if you ignore the slashes).&lt;/p&gt;
&lt;p&gt;If we write out dates in the American format of MM/DD/YYYY (i.e., the two digits of the month, followed by the two digits of the day, followed by the four digits of the year), how many more palindromic dates will there be this century?&lt;/p&gt;
&lt;p&gt;– &lt;a href=&#34;https://fivethirtyeight.com/contributors/zach-wissner-gross/&#34;&gt;Zach Wissner-Gross&lt;/a&gt;, &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-roll-the-perfect-bowl/&#34;&gt;“How Many More Palindrome Dates Will You See,” FiveThirtyEight&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I took a simple brute-force approach. Within a dataframe and using a little code from R’s &lt;code&gt;tidyverse&lt;/code&gt; I…&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;created a column&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; containing each date from now until the end of the century&lt;/li&gt;
&lt;li&gt;created another column that contains the reverse of this&lt;/li&gt;
&lt;li&gt;filtered to only rows where the columns equal the same value&lt;/li&gt;
&lt;li&gt;counted the number of rows&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;dates&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;dates_rev&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12022021&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12022021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03022030&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;03022030&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04022040&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;04022040&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;05022050&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;05022050&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;06022060&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;06022060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;07022070&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;07022070&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;08022080&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;08022080&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;09022090&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09022090&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Which shows there will be eight more pallindromic dates in the century – one in each decade remaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;riddler-classic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Classic&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The riddle:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
Also on Super Bowl Sunday, math professor Jim Propp made a rather interesting observation:
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
I told my kid (who’d asked about absolute value signs) “They’re just like parentheses so there’s never any ambiguity,” but then I realized that things are more complicated; for instance |-1|-2|-3| could be 5 or -5. Has anyone encountered ambiguities like this in the wild?
&lt;/p&gt;
— James Propp (&lt;span class=&#34;citation&#34;&gt;@JimPropp&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/JimPropp/status/1224177172362989571?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;At first glance, this might look like one of those annoying memes about order of operations that goes viral every few years — but it’s not.&lt;/p&gt;
&lt;p&gt;When you write lengthy mathematical expressions using parentheses, it’s always clear which “open” parenthesis corresponds to which “close” parenthesis. For example, in the expression (1+2(3−4)+5), the closing parenthesis after the 4 pairs with the opening parenthesis before the 3, and not with the opening parenthesis before the 1.&lt;/p&gt;
&lt;p&gt;But pairings of other mathematical symbols can be more ambiguous. Take the absolute value symbols in Jim’s example, which are vertical bars, regardless of whether they mark the opening or closing of the absolute value. As Jim points out, |−1|−2|−3| has two possible interpretations:&lt;/p&gt;
&lt;p&gt;The two left bars are a pair and the two right bars are a pair. In this case, we have 1−2·3 = 1−6 = −5.
The two outer bars are a pair and the two inner bars are a pair. In this case, we have |−1·2−3| = |−2−3| = |−5| = 5.
Of course, if we gave each pair of bars a different height (as is done in mathematical typesetting), this wouldn’t be an issue. But for the purposes of this problem, assume the bars are indistinguishable.&lt;/p&gt;
&lt;p&gt;How many different values can the expression |−1|−2|−3|−4|−5|−6|−7|−8|−9| have?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;– &lt;a href=&#34;https://fivethirtyeight.com/contributors/zach-wissner-gross/&#34;&gt;Zach Wissner-Gross&lt;/a&gt;, &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-roll-the-perfect-bowl/&#34;&gt;“How Many More Palindrome Dates Will You See,” FiveThirtyEight&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The question is how many ways can you interpret the expression above. As hinted at by the author, the ambiguity in the expression becomes resolved based on where the parentheses are placed. Hence the question is how many different ways can we arrange the parentheses?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs_files/solutions_cropped.gif&#34; alt=&#34;Potential parentheses placements&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Potential parentheses placements&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Constraints on placing parentheses:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parentheses form pairs, hence there must be an equal numbers of left-closed and right-closed parentheses, i.e. &lt;code&gt;)&lt;/code&gt; and &lt;code&gt;(&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;We need to avoid adding meaningless parentheses (that don’t lessen ambiguity). Hence like those on the left of this expression should not count as placing a parentheses:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;|(-1)|(-2)|(-3)| &lt;span class=&#34;math inline&#34;&gt;\(\Leftrightarrow\)&lt;/span&gt; |-1|-2|-3|&lt;/p&gt;
&lt;p&gt;Hence, we will say…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A bar can only have a single parentheses placed next to it (either a right or left closed)&lt;/li&gt;
&lt;li&gt;Right-closed will be placed to the left of a bar and left closed to the right of a bar, i.e. &lt;code&gt;|)&lt;/code&gt; and &lt;code&gt;(|&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;We can ignore the left and right most bars and say that a left-closed parenthese has to go on the left, and a right closed parentheses on the right, hence we can start the problem like “(|-1|-2|-3|)”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these rules we can tackle the first part of the problem and think of each interior bar as representing a place-holder, the collection of which must be filled by an equal number of &lt;code&gt;)&lt;/code&gt; and &lt;code&gt;(&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;(|−1 _ −2 _ −3 _ −4 _ −5 _ −6 _ −7 _ −8 _−9|)&lt;/p&gt;
&lt;p&gt;This can be represented as a combinatorics&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; problem that can be represented by &lt;span class=&#34;math inline&#34;&gt;\(6 \choose 3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We could use the &lt;code&gt;combn()&lt;/code&gt; function in R to generate all these combinations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;, there is a problem; some of the combinations created could result in configurations with open parentheses. For example, even on a shorter version of this problem, the rules above would not safeguard from configurations such as:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:960px;height:100%;&#34; class=&#34;str_view html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;ul&gt;\n  &lt;li&gt;&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-1|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-2|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-3&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-4&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-5|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;&lt;\/li&gt;\n&lt;\/ul&gt;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;that go against the rules of parentheses.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You might take one of these approaches:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;plug all combinations into a calculator and throw-out those that return an error&lt;/li&gt;
&lt;li&gt;define additional rules about the configuration of parentheses that will filter out those configurations, like the one above, that would break (more effort)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I ended-up doing it both ways (was a good way to verify my work). See &lt;a href=&#34;#define-more-rules&#34;&gt;Define more rules&lt;/a&gt; in the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt; if you want to see how you might take the latter approach. For now, I’ll go the easy route and start computing our expressions.&lt;/p&gt;
&lt;p&gt;One thing I needed to do was make it so our mathematical expressions, i.e.:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:960px;height:100%;&#34; class=&#34;str_view html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;ul&gt;\n  &lt;li&gt;&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-1&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-2&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-3&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-4&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-5|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-6|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-7|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-8|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-9|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;&lt;\/li&gt;\n  &lt;li&gt;&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-1&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-2&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-3&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-4|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-5&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;|-6|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-7|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-8|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-9|&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;&lt;\/li&gt;\n  &lt;li&gt;...&lt;\/li&gt;\n&lt;\/ul&gt;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Could be represented as meaningful expressions within the R programming language, i.e.:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:960px;height:100%;&#34; class=&#34;str_view html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;ul&gt;\n  &lt;li&gt;abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-1*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-2*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-3*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-4*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-5&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-6&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-7&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-8&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-9&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;&lt;\/li&gt;\n  &lt;li&gt;abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-1*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-2*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-3*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-4&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-5*abs&lt;span class=&#39;match&#39;&gt;(&lt;\/span&gt;-6&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-7&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-8&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;-9&lt;span class=&#39;match&#39;&gt;)&lt;\/span&gt;&lt;\/li&gt;\n  &lt;li&gt;...&lt;\/li&gt;\n&lt;\/ul&gt;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;I made an equation &lt;code&gt;create_solve_expr_df()&lt;/code&gt; that creates the expressions and computes the solutions. See the &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs.Rmd&#34;&gt;raw Rmd file&lt;/a&gt; on my github to see my code&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After creating all possible configurations, I need to actually compute each viable expression to check if any of the configurations resulted in duplicate solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Number of different configurations of parentheses:&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solution_9 %&amp;gt;% 
  nrow()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;There are 42 individual configurations.&lt;/strong&gt; However we need to check if all of the evaluated solutions are unique.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solution_9 %&amp;gt;% 
  distinct(evaluated) %&amp;gt;% 
  nrow()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Given these particular inputs, there are only 39 unique solutions&lt;/em&gt;, meaning that three configurations of parentheses led to duplicate solutions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;on-duplicates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;On duplicates&lt;/h2&gt;
&lt;p&gt;You might wonder if a different set of inputs to the expression &lt;span class=&#34;math inline&#34;&gt;\(|x_1|x_2|x_3|...|x_9|\)&lt;/span&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; would lead to 39 unique solutions, or if there would be 42 unique solutions – one for each configuration. (I.e. whether the duplicates were specific to the integer inputs -1, -2, -3, -4, -5, -6, -7, -8, -9 into the expression, or would have occurred regardless of input).&lt;/p&gt;
&lt;p&gt;To verify that you could in fact get 42 unique solutions, I passed in random negative numbers with decimals to see if the function would output unique values for all configurations, or if there would again be duplicates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
solution_rand9 &amp;lt;- create_solve_expr_df(-runif(9))

solution_rand9 %&amp;gt;% 
  nrow()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This led to an equal number of expressions and unique solutions – no duplicates. Hence the fact there were duplicates in our problem was specific to the inputs of -1 to -9 not something that would result when inputting any 9 numbers into this expression. I also found this to be the case on longer expressions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-than-9-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More than 9 numbers&lt;/h2&gt;
&lt;p&gt;With the above set-up you could calculate the number of configurations for any length of input. Though I found that the computational time required increases quickly (once I started getting into problems into the 20’s things take a long-time to process). See below for a chart of unique solutions from 1 to 15&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;define-more-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Define more rules&lt;/h2&gt;
&lt;p&gt;We could define a few more rules about the configuration of our parentheses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Counting from left to right, the number of &lt;code&gt;)&lt;/code&gt; should never exceed the number of &lt;code&gt;(&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Counting from right to left, the number of &lt;code&gt;(&lt;/code&gt; should never exceed the number of &lt;code&gt;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I couldn’t immediately think of a clean way of representing this using combinatorics, so instead decided to run a simulation on our existing subset of combinations from &lt;span class=&#34;math inline&#34;&gt;\(6 \choose 3\)&lt;/span&gt; that would filter out examples that break the above rules.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs.Rmd&#34;&gt;My set-up&lt;/a&gt; took inspiration from David Robinson’s approach to a different &lt;a href=&#34;https://www.youtube.com/watch?v=TDzd73z8thU&#34;&gt;FiveThirtyEight “Riddler” problem&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   num_possible_combinations
##                       &amp;lt;int&amp;gt;
## 1                        42&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Gives the number of meaningful configurations of parentheses
&lt;ul&gt;
&lt;li&gt;Would still need to go and evaluate all of these for the given inputs (-1 to -9)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-gif&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating gif&lt;/h2&gt;
&lt;p&gt;I used &lt;code&gt;gganimate&lt;/code&gt; to create the gif of the different parentheses combinations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

set.seed(1234)
p &amp;lt;- solution_9 %&amp;gt;% 
  mutate(comb_index = row_number()) %&amp;gt;% 
  sample_n(42) %&amp;gt;% 
  select(comb_index, equation) %&amp;gt;% 
  ggplot()+
  coord_cartesian(xlim = c(-.050, 0.050), ylim = c(-0.1, 0.1))+
  geom_text(aes(x = 0, y = 0, label = equation), size = 6)+
  ggforce::theme_no_axes()+
  theme(legend.position = &amp;quot;none&amp;quot;, panel.border = element_blank())

p + transition_states(comb_index)
gganimate::anim_save(here::here(&amp;quot;static/post/2020-02-13-fivethirtyeightriddlersolutions-palindrome-debts-and-ambiguous-absolut-value-signs_files/solutions.gif&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;vector&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;khanacademy.org&#34;&gt;Khan Academy&lt;/a&gt; if you want to brush up on your combinatorics skills.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The code isn’t &lt;em&gt;the most&lt;/em&gt; attractive. The dataframe set-up could be cleaner. Also I’d like to go back and rewrite the expression part of this using &lt;code&gt;rlang&lt;/code&gt; and some of the cool things you can do with manipulating environments and expressions in R… but alas… hacked this solution together by just stitching together text…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(x_n &amp;lt; 0\)&lt;/span&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Note also that this problem requires that there be an odd number of inputs and that they all be negative.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Riddler Solutions: Perfect Bowl &amp; Magnetic Volume</title>
      <link>/2020/02/06/maximizing-magnetic-volume-the-perfect-bowl/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/06/maximizing-magnetic-volume-the-perfect-bowl/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#riddler-express&#34;&gt;Riddler Express&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#riddler-classic&#34;&gt;Riddler Classic&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#area-of-the-base-of-the-pyramid&#34;&gt;Area of the base of the pyramid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#height-of-the-pyramid&#34;&gt;Height of the pyramid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encode-functions-and-calculate-volumes&#34;&gt;Encode functions and calculate volumes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This post contains solutions to FiveThirtyEight’s two riddles released 2020-01-31, &lt;a href=&#34;#riddler-express&#34;&gt;Riddler Express&lt;/a&gt; and &lt;a href=&#34;#riddler-classic&#34;&gt;Riddler Classic&lt;/a&gt;. Code for figures and solutions can be found on my &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl.Rmd&#34;&gt;github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;riddler-express&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Express&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The riddle:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
At the recent World Indoor Bowls Championships in Great Yarmouth, England, one of the rolls by Nick Brett went viral. Here it is in all its glory:
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
12/10 on the mindblowing scale 🤯 &lt;a href=&#34;https://twitter.com/hashtag/SCtop10?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SCtop10&lt;/a&gt;&lt;br&gt;&lt;br&gt;(via &lt;a href=&#34;https://twitter.com/BBCSport?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@BBCSport&lt;/span&gt;&lt;/a&gt;) &lt;a href=&#34;https://t.co/6pN6ybzVel&#34;&gt;pic.twitter.com/6pN6ybzVel&lt;/a&gt;
&lt;/p&gt;
— SportsCenter (&lt;span class=&#34;citation&#34;&gt;@SportsCenter&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/SportsCenter/status/1220355057503363072?ref_src=twsrc%5Etfw&#34;&gt;January 23, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In order for Nick’s green bowl to split the two red bowls, he needed expert precision in both the speed of the roll and its final angle of approach.&lt;/p&gt;
&lt;p&gt;Suppose you were standing in Nick’s shoes, and you wanted to split two of your opponent’s bowls. Let’s simplify the math a little, and say that each bowl is a sphere with a radius of 1. Let’s further suppose that your opponent’s two red bowls are separated by a distance of 3 — that is, the centers of the red bowls are separated by a distance of 5. Define ɸ as the angle between the path your bowl is on and the line connecting your opponent’s bowls.
For example, here’s how you could split your opponent’s bowls when ɸ is 75°:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://fivethirtyeight.com/wp-content/uploads/2020/01/bowls.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;– &lt;a href=&#34;https://fivethirtyeight.com/contributors/zach-wissner-gross/&#34;&gt;Zach Wissner-Gross&lt;/a&gt;, &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-roll-the-perfect-bowl/&#34;&gt;&#34;Can You Roll The Perfect Bowl? FiveThirtyEight&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;My Approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketched-out:&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/bowl_calc.jpg&#34; alt=&#34;My drawings are rotated 90° clockwise from the problem description (does not affect solution)&#34; style=&#34;width:58.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;My drawings are rotated 90° clockwise from the problem description (does not affect solution)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Walked through:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I.&lt;/strong&gt; The minimum angle will be one where the green bowl touches points on both red bowls – this creates two tangents that you can think of as forming the track the green bowl travels down. Given the distance between the centers of the red bowls is 5 units, the distance between a green and a red bowl&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; will be half this, 2.5 units. Also, the lines tangent to a red bowl and the green bowl will pass a point halfway between this at 1.25 units from the center of a red circle&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;II.&lt;/strong&gt; Create the following three lines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Connect the centers of the red circles&lt;/li&gt;
&lt;li&gt;The line tangent to both a red circle and the green circle&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The line perpendicular to the tangent point on the red circle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Connecting these lines will create a right triangle with side length of 1 and hypotenuse of 1.25.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;III.&lt;/strong&gt; If you remember the &lt;em&gt;soh cah toa&lt;/em&gt; rules from trigonometry, you can use the identity &lt;span class=&#34;math inline&#34;&gt;\(sin(\phi) = \frac{opposite}{hypotenuse} \longrightarrow \phi = arcsin(\frac{1}{1.25})\)&lt;/span&gt; and compute the minimum angle is ~53.13°.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What the path of the perfect bowl would look like:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/figure-html/bowl-perfect-path-figure-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;riddler-classic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Classic&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The riddle:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From Robert Berger comes a question of maximizing magnetic volume:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Robert’s daughter has a set of Magna-Tiles, which, as their name implies, are tiles with magnets on the edges that can be used to build various polygons and polyhedra. Some of the tiles are identical isosceles triangles with one 30 degree angle and two 75 degree angles. If you were to arrange 12 of these tiles with their 30 degree angles in the center, they would lay flat and form a regular dodecagon. If you were to put fewer (between three and 11) of those tiles together in a similar way, they would form a pyramid whose base is a regular polygon. Robert has graciously provided a photo of the resulting pyramids when three and 11 tiles are used:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://fivethirtyeight.com/wp-content/uploads/2020/01/pyramids.png&#34; style=&#34;width:58.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;– &lt;a href=&#34;https://fivethirtyeight.com/contributors/zach-wissner-gross/&#34;&gt;Zach Wissner-Gross&lt;/a&gt;, &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-roll-the-perfect-bowl/&#34;&gt;&#34;Can You Roll The Perfect Bowl? FiveThirtyEight&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;My Approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Magna-Tiles will form regular pyramids. The question is which &lt;em&gt;n-sided&lt;/em&gt; pyramid will have the greatest volume. &lt;span class=&#34;math display&#34;&gt;\[(Volume\;of\;a\;pyramid) = \frac{1}{3}(area\;of\;base)(height\;of\;pyramid)\]&lt;/span&gt; Hence we need to first calculate the &lt;a href=&#34;#area-of-the-base-of-the-pyramid&#34;&gt;Area of the base of the pyramid&lt;/a&gt; and the &lt;a href=&#34;#height-of-the-pyramid&#34;&gt;Height of the pyramid&lt;/a&gt;. I’ll set-up a way of calculating these as a function of the number of (75°-75°-30°) Magna-Tiles.&lt;/p&gt;
&lt;div id=&#34;area-of-the-base-of-the-pyramid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Area of the base of the pyramid&lt;/h2&gt;
&lt;p&gt;The side length of the base of our pyramid will be the length of the shortest side of a Magna-Tile. We weren’t told the lengths of the sides of the Magna-Tiles but they don’t matter for this problem&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. To keep things simple, I’ll say the two congruent sides of the Magna-Tiles are equal to 1 unit.&lt;/p&gt;
&lt;p&gt;Drawing a line perpendicular to the base splits our Magna-Tile into two congruent triangles. Given the trigonometric identity that &lt;span class=&#34;math inline&#34;&gt;\(sin(\theta) = \frac{opposite}{hypotenuse}\)&lt;/span&gt; and that the hypotenuse of each triangle was set at 1 unit, we can calculate the length of the base of the Magna-Tile is &lt;em&gt;2sin(15°).&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/base_length.jpg&#34; alt=&#34;Base polygon side length&#34; style=&#34;width:58.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Base polygon side length&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I used &lt;em&gt;Math Open Reference&lt;/em&gt; to find an equation for &lt;a href=&#34;https://www.mathopenref.com/polygonregulararea.html&#34;&gt;area of a regular polygon&lt;/a&gt; as a function of side length and number of sides: &lt;span class=&#34;math display&#34;&gt;\[(area\;of\;regular\;polygon)\;=\;\frac{(side\;length)^{2}(number\;of\;sides)}{4tan(\frac{180^{\circ}}{number\;of\;sides})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can replace &lt;em&gt;(side length)&lt;/em&gt; in this equation with &lt;em&gt;2sin(15°)&lt;/em&gt; (calculated above),
making area a function of &lt;em&gt;only&lt;/em&gt; the number of sides on our pyramid (i.e. the number of Magna-Tiles).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;height-of-the-pyramid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Height of the pyramid&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Sketched out:&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/height_calc.jpg&#34; alt=&#34;Finding pyramid height&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Finding pyramid height&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Walked through:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The highest point of the pyramid will rest over the center of the base polygon. You can imagine a right triangle on the interior of a regular n-sided pyramid with its three line segments corresponding with:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I.&lt;/strong&gt; the length of a Magna-Tile (over its line of symmetry)&lt;br /&gt;
&lt;strong&gt;II.&lt;/strong&gt; an apothem of the pyramid’s base (an apothem is just a line segment from the center of a regular polygon to the middle of any side)&lt;br /&gt;
&lt;strong&gt;III.&lt;/strong&gt; the pyramid’s height&lt;/p&gt;
&lt;p&gt;Calculating &lt;strong&gt;I&lt;/strong&gt; &amp;amp; &lt;strong&gt;II&lt;/strong&gt; will enable us to use the Pythagorean Theorem to calculate the &lt;strong&gt;pyramid height&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I.&lt;/strong&gt; Length of a Magna-Tile (over its line of symmetry)&lt;/p&gt;
&lt;p&gt;Using the trigonometric rule that &lt;span class=&#34;math inline&#34;&gt;\(cos(\theta) = \frac{adjacent}{hypotenuse}\)&lt;/span&gt; reveals the length of a Magna-Tile as equal to &lt;em&gt;cos(15°)&lt;/em&gt; – remember we are treating the longest sides of the Magna-Tile as equal to 1 unit&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/mag_length.jpg&#34; alt=&#34;Magna-Tile length (over its line of symmetry)&#34; width=&#34;200&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Magna-Tile length (over its line of symmetry)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;II.&lt;/strong&gt; I used Math Open Reference to find the equation for &lt;a href=&#34;https://www.mathopenref.com/apothem.html&#34;&gt;apothem length&lt;/a&gt; as a function of number and length of sides in a regular polygon. &lt;span class=&#34;math display&#34;&gt;\[apothem\;length = \frac{side\;length}{2tan(\frac{180^{\circ}}{number\;of\;sides})}\]&lt;/span&gt; You can replace &lt;em&gt;side length&lt;/em&gt; in this equation with &lt;em&gt;2sin(15°)&lt;/em&gt; (calculated above), making apothem length a function of &lt;em&gt;only&lt;/em&gt; the number of sides on our pyramid (i.e. the number of Magna-Tiles).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;III.&lt;/strong&gt; Parts &lt;strong&gt;I&lt;/strong&gt; and &lt;strong&gt;II&lt;/strong&gt; represent two sides of a right triangle. To find the third side (corresponding with &lt;em&gt;pyramid height&lt;/em&gt;) simply use the Pythagorean theorem: &lt;span class=&#34;math display&#34;&gt;\[pyramid\;height = \sqrt{(MagnaTile\;length)^{2} - (apothem\;length)^{2}}\]&lt;/span&gt;
Fill in the values for &lt;em&gt;(Magna-Tile length)&lt;/em&gt; and &lt;em&gt;(apothem length)&lt;/em&gt; (as described in &lt;strong&gt;I&lt;/strong&gt; &amp;amp; &lt;strong&gt;II&lt;/strong&gt;) and you’ll see pyramid height is now represented as a function of &lt;em&gt;only&lt;/em&gt; number of sides (i.e. number of Magna-Tiles).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/height_calc.jpg&#34; alt=&#34;Finding pyramid height&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Finding pyramid height&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;encode-functions-and-calculate-volumes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Encode functions and calculate volumes&lt;/h2&gt;
&lt;p&gt;I used &lt;a href=&#34;https://github.com/brshallo/brshallo/blob/master/content/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl.Rmd&#34;&gt;R to encode&lt;/a&gt; these functions&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; and calculate the volumes for pyramids built from 2 to 12 Magna-Tiles&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/figure-html/volumes-graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Volume increases up until 10 Magna-Tiles and then decreases at 11&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Bonus plot showing number of tiles (and size of pyramid base) vs pyramid height.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-06-maximizing-magnetic-volume-the-perfect-bowl_files/figure-html/heights-and-apothem-graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;When passing the line between the centers of the red circles.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;When passing the line between the centers of the red circles.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Could do for either or both circles and would get same solution as below steps will form congruent triangles – but following these steps using the top red circle more closely follows the story of the problem.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;As the pyramids created will be similar so each pyramid would scale proportionally to one another.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;In this case the longest sides are each a hypotenuse.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;In the code I use pi / 12 radians, which is equivalent to 15° used throughout the descriptions.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;2 and 12 Magna-Tiles represent flat structures ad therefore no volume.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;At 12 the structure is flat so no longer has volume.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>